<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-zookeeper部署文档" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/19/zookeeper%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/" class="article-date">
  <time class="dt-published" datetime="2023-06-19T00:49:49.455Z" itemprop="datePublished">2023-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/19/zookeeper%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/">Zookeeper部署</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="第一步-准备工作"><a href="#第一步-准备工作" class="headerlink" title="第一步 准备工作"></a><strong>第一步</strong> 准备工作</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">安装前需要安装好jdk</span><br><span class="line"></span><br><span class="line">检测集群时间是否同步</span><br><span class="line">检测防火墙是否关闭</span><br><span class="line">检测主机 ip映射有没有配置</span><br></pre></td></tr></table></figure>

<h1 id="第二步-解压"><a href="#第二步-解压" class="headerlink" title="第二步 解压"></a><strong>第二步</strong> 解压</h1><p>在node1主机上，解压zookeeper的压缩包到&#x2F;export&#x2F;server路径下去，然后准备进行安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/software</span><br><span class="line">tar -zxvf zookeeper.tar.gz -C /export/server/</span><br><span class="line">cd /export/server/</span><br><span class="line">ln -s zookeeper/ zookeeper</span><br></pre></td></tr></table></figure>

<h1 id="第三步-环境变量"><a href="#第三步-环境变量" class="headerlink" title="第三步 环境变量"></a><strong>第三步</strong> 环境变量</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#修改环境变量（注意：3台zookeeper都需要修改）</span></span></span><br><span class="line"></span><br><span class="line">vi /etc/profile</span><br><span class="line">export ZOOKEEPER_HOME=/export/server/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h1 id="第四步-配置文件"><a href="#第四步-配置文件" class="headerlink" title="第四步 配置文件"></a><strong>第四步</strong> 配置文件</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##修改Zookeeper配置文件</span></span><br><span class="line"><span class="built_in">cd</span> /export/server/zookeeper/conf/</span><br><span class="line"><span class="built_in">cp</span> zoo_sample.cfg zoo.cfg</span><br><span class="line"><span class="built_in">mkdir</span> -p /export/data/zookeeper/zkdatas/</span><br><span class="line">vim zoo.cfg</span><br></pre></td></tr></table></figure>

<p>修改以下内容</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Zookeeper的数据存放目录</span></span><br><span class="line"><span class="attr">dataDir</span> = <span class="string">/export/data/zookeeper/zkdatas/</span></span><br><span class="line"><span class="comment"># 保留多少个快照</span></span><br><span class="line"><span class="attr">autopurge.snapRetainCount</span> = <span class="string">3</span></span><br><span class="line"><span class="comment"># 日志多少小时清理一次</span></span><br><span class="line"><span class="attr">autopurge.purgeInterval</span> = <span class="string">1</span></span><br><span class="line"><span class="comment"># 集群中服务器地址</span></span><br><span class="line"><span class="attr">server.1</span> = <span class="string">node1:2888:3888</span></span><br><span class="line"><span class="attr">server.2</span> = <span class="string">node2:2888:3888</span></span><br><span class="line"><span class="attr">server.3</span> = <span class="string">node3:2888:3888</span></span><br></pre></td></tr></table></figure>

<h1 id="第五步-添加myid配置"><a href="#第五步-添加myid配置" class="headerlink" title="**第五步 **添加myid配置"></a>**第五步 **添加myid配置</h1><p>在node1主机的&#x2F;export&#x2F;data&#x2F;zookeeper&#x2F;zkdatas&#x2F;这个路径下创建一个文件，文件名为myid ,文件内容为1</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 1 &gt; /export/data/zookeeper/zkdatas/myid </span><br></pre></td></tr></table></figure>

<h1 id="第六步-安装包分发并修改myid的值"><a href="#第六步-安装包分发并修改myid的值" class="headerlink" title="**第六步 ** 安装包分发并修改myid的值"></a>**第六步 ** 安装包分发并修改myid的值</h1><p>在node1主机上，将安装包分发到其他机器</p>
<p>第一台机器上面执行以下两个命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/</span><br><span class="line"></span><br><span class="line">scp -r /export/server/zookeeper-3.4.6/ root@node2:/export/server/</span><br><span class="line"></span><br><span class="line">scp -r /export/server/zookeeper-3.4.6/ root@node2:/export/server/</span><br></pre></td></tr></table></figure>

<p>第二台机器上建立软连接, 并修改myid的值为2</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/</span><br><span class="line"></span><br><span class="line"><span class="built_in">ln</span> -s zookeeper-3.4.6/ zookeeper</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 2 &gt; /export/data/zookeeper/zkdatas/myid </span><br></pre></td></tr></table></figure>

<p>第三台机器上建立软连接, 并修改myid的值为3</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/</span><br><span class="line"></span><br><span class="line"><span class="built_in">ln</span> -s zookeeper-3.4.6/ zookeeper</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 3 &gt; /export/data/zookeeper/zkdatas/myid</span><br></pre></td></tr></table></figure>

<h1 id="第七步-三台机器启动zookeeper服务"><a href="#第七步-三台机器启动zookeeper服务" class="headerlink" title="**第七步 ** 三台机器启动zookeeper服务"></a>**第七步 ** 三台机器启动zookeeper服务</h1><p>三台机器分别启动zookeeper服务</p>
<p>这个命令三台机器都要执行</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/server/zookeeper/bin/zkServer.sh start</span><br></pre></td></tr></table></figure>

<p>三台主机分别查看启动状态</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/server/zookeeper/bin/zkServer.sh status</span><br></pre></td></tr></table></figure>

<p>##启动（每台机器）<br>zkServer.sh start<br>或者编写一个脚本来批量启动所有机器：</p>
<p>方法1：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> host <span class="keyword">in</span> <span class="string">&quot;node1 node2 node3&quot;</span></span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">   ssh <span class="variable">$host</span> <span class="string">&quot;source/etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot;</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<p>方法2：</p>
<p>1.创建&#x2F;export&#x2F;server&#x2F;start&#x2F;zk_start目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> /export/shell</span><br></pre></td></tr></table></figure>

<p>2.编辑创建zk.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim zkall.sh</span><br></pre></td></tr></table></figure>

<p>3.写shell脚本</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">&quot;start&quot;</span>)&#123;</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> node1 node2 node3</span><br><span class="line">	<span class="keyword">do</span></span><br><span class="line">		<span class="built_in">echo</span> ---------- zookeeper <span class="variable">$i</span> 启动 ------------</span><br><span class="line">		ssh <span class="variable">$i</span> <span class="string">&quot;/export/server/zookeeper/bin/zkServer.sh start&quot;</span></span><br><span class="line">	<span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">&quot;stop&quot;</span>)&#123;</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> node1 node2 node3</span><br><span class="line">	<span class="keyword">do</span></span><br><span class="line">		<span class="built_in">echo</span> ---------- zookeeper <span class="variable">$i</span> 停止 ------------ </span><br><span class="line">		ssh <span class="variable">$i</span> <span class="string">&quot;/export/server/zookeeper/bin/zkServer.sh stop&quot;</span></span><br><span class="line">	<span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">&quot;status&quot;</span>)&#123;</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> node1 node2 node3</span><br><span class="line">	<span class="keyword">do</span></span><br><span class="line">		<span class="built_in">echo</span> ---------- zookeeper <span class="variable">$i</span> 状态 ------------ </span><br><span class="line">		ssh <span class="variable">$i</span> <span class="string">&quot;/export/server/zookeeper/bin/zkServer.sh status&quot;</span></span><br><span class="line">	<span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure>

<p>4.配置zk脚本环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#ZOOKEEPER_SHELL_HOME</span></span><br><span class="line"><span class="built_in">export</span> ZKS_HOME=/export/shell/</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ZKS_HOME</span></span><br></pre></td></tr></table></figure>

<p>5.zookeeper的环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> ZK_HOME=/export/server/zookeeper</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;ZK_HOME&#125;</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>

<p>6.让环境变量生效</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>

<p>7.启动测试</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chmod</span> 777 /export/shell/zkall.sh</span><br><span class="line">zkall.sh start</span><br></pre></td></tr></table></figure>

<p>方法3：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -eq 0 ] <span class="comment">#  $#参数的个数</span></span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;please input param:start stop status&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="keyword">if</span> [ <span class="variable">$1</span> = start  ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ing node<span class="variable">$&#123;i&#125;</span>&quot;</span></span><br><span class="line">            ssh node<span class="variable">$&#123;i&#125;</span> <span class="string">&quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot;</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> [ <span class="variable">$1</span> = stop ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ping node<span class="variable">$&#123;i&#125;</span>&quot;</span></span><br><span class="line">            ssh node<span class="variable">$&#123;i&#125;</span> <span class="string">&quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh stop&quot;</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> [ <span class="variable">$1</span> = status ]</span><br><span class="line">    <span class="keyword">then</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;</span><br><span class="line">        <span class="keyword">do</span></span><br><span class="line">            <span class="built_in">echo</span> <span class="string">&quot;node<span class="variable">$&#123;i&#125;</span> status:&quot;</span></span><br><span class="line">            ssh node<span class="variable">$&#123;i&#125;</span> <span class="string">&quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh status&quot;</span></span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>



<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -eq 0 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">	<span class="built_in">echo</span> <span class="string">&quot;please input param:start stop&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">	<span class="keyword">if</span> [ <span class="variable">$1</span> = start  ]</span><br><span class="line">		<span class="keyword">then</span></span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;</span><br><span class="line">			<span class="keyword">do</span></span><br><span class="line">				<span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ing node<span class="variable">$&#123;i&#125;</span>&quot;</span></span><br><span class="line">				ssh node<span class="variable">$&#123;i&#125;</span> <span class="string">&quot;source /etc/profile;/export/server/kafka/bin/kafka-server-start.sh -daemon /export/server/kafka/config/server.properties&quot;</span></span><br><span class="line">			<span class="keyword">done</span></span><br><span class="line">	<span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> [ <span class="variable">$1</span> = stop ]</span><br><span class="line">		<span class="keyword">then</span></span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;</span><br><span class="line">			<span class="keyword">do</span></span><br><span class="line">				<span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ing node<span class="variable">$&#123;i&#125;</span>&quot;</span></span><br><span class="line">				ssh node<span class="variable">$&#123;i&#125;</span> <span class="string">&quot;source 		/etc/profile;/export/server/kafka/bin/kafka-server-stop.sh&quot;</span></span><br><span class="line">			<span class="keyword">done</span></span><br><span class="line">	<span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">	<span class="keyword">if</span> [ <span class="variable">$1</span> = status ]</span><br><span class="line">		<span class="keyword">then</span></span><br><span class="line">			<span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;</span><br><span class="line">			<span class="keyword">do</span></span><br><span class="line">				<span class="built_in">echo</span> <span class="string">&quot;node<span class="variable">$&#123;i&#125;</span> status:&quot;</span></span><br><span class="line">				ssh node<span class="variable">$&#123;i&#125;</span> <span class="string">&quot;source /etc/profile;/export/server/kafka/bin/kafka-server-status.sh&quot;</span></span><br><span class="line">			<span class="keyword">done</span></span><br><span class="line">	<span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<hr>
<p>配置文件中参数说明:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">tickTime这个时间是作为zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔,也就是说每个tickTime时间就会发送一个心跳。</span><br><span class="line"></span><br><span class="line">initLimit这个配置项是用来配置zookeeper接受客户端（这里所说的客户端不是用户连接zookeeper服务器的客户端,而是zookeeper服务器集群中连接到leader的follower 服务器）初始化连接时最长能忍受多少个心跳时间间隔数。</span><br><span class="line"></span><br><span class="line">当已经超过10个心跳的时间（也就是tickTime）长度后 zookeeper 服务器还没有收到客户端的返回信息,那么表明这个客户端连接失败。总的时间长度就是 10*2000=20秒。</span><br><span class="line"></span><br><span class="line">syncLimit这个配置项标识leader与follower之间发送消息,请求和应答时间长度,最长不能超过多少个tickTime的时间长度,总的时间长度就是5*2000=10秒。</span><br><span class="line"></span><br><span class="line">dataDir顾名思义就是zookeeper保存数据的目录,默认情况下zookeeper将写数据的日志文件也保存在这个目录里；</span><br><span class="line"></span><br><span class="line">clientPort这个端口就是客户端连接Zookeeper服务器的端口,Zookeeper会监听这个端口接受客户端的访问请求；</span><br><span class="line"></span><br><span class="line">server.A=B:C:D中的A是一个数字,表示这个是第几号服务器,B是这个服务器的IP地址，C第一个端口用来集群成员的信息交换,表示这个服务器与集群中的leader服务器交换信息的端口，D是在leader挂掉时专门用来进行选举leader所用的端口。</span><br></pre></td></tr></table></figure>



<hr>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">export</span> <span class="string">JAVA_HOME=/export/server/jdk</span></span><br><span class="line"><span class="attr">export</span> <span class="string">PATH=$PATH:$JAVA_HOME/bin</span></span><br><span class="line"><span class="attr">export</span> <span class="string">CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span></span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/19/zookeeper%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/" data-id="clj25kfye0008n0ur468g0ouj" data-title="Zookeeper部署" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Sqoop" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/19/Sqoop/" class="article-date">
  <time class="dt-published" datetime="2023-06-19T00:49:49.453Z" itemprop="datePublished">2023-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/19/Sqoop/">Sqoop</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="一、sqoop介绍"><a href="#一、sqoop介绍" class="headerlink" title="一、sqoop介绍"></a>一、sqoop介绍</h1><p><strong>Apache Sqoop是在<code>Hadoop生态体系和RDBMS体系</code>之间<code>传送数据</code>的一种工具。来自于Apache软件基金会提供。</strong></p>
<p>Sqoop工作机制是将导入或导出命令**<code>翻译成mapreduce程序</code>**来实现。在翻译出的mapreduce中主要是对inputformat和outputformat进行定制。</p>
<ul>
<li>Hadoop生态系统包括：HDFS、Hive、Hbase等</li>
<li>RDBMS体系包括：Mysql、Oracle、DB2等</li>
<li>Sqoop可以理解为：<code>“SQL 到 Hadoop 和 Hadoop 到SQL”。</code></li>
</ul>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230201223800617.png" alt="image-20230201223800617"></p>
<p>站在Apache立场看待数据流转问题，可以分为数据的导入导出:</p>
<ul>
<li>Import：数据导入。RDBMS—–&gt;Hadoop</li>
<li>Export：数据导出。Hadoop—-&gt;RDBMS</li>
</ul>
<h1 id="二、sqoop安装"><a href="#二、sqoop安装" class="headerlink" title="二、sqoop安装"></a>二、sqoop安装</h1><p>安装sqoop的前提是已经具备java、mysql、hadoop和hive环境。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">最新稳定版： 1.4.6</span><br><span class="line"></span><br><span class="line">安装位置：node2</span><br><span class="line"></span><br><span class="line">配置文件修改：</span><br><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment">#SQOOP_HOME</span></span><br><span class="line"><span class="built_in">export</span> SQOOP_HOME=/export/server/sqoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SQOOP_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> <span class="variable">$SQOOP_HOME</span>/conf</span><br><span class="line"><span class="built_in">mv</span> sqoop-env-template.sh sqoop-env.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi sqoop-env.sh</span><br><span class="line"></span><br><span class="line">export HADOOP_COMMON_HOME= /export/server/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME= /export/server/hadoop</span><br><span class="line">export HIVE_HOME= /export/server/hive</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">加入mysql的jdbc驱动包</span><br><span class="line"><span class="built_in">cp</span> /export/server/hive/lib/mysql-connector-java-5.1.32.jar <span class="variable">$SQOOP_HOME</span>/lib/</span><br></pre></td></tr></table></figure>


<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">验证启动</span><br><span class="line">bin/sqoop list-databases \</span><br><span class="line"> --connect jdbc:mysql://node1:3306/ \</span><br><span class="line"> --username root --password hadoop</span><br><span class="line">本命令会列出所有mysql的数据库。</span><br><span class="line">到这里，整个Sqoop安装工作完成。</span><br></pre></td></tr></table></figure>

<img src=".\md图\sqoop.assets\image-20230202135937633.png" alt="image-20230202135937633" style="zoom:80%;" />

<h1 id="三、Sqoop导入"><a href="#三、Sqoop导入" class="headerlink" title="三、Sqoop导入"></a>三、Sqoop导入</h1><p>“导入工具”<code>导入单个表从RDBMS到HDFS</code>。表中的<code>每一行被视为HDFS的记录</code>。所有记录都存储为文本文件的文本数据</p>
<p>下面的语法用于将数据导入HDFS。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sqoop import (generic-args) (import-args)</span><br></pre></td></tr></table></figure>

<p><strong>Sqoop测试表数据</strong></p>
<p><code>在mysql中创建数据库userdb</code>，然后执行参考资料中的sql脚本：</p>
<p>创建三张表: **<code>emp</code>**雇员表、 **<code>emp_add</code><strong>雇员地址表、</strong><code>emp_conn</code>**雇员联系表。</p>
<img src=".\md图\sqoop.assets\image-20230201233237216.png" alt="image-20230201233237216" style="zoom:80%;" />

<h2 id="1．-全量导入mysql表数据到HDFS"><a href="#1．-全量导入mysql表数据到HDFS" class="headerlink" title="1． 全量导入mysql表数据到HDFS"></a>1． 全量导入mysql表数据到HDFS</h2><p>下面的命令用于从MySQL数据库服务器中的emp表导入HDFS。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example1-mysql-hdfs-start</span></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--target-dir /sqoop/sqoopresult \</span><br><span class="line">--table emp --m 1</span><br></pre></td></tr></table></figure>

<p>其中**<code>--target-dir可以用来指定导出数据存放至HDFS的目录；</code>**</p>
<p>为了验证在HDFS导入的数据，请使用以下命令查看导入的数据：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -<span class="built_in">cat</span> /sqoopresult/part-m-00000</span><br></pre></td></tr></table></figure>

<p>可以看出它会在HDFS上默认用逗号,分隔emp表的数据和字段。</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230201235032488.png" alt="image-20230201235032488"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> 注意：</span><br><span class="line">- mysql的地址尽量不要使用localhost  请使用ip或者host</span><br><span class="line">- 如果不指定，导入到hdfs默认分隔符是  &quot;,&quot;</span><br><span class="line">- 可以通过-- fields-terminated-by &#x27;\t&#x27;指定具体的分隔符</span><br><span class="line">- 如果表的数据比较大 可以并行启动多个maptask执行导入操作，如果表没有主键，请指定根据哪个字段进行切分（使用--m 指定并行度）</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example2-mysql-hdfs-terminated</span></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--target-dir /sqoop/sqoopresult2 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--table emp --m 1</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230201235320242.png" alt="image-20230201235320242"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example3-mysql-hdfs-split</span></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--target-dir /sqoop/sqoopresult3 \</span><br><span class="line">--fields-terminated-by &#x27;\t&#x27; \</span><br><span class="line">--split-by id \</span><br><span class="line">--table emp --m 2</span><br></pre></td></tr></table></figure>

<p> <strong>part-m-00000</strong></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230201235706065.png" alt="image-20230201235706065"></p>
<p> <strong>part-m-00001</strong></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230201235723198.png" alt="image-20230201235723198"></p>
<h2 id="2．-全量导入mysql表数据到HIVE"><a href="#2．-全量导入mysql表数据到HIVE" class="headerlink" title="2． 全量导入mysql表数据到HIVE"></a>2． 全量导入mysql表数据到HIVE</h2><h3 id="2-1．-方式一：先复制表结构到hive中再导入数据"><a href="#2-1．-方式一：先复制表结构到hive中再导入数据" class="headerlink" title="2.1． 方式一：先复制表结构到hive中再导入数据"></a>2.1． 方式一：先复制表结构到hive中再导入数据</h3><h4 id="1-在hive中新建数据库sqoop-test用于测试"><a href="#1-在hive中新建数据库sqoop-test用于测试" class="headerlink" title="1.在hive中新建数据库sqoop_test用于测试"></a>1.在hive中新建数据库<strong>sqoop_test</strong>用于测试</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> sqoop_test comment &quot;this is sqoop db&quot; <span class="keyword">with</span> dbproperties(<span class="string">&#x27;createdBy&#x27;</span><span class="operator">=</span><span class="string">&#x27;yzl&#x27;</span>);</span><br><span class="line"></span><br><span class="line">use sqoop_test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">show</span> tables;</span><br><span class="line"></span><br><span class="line"><span class="keyword">desc</span> formatted emp_add_sp;</span><br></pre></td></tr></table></figure>



<h4 id="2-将关系型数据的表结构复制到hive中"><a href="#2-将关系型数据的表结构复制到hive中" class="headerlink" title="2.将关系型数据的表结构复制到hive中"></a>2.将关系型数据的表结构复制到hive中</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example4-1-mysql-hive-structure</span></span><br><span class="line">bin/sqoop create-hive-table \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--table emp_add \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--hive-table sqoop_test.emp_add_sp</span><br><span class="line"></span><br><span class="line">其中：</span><br><span class="line"> --table emp_add为mysql中的数据库userdb中的表。  </span><br><span class="line"> --hive-table emp_add_sp 为hive中新建的表名称。</span><br></pre></td></tr></table></figure>



<h4 id="3-从关系数据库导入文件到hive中"><a href="#3-从关系数据库导入文件到hive中" class="headerlink" title="3.从关系数据库导入文件到hive中"></a>3.从关系数据库导入文件到hive中</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example4-2-mysql-hive-data</span></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table emp_add \</span><br><span class="line">--hive-table sqoop_test.emp_add_sp \</span><br><span class="line">--hive-import \</span><br><span class="line">--m 1</span><br></pre></td></tr></table></figure>



<h3 id="2-2．-方式二：直接复制表结构数据到hive中"><a href="#2-2．-方式二：直接复制表结构数据到hive中" class="headerlink" title="2.2． 方式二：直接复制表结构数据到hive中"></a>2.2． 方式二：直接复制表结构数据到hive中</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example5-mysql-hive</span></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table emp_conn \</span><br><span class="line">--hive-import \</span><br><span class="line">--m 1 \</span><br><span class="line">--hive-database sqoop_test;</span><br></pre></td></tr></table></figure>

<img src=".\md图\sqoop.assets\image-20230201223342410.png" alt="image-20230201223342410" style="zoom:80%;" />



<h2 id="3．-导入表数据子集-where过滤"><a href="#3．-导入表数据子集-where过滤" class="headerlink" title="3． 导入表数据子集(where过滤)"></a>3． 导入表数据子集(where过滤)</h2><blockquote>
<p>–where可以指定从关系数据库导入数据时的查询条件。它执行在数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example6-mysql-hdfs-where</span></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--where &quot;city =&#x27;sec-bad&#x27;&quot; \</span><br><span class="line">--target-dir /sqoop/wherequery \</span><br><span class="line">--table emp_add --m 1</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230203103145831.png" alt="image-20230203103145831"></p>
<h2 id="4．-导入表数据子集-query查询"><a href="#4．-导入表数据子集-query查询" class="headerlink" title="4． 导入表数据子集(query查询)"></a>4． 导入表数据子集(query查询)</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">注意事项：</span><br><span class="line">-使用query sql语句来进行查找不能加参数--table ;</span><br><span class="line">-并且必须要添加where条件;</span><br><span class="line">-并且where条件后面必须带一个$CONDITIONS 这个字符串;</span><br><span class="line">-并且这个sql语句必须用单引号，不能用双引号;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example7-mysql-hdfs-query</span></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--target-dir /sqoop/wherequery2 \</span><br><span class="line">--query &#x27;select id,name,deg from emp WHERE  id&gt;1203 and $CONDITIONS&#x27; \</span><br><span class="line">--split-by id \</span><br><span class="line">--fields-terminated-by &#x27;\001&#x27; \</span><br><span class="line">--m 2</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230203103632861.png" alt="image-20230203103632861"></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230203103652011.png" alt="image-20230203103652011"></p>
<blockquote>
<p>sqoop命令中 <strong>–split-by id</strong>通常配合**-m 10**参数使用。<br>首先sqoop会向关系型数据库比如mysql发送一个命令:select max(id),min(id) from test。<br>然后会把max、min之间的区间平均分为10分，最后10个并行的map去找数据库，导数据就正式开始。</p>
</blockquote>
<h2 id="5．-增量导入"><a href="#5．-增量导入" class="headerlink" title="5． 增量导入"></a>5． 增量导入</h2><p>在实际工作当中，数据的导入，很多时候都是只需要导入增量数据即可，并不需要将表中的数据每次都全部导入到hive或者hdfs当中去，这样会造成数据重复的问题。因此一般都是选用一些字段进行增量的导入， sqoop支持增量的导入数据。</p>
<p><strong>增量导入是仅导入新添加的表中的行的技术。</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">--check-column (col)</span><br><span class="line">用来指定一些列，这些列在增量导入时用来检查这些数据是否作为增量数据进行导入，和关系型数据库中的自增字段及时间戳类似。</span><br><span class="line">注意:这些被指定的列的类型不能使任意字符类型，如char、varchar等类型都是不可以的，同时-- check-column可以去指定多个列。</span><br><span class="line"></span><br><span class="line">--incremental (mode)</span><br><span class="line">append：追加，比如对大于last-value指定的值之后的记录进行追加导入。</span><br><span class="line">lastmodified：最后的修改时间，追加last-value指定的日期之后的记录</span><br><span class="line"></span><br><span class="line">--last-value (value)</span><br><span class="line">指定自从上次导入后列的最大值（大于该指定的值），也可以自己设定某一值</span><br></pre></td></tr></table></figure>

<h3 id="5-1．-Append模式增量导入"><a href="#5-1．-Append模式增量导入" class="headerlink" title="5.1． Append模式增量导入"></a>5.1． Append模式增量导入</h3><p>执行以下指令先将我们之前的数据导入</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example8-1-mysql-hdfs-append</span></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--target-dir /sqoop/appendresult \</span><br><span class="line">--table emp --m 1</span><br></pre></td></tr></table></figure>

<p>使用hdfs dfs -cat查看生成的数据文件，发现数据已经导入到hdfs中</p>
<p>然后在mysql的emp表中插入2条数据:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> `userdb`.`emp` (`id`, `name`, `deg`, `salary`, `dept`) <span class="keyword">values</span> (<span class="string">&#x27;1206&#x27;</span>, <span class="string">&#x27;allen&#x27;</span>, <span class="string">&#x27;admin&#x27;</span>, <span class="string">&#x27;30000&#x27;</span>, <span class="string">&#x27;tp&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> `userdb`.`emp` (`id`, `name`, `deg`, `salary`, `dept`) <span class="keyword">values</span> (<span class="string">&#x27;1207&#x27;</span>, <span class="string">&#x27;woon&#x27;</span>, <span class="string">&#x27;admin&#x27;</span>, <span class="string">&#x27;40000&#x27;</span>, <span class="string">&#x27;tp&#x27;</span>);</span><br></pre></td></tr></table></figure>

<p>执行如下的指令，实现增量的导入:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example8-2-mysql-hdfs-append</span></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table emp --m 1 \</span><br><span class="line">--target-dir /sqoop/appendresult \</span><br><span class="line">--incremental append \</span><br><span class="line">--check-column id \</span><br><span class="line">--last-value 1205</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">总结：增量数据的导入</span><br><span class="line">- 所谓的增量数据指的是上次至今中间新增加的数据</span><br><span class="line">- sqoop支持两种模式的增量导入 </span><br><span class="line">- append追加 根据数值类型字段进行追加导入  大于指定的last-value</span><br><span class="line">- lastmodified 根据时间戳类型字段进行追加  大于等于指定的last-value</span><br><span class="line">- 注意在lastmodified 模式下 还分为两种情形：append  merge-key</span><br></pre></td></tr></table></figure>

<img src=".\md图\sqoop.assets\image-20230202134830695.png" alt="image-20230202134830695" style="zoom:150%;" />

<p>最后验证导入数据目录 可以发现多了一个文件 里面就是增量数据</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230202134847323.png" alt="image-20230202134847323"></p>
<h3 id="5-2．-Lastmodified模式增量导入"><a href="#5-2．-Lastmodified模式增量导入" class="headerlink" title="5.2． Lastmodified模式增量导入"></a>5.2． Lastmodified模式增量导入</h3><p>（1）首先创建一个customer表，指定一个时间戳字段：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> customertest(id <span class="type">int</span>,name <span class="type">varchar</span>(<span class="number">20</span>),last_mod <span class="type">timestamp</span> <span class="keyword">default</span> <span class="built_in">current_timestamp</span> <span class="keyword">on</span> <span class="keyword">update</span> <span class="built_in">current_timestamp</span>);</span><br></pre></td></tr></table></figure>

<p><strong>此处的时间戳设置为在数据的产生和更新时都会发生改变.</strong> </p>
<p>（2）插入如下记录:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> customertest(id,name) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">&#x27;neil&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> customertest(id,name) <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">&#x27;jack&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> customertest(id,name) <span class="keyword">values</span>(<span class="number">3</span>,<span class="string">&#x27;martin&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> customertest(id,name) <span class="keyword">values</span>(<span class="number">4</span>,<span class="string">&#x27;tony&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> customertest(id,name) <span class="keyword">values</span>(<span class="number">5</span>,<span class="string">&#x27;eric&#x27;</span>);</span><br></pre></td></tr></table></figure>

<p>（3）此时执行sqoop指令将数据导入hdfs:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example9-1-mysql-hdfs-Lastmodified</span></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--target-dir /sqoop/lastmodifiedresult \</span><br><span class="line">--table customertest --m 1</span><br></pre></td></tr></table></figure>

<p>查看此时导入的结果数据：</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230202135317642.png" alt="image-20230202135317642"></p>
<p>（4）再次插入一条数据进入customertest表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> customertest(id,name) <span class="keyword">values</span>(<span class="number">6</span>,<span class="string">&#x27;james&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>（5）使用incremental的方式进行增量的导入:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example9-2-mysql-hdfs-Lastmodified</span></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table customertest \</span><br><span class="line">--target-dir /sqoop/lastmodifiedresult \</span><br><span class="line">--check-column last_mod \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--last-value &quot;2019-05-28 18:42:06&quot; \</span><br><span class="line">--m 1 \</span><br><span class="line">--append</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230202135437377.png" alt="image-20230202135437377"></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230202135443401.png" alt="image-20230202135443401"></p>
<p>此处已经会导入我们最后插入的一条记录,但是我们却发现此处插入了2条数据，这是为什么呢？ </p>
<p>这是因为采用<strong>lastmodified模式去处理增量时，会将大于等于last-value值的数据当做增量插入。</strong></p>
<h3 id="5-3．-Lastmodified模式-append、merge-key"><a href="#5-3．-Lastmodified模式-append、merge-key" class="headerlink" title="5.3． Lastmodified模式:append、merge-key"></a>5.3． Lastmodified模式:append、merge-key</h3><p>使用lastmodified模式进行增量处理要指定增量数据是以</p>
<ul>
<li><strong>append</strong>模式(附加)</li>
<li><strong>merge-key</strong>(合并)模式添加</li>
</ul>
<p>下面演示使用merge-by的模式进行增量更新</p>
<p>（1）我们去更新 id为1的name字段。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">update</span> customertest <span class="keyword">set</span> name <span class="operator">=</span> <span class="string">&#x27;Neil&#x27;</span> <span class="keyword">where</span> id <span class="operator">=</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure>

<p>更新之后，这条数据的时间戳会更新为更新数据时的系统时间.</p>
<p>（2）执行如下指令，把id字段作为merge-key:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example10-mysql-hdfs-merge-key</span></span><br><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table customertest \</span><br><span class="line">--target-dir /sqoop/lastmodifiedresult \</span><br><span class="line">--check-column last_mod \</span><br><span class="line">--incremental lastmodified \</span><br><span class="line">--last-value &quot;2019-05-28 18:42:06&quot; \</span><br><span class="line">--m 1 \</span><br><span class="line">--merge-key id</span><br></pre></td></tr></table></figure>

<blockquote>
<p>由于merge-key这种模式是进行了一次完整的mapreduce操作，<br>因此最终我们在lastmodifiedresult文件夹下可以看到生成的为part-r-00000这样的文件，<br>会发现<strong>id&#x3D;1的name</strong>已经得到修改，同时<strong>新增了id&#x3D;6</strong>的数据</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">总结：</span><br><span class="line">关于lastmodified 中的两种模式：</span><br><span class="line">- append 只会追加增量数据到一个新的文件中  并且会产生数据的重复问题</span><br><span class="line">  因为默认是从指定的last-value 大于等于其值的数据开始导入</span><br><span class="line">- merge-key 把增量的数据合并到一个文件中  处理追加增量数据之外 如果之前的数据有变化修改</span><br><span class="line">  也可以进行修改操作 底层相当于进行了一次完整的mr作业。数据不会重复。</span><br></pre></td></tr></table></figure>



<h1 id="四、-Sqoop导出"><a href="#四、-Sqoop导出" class="headerlink" title="四、 Sqoop导出"></a>四、 Sqoop导出</h1><p><strong>将数据从Hadoop生态体系导出到RDBMS数据库导出前，目标表必须存在于目标数据库中。</strong></p>
<p>export有三种模式：</p>
<ol>
<li><p>默认操作是从将文件中的数据使用INSERT语句插入到表中。</p>
</li>
<li><p>更新模式：Sqoop将生成UPDATE替换数据库中现有记录的语句。</p>
</li>
<li><p>调用模式：Sqoop将为每条记录创建一个存储过程调用。</p>
</li>
</ol>
<p>以下是export命令语法：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sqoop <span class="built_in">export</span> (generic-args) (export-args)</span></span><br></pre></td></tr></table></figure>



<h2 id="1．-默认模式导出HDFS数据到mysql"><a href="#1．-默认模式导出HDFS数据到mysql" class="headerlink" title="1． 默认模式导出HDFS数据到mysql"></a>1． 默认模式导出HDFS数据到mysql</h2><p>默认情况下，sqoop export将每行输入记录转换成一条INSERT语句，添加到目标数据库表中。如果数据库中的表具有约束条件（例如，其值必须唯一的主键列）并且已有数据存在，则必须注意避免插入违反这些约束条件的记录。如果INSERT语句失败，导出过程将失败。<strong>此模式主要用于将记录导出到可以接收这些结果的空表中</strong>。通常用于全表数据导出。</p>
<p>导出时可以是将Hive表中的全部记录或者HDFS数据（可以是全部字段也可以部分字段）导出到Mysql目标表。</p>
<h3 id="1-1．-准备HDFS数据"><a href="#1-1．-准备HDFS数据" class="headerlink" title="1.1． 准备HDFS数据"></a>1.1． 准备HDFS数据</h3><p> 在HDFS文件系统中“&#x2F;emp&#x2F;”目录的下创建一个文件emp_data.txt：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir /export/data/sqoop-data/emp/</span><br><span class="line"></span><br><span class="line">vim emp_data.txt</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1201,gopal,manager,50000,TP</span><br><span class="line">1202,manisha,preader,50000,TP</span><br><span class="line">1203,kalil,php dev,30000,AC</span><br><span class="line">1204,prasanth,php dev,30000,AC</span><br><span class="line">1205,kranthi,admin,20000,TP</span><br><span class="line">1206,satishp,grpdes,20000,GR</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">上传至hdfs</span></span><br><span class="line">hadoop fs -mkdir /sqoop/emp_data</span><br><span class="line">hadoop fs -put emp_data.txt /sqoop/emp_data </span><br></pre></td></tr></table></figure>

<h3 id="1-2．-手动创建mysql中的目标表"><a href="#1-2．-手动创建mysql中的目标表" class="headerlink" title="1.2． 手动创建mysql中的目标表"></a>1.2． 手动创建mysql中的目标表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> use userdb;</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> employee ( </span><br><span class="line">   id <span class="type">int</span> <span class="keyword">not</span> <span class="keyword">null</span> <span class="keyword">primary</span> key, </span><br><span class="line">   name <span class="type">varchar</span>(<span class="number">20</span>), </span><br><span class="line">   deg <span class="type">varchar</span>(<span class="number">20</span>),</span><br><span class="line">   salary <span class="type">int</span>,</span><br><span class="line">   dept <span class="type">varchar</span>(<span class="number">10</span>));</span><br></pre></td></tr></table></figure>

<h3 id="1-3．-执行导出命令"><a href="#1-3．-执行导出命令" class="headerlink" title="1.3． 执行导出命令"></a>1.3． 执行导出命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example10-hdfs-mysql-export</span></span><br><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table employee1 \</span><br><span class="line">--columns id,name,deg,salary,dept \</span><br><span class="line">--export-dir /sqoop/emp_data/</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230202170211379.png" alt="image-20230202170211379"></p>
<h3 id="1-4．-相关配置参数"><a href="#1-4．-相关配置参数" class="headerlink" title="1.4． 相关配置参数"></a>1.4． 相关配置参数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--input-fields-terminated-by &#x27;\t&#x27;  </span><br><span class="line">指定文件中的分隔符</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">--columns </span><br><span class="line">选择列并控制它们的排序。当导出数据文件和目标表字段列顺序完全一致的时候可以不写。否则以逗号为间隔选择和排列各个列。没有被包含在–columns后面列名或字段要么具备默认值，要么就允许插入空值。否则数据库会拒绝接受sqoop导出的数据，导致Sqoop作业失败</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--export-dir 导出目录，在执行导出的时候，必须指定这个参数，同时需要具备--table或--call参数两者之一，</span><br><span class="line">--table是指的导出数据库当中对应的表，</span><br><span class="line">--call是指的某个存储过程。</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--input-null-string --input-null-non-string</span><br><span class="line">如果没有指定第一个参数，对于字符串类型的列来说，“NULL”这个字符串就回被翻译成空值，如果没有使用第二个参数，无论是“NULL”字符串还是说空字符串也好，对于非字符串类型的字段来说，这两个类型的空串都会被翻译成空值。比如：</span><br><span class="line">--input-null-string &quot;\\N&quot; --input-null-non-string &quot;\\N&quot;</span><br></pre></td></tr></table></figure>

<h3 id="1-5-注意"><a href="#1-5-注意" class="headerlink" title="1.5 注意"></a>1.5 注意</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">数据导出操作</span><br><span class="line"></span><br><span class="line">- 注意：导出的目标表需要自己手动提前创建 也就是sqoop并不会帮我们创建复制表结构</span><br><span class="line">- 导出有三种模式：</span><br><span class="line">  - 默认模式   目标表是空表  底层把数据一条条insert进去</span><br><span class="line">  - 更新模式   底层是update语句</span><br><span class="line">  - 调用模式   调用存储过程</span><br><span class="line">- 相关配置参数</span><br><span class="line">  - 导出文件的分隔符  如果不指定 默认以“,”去切割读取数据文件   --input-fields-terminated-by</span><br><span class="line">  - 如果文件的字段顺序和表中顺序不一致 需要--columns 指定 多个字段之间以&quot;,&quot;</span><br><span class="line">  - 导出的时候需要指定导出数据的目的 export-dir 和导出到目标的表名或者存储过程名</span><br><span class="line">  - 针对空字符串类型和非字符串类型的转换  &quot;\n&quot;</span><br></pre></td></tr></table></figure>



<h2 id="2．-更新导出（updateonly模式）"><a href="#2．-更新导出（updateonly模式）" class="headerlink" title="2． 更新导出（updateonly模式）"></a>2． 更新导出（updateonly模式）</h2><h3 id="2-1．-参数说明"><a href="#2-1．-参数说明" class="headerlink" title="2.1． 参数说明"></a>2.1． 参数说明</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-- update-key,更新标识，即根据某个字段进行更新，例如id，可以指定多个更新标识的字段，多个字段之间用逗号分隔。</span><br><span class="line"></span><br><span class="line">-- updatemod，指定updateonly（默认模式），仅仅更新已存在的数据记录，不会插入新纪录。</span><br></pre></td></tr></table></figure>

<h3 id="2-2．-准备HDFS数据"><a href="#2-2．-准备HDFS数据" class="headerlink" title="2.2． 准备HDFS数据"></a>2.2． 准备HDFS数据</h3><p>在HDFS文件系统中&#x2F;sqoop&#x2F;updateonly_1&#x2F;目录的下创建一个文件updateonly_1.txt：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1201,gopal,manager,50000</span><br><span class="line">1202,manisha,preader,50000</span><br><span class="line">1203,kalil,php dev,30000</span><br></pre></td></tr></table></figure>

<h3 id="2-3．-手动创建mysql中的目标表"><a href="#2-3．-手动创建mysql中的目标表" class="headerlink" title="2.3． 手动创建mysql中的目标表"></a>2.3． 手动创建mysql中的目标表</h3><p>手动创建mysql中的目标表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> USE userdb;</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> updateonly ( </span><br><span class="line">   id <span class="type">INT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span> <span class="keyword">PRIMARY</span> KEY, </span><br><span class="line">   name <span class="type">VARCHAR</span>(<span class="number">20</span>), </span><br><span class="line">   deg <span class="type">VARCHAR</span>(<span class="number">20</span>),</span><br><span class="line">   salary <span class="type">INT</span>);</span><br></pre></td></tr></table></figure>

<h3 id="2-4．-先执行全部导出操作"><a href="#2-4．-先执行全部导出操作" class="headerlink" title="2.4． 先执行全部导出操作"></a>2.4． 先执行全部导出操作</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example11-1-hdfs-mysql-export-updateonly</span></span><br><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table updateonly \</span><br><span class="line">--export-dir /sqoop/updateonly_1/</span><br></pre></td></tr></table></figure>

<h3 id="2-5．-查看此时mysql中的数据"><a href="#2-5．-查看此时mysql中的数据" class="headerlink" title="2.5． 查看此时mysql中的数据"></a>2.5． 查看此时mysql中的数据</h3><p>可以发现是全量导出，全部的数据</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230202213846761.png" alt="image-20230202213846761"></p>
<h3 id="2-6．-新增一个文件"><a href="#2-6．-新增一个文件" class="headerlink" title="2.6． 新增一个文件"></a>2.6． 新增一个文件</h3><p>新增一个文件updateonly_2.txt：<strong>修改了前三条数据并且新增了一条记录</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1201,gopal,manager,1212</span><br><span class="line">1202,manisha,preader,1313</span><br><span class="line">1203,kalil,php dev,1414</span><br><span class="line">1204,allen,java,1515</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /sqoop/updateonly_2</span><br><span class="line">hadoop fs -put updateonly_2.txt /sqoop/updateonly_2</span><br></pre></td></tr></table></figure>

<h3 id="2-7．-执行更新导出"><a href="#2-7．-执行更新导出" class="headerlink" title="2.7． 执行更新导出"></a>2.7． 执行更新导出</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example11-2-hdfs-mysql-export-updateonly</span></span><br><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table updateonly \</span><br><span class="line">--export-dir /sqoop/updateonly_2 \</span><br><span class="line">--update-key id \</span><br><span class="line">--update-mode updateonly</span><br></pre></td></tr></table></figure>

<h3 id="2-8．-查看最终结果"><a href="#2-8．-查看最终结果" class="headerlink" title="2.8． 查看最终结果"></a>2.8． 查看最终结果</h3><p>虽然导出时候的日志显示导出4条记录：</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230202214142738.png" alt="image-20230202214142738"></p>
<p>但最终只进行了更新操作</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230202214154136.png" alt="image-20230202214154136"></p>
<h2 id="3．-更新导出（allowinsert模式）"><a href="#3．-更新导出（allowinsert模式）" class="headerlink" title="3． 更新导出（allowinsert模式）"></a>3． 更新导出（allowinsert模式）</h2><h3 id="3-1．-参数说明"><a href="#3-1．-参数说明" class="headerlink" title="3.1． 参数说明"></a>3.1． 参数说明</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-- update-key，更新标识，即根据某个字段进行更新，例如id，可以指定多个更新标识的字段，多个字段之间用逗号分隔。</span><br><span class="line"></span><br><span class="line">-- updatemod，指定allowinsert，更新已存在的数据记录，同时插入新纪录。实质上是一个insert &amp; update的操作。</span><br></pre></td></tr></table></figure>

<h3 id="3-2．-准备HDFS数据"><a href="#3-2．-准备HDFS数据" class="headerlink" title="3.2． 准备HDFS数据"></a>3.2． 准备HDFS数据</h3><p>在HDFS &#x2F;sqoop&#x2F;allowinsert_1&#x2F;目录的下创建一个文件allowinsert_1.txt：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1201,gopal,manager,50000</span><br><span class="line">1202,manisha,preader,50000</span><br><span class="line">1203,kalil,php dev,30000</span><br></pre></td></tr></table></figure>

<h3 id="3-3．-手动创建mysql中的目标表"><a href="#3-3．-手动创建mysql中的目标表" class="headerlink" title="3.3． 手动创建mysql中的目标表"></a>3.3． 手动创建mysql中的目标表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> USE userdb;</span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> allowinsert ( </span><br><span class="line">   id <span class="type">INT</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span> <span class="keyword">PRIMARY</span> KEY, </span><br><span class="line">   name <span class="type">VARCHAR</span>(<span class="number">20</span>), </span><br><span class="line">   deg <span class="type">VARCHAR</span>(<span class="number">20</span>),</span><br><span class="line">   salary <span class="type">INT</span>);</span><br></pre></td></tr></table></figure>

<h3 id="3-4．-先执行全部导出操作"><a href="#3-4．-先执行全部导出操作" class="headerlink" title="3.4． 先执行全部导出操作"></a>3.4． 先执行全部导出操作</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example12-1-hdfs-mysql-export-allowinsert</span></span><br><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--table allowinsert \</span><br><span class="line">--export-dir /sqoop/allowinsert_1/</span><br></pre></td></tr></table></figure>

<h3 id="3-5．-查看此时mysql中的数据"><a href="#3-5．-查看此时mysql中的数据" class="headerlink" title="3.5． 查看此时mysql中的数据"></a>3.5． 查看此时mysql中的数据</h3><p>可以发现是全量导出，全部的数据</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230202214522375.png" alt="image-20230202214522375"></p>
<h3 id="3-6．-新增文件"><a href="#3-6．-新增文件" class="headerlink" title="3.6． 新增文件"></a>3.6． 新增文件</h3><p>创建文件allowinsert_2.txt。修改前三条数据并且新增了一条记录。上传至 &#x2F;sqoop&#x2F;allowinsert_2&#x2F;目录下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1201,gopal,manager,1212</span><br><span class="line">1202,manisha,preader,1313</span><br><span class="line">1203,kalil,php dev,1414</span><br><span class="line">1204,allen,java,1515</span><br></pre></td></tr></table></figure>

<h3 id="3-7．-执行更新导出"><a href="#3-7．-执行更新导出" class="headerlink" title="3.7． 执行更新导出"></a>3.7． 执行更新导出</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example12-2-hdfs-mysql-export-allowinsert</span></span><br><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root --password hadoop \</span><br><span class="line">--table allowinsert \</span><br><span class="line">--export-dir /sqoop/allowinsert_2/ \</span><br><span class="line">--update-key id \</span><br><span class="line">--update-mode allowinsert</span><br></pre></td></tr></table></figure>

<h3 id="3-8．-查看最终结果"><a href="#3-8．-查看最终结果" class="headerlink" title="3.8． 查看最终结果"></a>3.8． 查看最终结果</h3><p>导出时候的日志显示导出4条记录：</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230202214838765.png" alt="image-20230202214838765"></p>
<p>数据进行更新操作的同时也进行了新增的操作</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Csqoop.assets%5Cimage-20230202214850120.png" alt="image-20230202214850120"></p>
<h3 id="3-9-总结"><a href="#3-9-总结" class="headerlink" title="3.9 总结"></a>3.9 总结</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">更新导出</span><br><span class="line"></span><br><span class="line">- updateonly  只更新已经存在的数据  不会执行insert增加新的数据</span><br><span class="line">- allowinsert  更新已有的数据  插入新的数据 底层相当于insert&amp;update</span><br></pre></td></tr></table></figure>

<h1 id="五、sqoop-job作业介绍"><a href="#五、sqoop-job作业介绍" class="headerlink" title="五、sqoop job作业介绍"></a>五、sqoop job作业介绍</h1><h2 id="1-job语法"><a href="#1-job语法" class="headerlink" title="1.job语法"></a>1.job语法</h2><p>以下是创建Sqoop作业的语法。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sqoop job (generic-args) (job-args)</span></span><br><span class="line">   [-- [subtool-name] (subtool-args)]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sqoop-job (generic-args) (job-args)</span></span><br><span class="line">   [-- [subtool-name] (subtool-args)]</span><br></pre></td></tr></table></figure>

<h2 id="2-创建job-–create"><a href="#2-创建job-–create" class="headerlink" title="2.创建job(–create)"></a>2.创建job(–create)</h2><p>在这里，我们创建一个名为myjob，这可以从RDBMS表的数据导入到HDFS作业。下面的命令用于创建一个从DB数据库的employee表导入到HDFS文件的作业。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example13-1-mysql-hdfs-job</span></span><br><span class="line">bin/sqoop job --create myjob -- import --connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password hadoop \</span><br><span class="line">--target-dir /sqoop/sqoopresult555 \</span><br><span class="line">--table emp --m 1</span><br><span class="line"></span><br><span class="line">注意import前要有空格</span><br></pre></td></tr></table></figure>

<h2 id="3-验证job-–list"><a href="#3-验证job-–list" class="headerlink" title="3.验证job (–list)"></a>3.验证job (–list)</h2><p><strong>‘–list’</strong> 参数是用来验证保存的作业。下面的命令用来验证保存Sqoop作业的列表。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example13-2-mysql-hdfs-job</span></span><br><span class="line">bin/sqoop job --list</span><br></pre></td></tr></table></figure>

<p>它显示了保存作业列表。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Available jobs: </span><br><span class="line">   myjob</span><br></pre></td></tr></table></figure>

<h2 id="4-检查job-–show"><a href="#4-检查job-–show" class="headerlink" title="4.检查job(–show)"></a>4.检查job(–show)</h2><p><strong>‘–show’</strong> 参数用于检查或验证特定的工作，及其详细信息。以下命令和样本输出用来验证一个名为myjob的作业。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example13-3-mysql-hdfs-job</span></span><br><span class="line">bin/sqoop job --show myjob</span><br></pre></td></tr></table></figure>

<p>它显示了工具和它们的选择，这是使用在myjob中作业情况。</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Job</span>: <span class="string">myjob </span></span><br><span class="line"> <span class="attr">Tool</span>: <span class="string">import Options:</span></span><br><span class="line"> <span class="attr">----------------------------</span> <span class="string"></span></span><br><span class="line"> <span class="attr">direct.import</span> = <span class="string">true</span></span><br><span class="line"> <span class="attr">codegen.input.delimiters.record</span> = <span class="string">0</span></span><br><span class="line"> <span class="attr">hdfs.append.dir</span> = <span class="string">false </span></span><br><span class="line"> <span class="attr">db.table</span> = <span class="string">employee</span></span><br><span class="line"> <span class="attr">...</span></span><br><span class="line"> <span class="attr">incremental.last.value</span> = <span class="string">1206</span></span><br><span class="line"> <span class="attr">...</span></span><br></pre></td></tr></table></figure>

<h2 id="5-执行job-–exec"><a href="#5-执行job-–exec" class="headerlink" title="5.执行job (–exec)"></a>5.执行job (–exec)</h2><p><strong>‘–exec’</strong> 选项用于执行保存的作业。下面的命令用于执行保存的作业称为myjob。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example13-4-mysql-hdfs-job</span></span><br><span class="line">bin/sqoop job --exec myjob</span><br><span class="line"></span><br><span class="line">sqoop需要输入mysql密码</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>它会显示下面的输出。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10/08/19 13:08:45 INFO tool.CodeGenTool: Beginning code generation </span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="6-job的免密输入"><a href="#6-job的免密输入" class="headerlink" title="6.job的免密输入"></a>6.job的免密输入</h2><p>sqoop在创建job时，使用–password-file参数，可以避免输入mysql密码，如果使用–password将出现警告，并且每次都要手动输入密码才能执行job，<strong>sqoop规定密码文件必须存放在HDFS上，并且权限必须是400</strong>。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">echo -n &quot;hadoop&quot; &gt; node1-mysql.pwd</span><br><span class="line">hadoop fs -mkdir -p /sqoop/pwd/</span><br><span class="line">hadoop fs -put node1-mysql.pwd /sqoop/pwd/</span><br><span class="line">hadoop fs -chmod 400 /sqoop/pwd/node1-mysql.pwd</span><br></pre></td></tr></table></figure>

<p><strong>检查sqoop的sqoop-site.xml是否存在如下配置：</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>sqoop.metastore.client.record.password<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, allow saved passwords in the metastore.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li><h5 id="创建sqoop-job"><a href="#创建sqoop-job" class="headerlink" title="创建sqoop job"></a>创建sqoop job</h5></li>
</ul>
<p>在创建job时，使用–password-file参数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example14-1-mysql-hdfs-job-nopwd</span></span><br><span class="line">bin/sqoop job --create myjob2 -- import --connect jdbc:mysql://node1:3306/userdb \</span><br><span class="line">--username root \</span><br><span class="line">--password-file /sqoop/pwd/node1-mysql.pwd \</span><br><span class="line">--target-dir /sqoop/sqoopresult666 \</span><br><span class="line">--table emp --m 1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><h5 id="执行job"><a href="#执行job" class="headerlink" title="执行job"></a>执行job</h5></li>
</ul>
<p>通过命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example14-2-mysql-hdfs-job-nopwd</span></span><br><span class="line">sqoop job -exec myjob2</span><br></pre></td></tr></table></figure>

<p>如果password文件格式错误会有如下提示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ERROR manager.SqlManager: Error executing statement: java.sql.SQLException: Access denied for user &#x27;root&#x27;@&#x27;spark220&#x27; (using password: YES)</span><br><span class="line"></span><br><span class="line">ERROR tool.ImportTool: Encountered IOException running import job: java.io.IOException: No columns to generate for ClassWriter at org.apache.sqoop.orm.ClassWriter.generate(ClassWriter.java:1652)</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/19/Sqoop/" data-id="clj25kfyd0007n0urd31s36ka" data-title="Sqoop" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Spark部署文档" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/19/Spark%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/" class="article-date">
  <time class="dt-published" datetime="2023-06-19T00:49:49.451Z" itemprop="datePublished">2023-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/19/Spark%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/">Spark部署</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Spark-Local环境部署"><a href="#Spark-Local环境部署" class="headerlink" title="Spark Local环境部署"></a>Spark Local环境部署</h1><h2 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h2><p><a target="_blank" rel="noopener" href="https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz">https://dlcdn.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz</a></p>
<h2 id="条件"><a href="#条件" class="headerlink" title="条件"></a>条件</h2><ul>
<li>PYTHON 推荐3.8</li>
<li>JDK 1.8</li>
</ul>
<h2 id="Anaconda-On-Linux-安装"><a href="#Anaconda-On-Linux-安装" class="headerlink" title="Anaconda On Linux 安装"></a>Anaconda On Linux 安装</h2><p>本次课程的Python环境需要安装到Linux(虚拟机)和Windows(本机)上</p>
<p>参见最下方, 附1: Anaconda On Linux 安装</p>
<h2 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h2><p>解压下载的Spark安装包</p>
<p><code>tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</code></p>
<h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><p>配置Spark由如下5个环境变量需要设置</p>
<ul>
<li>SPARK_HOME: 表示Spark安装路径在哪里 </li>
<li>PYSPARK_PYTHON: 表示Spark想运行Python程序, 那么去哪里找python执行器 </li>
<li>JAVA_HOME: 告知Spark Java在哪里 </li>
<li>HADOOP_CONF_DIR: 告知Spark Hadoop的配置文件在哪里 </li>
<li>HADOOP_HOME: 告知Spark  Hadoop安装在哪里</li>
</ul>
<p>这5个环境变量 都需要配置在: <code>/etc/profile</code>中<br>​</p>
<p><img src="/./md%E5%9B%BE/spark.assets/1.jpg"></p>
<p>PYSPARK_PYTHON和 JAVA_HOME 需要同样配置在: <code>/root/.bashrc</code>中</p>
<p><img src="/./md%E5%9B%BE/spark.assets/2.jpg"></p>
<h2 id="上传Spark安装包"><a href="#上传Spark安装包" class="headerlink" title="上传Spark安装包"></a>上传Spark安装包</h2><p>资料中提供了: <code>spark-3.2.0-bin-hadoop3.2.tgz</code></p>
<p>上传这个文件到Linux服务器中</p>
<p>将其解压, 课程中将其解压(安装)到: <code>/export/server</code>内.</p>
<p><code>tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</code></p>
<p>由于spark目录名称很长, 给其一个软链接:</p>
<p><code>ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</code><br>​</p>
<p><img src="/./md%E5%9B%BE/spark.assets/3.jpg"><br><img src="/./md%E5%9B%BE/spark.assets/4.jpg"></p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="bin-x2F-pyspark"><a href="#bin-x2F-pyspark" class="headerlink" title="bin&#x2F;pyspark"></a>bin&#x2F;pyspark</h3><p>bin&#x2F;pyspark 程序, 可以提供一个  <code>交互式</code>的 Python解释器环境, 在这里面可以写普通python代码, 以及spark代码<br>​</p>
<p><img src="/./md%E5%9B%BE/spark.assets/5.jpg"></p>
<p>如图:</p>
<p><img src="/./md%E5%9B%BE/spark.assets/6.jpg"></p>
<p>在这个环境内, 可以运行spark代码</p>
<p>图中的: <code>parallelize</code> 和 <code>map</code> 都是spark提供的API</p>
<p><code>sc.parallelize([1,2,3,4,5]).map(lambda x: x + 1).collect()</code><br>​</p>
<h3 id="WEB-UI-4040"><a href="#WEB-UI-4040" class="headerlink" title="WEB UI (4040)"></a>WEB UI (4040)</h3><p>Spark程序在运行的时候, 会绑定到机器的<code>4040</code>端口上.</p>
<p>如果4040端口被占用, 会顺延到4041 … 4042…<br><img src="/./md%E5%9B%BE/spark.assets/7.jpg"></p>
<p>4040端口是一个WEBUI端口, 可以在浏览器内打开:</p>
<p>输入:<code>服务器ip:4040</code> 即可打开:<br><img src="/./md%E5%9B%BE/spark.assets/8.jpg"></p>
<p>打开监控页面后, 可以发现 在程序内仅有一个Driver</p>
<p>因为我们是Local模式, Driver即管理 又 干活.</p>
<p>同时, 输入jps<br>​</p>
<p><img src="/./md%E5%9B%BE/spark.assets/9.jpg"></p>
<p>可以看到local模式下的唯一进程存在</p>
<p>这个进程 即是master也是worker</p>
<h3 id="bin-x2F-spark-shell-了解"><a href="#bin-x2F-spark-shell-了解" class="headerlink" title="bin&#x2F;spark-shell - 了解"></a>bin&#x2F;spark-shell - 了解</h3><p>同样是一个解释器环境, 和<code>bin/pyspark</code>不同的是, 这个解释器环境 运行的不是python代码, 而是scala程序代码</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">scala&gt; </span><span class="language-bash">sc.parallelize(Array(1,2,3,4,5)).map(x=&gt; x + 1).collect()</span></span><br><span class="line">res0: Array[Int] = Array(2, 3, 4, 5, 6)</span><br></pre></td></tr></table></figure>


<blockquote>
<p>这个仅作为了解即可, 因为这个是用于scala语言的解释器环境</p>
</blockquote>
<h3 id="bin-x2F-spark-submit-PI"><a href="#bin-x2F-spark-submit-PI" class="headerlink" title="bin&#x2F;spark-submit (PI)"></a>bin&#x2F;spark-submit (PI)</h3><p>作用: 提交指定的Spark代码到Spark环境中运行</p>
<p>使用方法:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">语法</span></span><br><span class="line">bin/spark-submit [可选的一些选项] jar包或者python代码的路径 [代码的参数]</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">示例</span></span><br><span class="line">bin/spark-submit /export/server/spark/examples/src/main/python/pi.py 10</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">此案例 运行Spark官方所提供的示例代码 来计算圆周率值.  后面的10 是主函数接受的参数, 数字越高, 计算圆周率越准确.</span></span><br></pre></td></tr></table></figure>


<p>对比</p>
<table>
<thead>
<tr>
<th>功能</th>
<th>bin&#x2F;spark-submit</th>
<th>bin&#x2F;pyspark</th>
<th>bin&#x2F;spark-shell</th>
</tr>
</thead>
<tbody><tr>
<td>功能</td>
<td>提交java\scala\python代码到spark中运行</td>
<td>提供一个<code>python</code></td>
<td></td>
</tr>
<tr>
<td>解释器环境用来以python代码执行spark程序</td>
<td>提供一个<code>scala</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>解释器环境用来以scala代码执行spark程序</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>特点</td>
<td>提交代码用</td>
<td>解释器环境 写一行执行一行</td>
<td>解释器环境 写一行执行一行</td>
</tr>
<tr>
<td>使用场景</td>
<td>正式场合, 正式提交spark程序运行</td>
<td>测试\学习\写一行执行一行\用来验证代码等</td>
<td>测试\学习\写一行执行一行\用来验证代码等</td>
</tr>
</tbody></table>
<h1 id="Spark-StandAlone环境部署"><a href="#Spark-StandAlone环境部署" class="headerlink" title="Spark StandAlone环境部署"></a>Spark StandAlone环境部署</h1><h2 id="新角色-历史服务器"><a href="#新角色-历史服务器" class="headerlink" title="新角色 历史服务器"></a>新角色 历史服务器</h2><blockquote>
<p>历史服务器不是Spark环境的必要组件, 是可选的.</p>
</blockquote>
<blockquote>
<p>回忆: 在YARN中 有一个历史服务器, 功能: 将YARN运行的程序的历史日志记录下来, 通过历史服务器方便用户查看程序运行的历史信息.</p>
</blockquote>
<p>Spark的历史服务器, 功能: 将Spark运行的程序的历史日志记录下来, 通过历史服务器方便用户查看程序运行的历史信息.</p>
<p>搭建集群环境, 我们一般<code>推荐将历史服务器也配置上</code>, 方面以后查看历史记录<br>​</p>
<h2 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h2><p>课程中 使用三台Linux虚拟机来组成集群环境, 非别是:</p>
<p>node1\ node2\ node3</p>
<p>node1运行: Spark的Master进程  和 1个Worker进程</p>
<p>node2运行: spark的1个worker进程</p>
<p>node3运行: spark的1个worker进程</p>
<p>整个集群提供: 1个master进程 和 3个worker进程</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="在所有机器安装Python-Anaconda"><a href="#在所有机器安装Python-Anaconda" class="headerlink" title="在所有机器安装Python(Anaconda)"></a>在所有机器安装Python(Anaconda)</h3><p>参考 附1内容, 如何在Linux上安装anaconda</p>
<p>同时不要忘记 都创建<code>pyspark</code>虚拟环境 以及安装虚拟环境所需要的包<code>pyspark jieba pyhive</code></p>
<h3 id="在所有机器配置环境变量"><a href="#在所有机器配置环境变量" class="headerlink" title="在所有机器配置环境变量"></a>在所有机器配置环境变量</h3><p>参考 Local模式下 环境变量的配置内容</p>
<p><code>确保3台都配置</code></p>
<h3 id="配置配置文件"><a href="#配置配置文件" class="headerlink" title="配置配置文件"></a>配置配置文件</h3><p>进入到spark的配置文件目录中, <code>cd $SPARK_HOME/conf</code></p>
<p>配置workers文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">改名, 去掉后面的.template后缀</span></span><br><span class="line">mv workers.template workers</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">编辑worker文件</span></span><br><span class="line">vim workers</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将里面的localhost删除, 追加</span></span><br><span class="line">node1</span><br><span class="line">node2</span><br><span class="line">node3</span><br><span class="line">到workers文件内</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">功能: 这个文件就是指示了  当前SparkStandAlone环境下, 有哪些worker</span></span><br></pre></td></tr></table></figure>


<p>配置spark-env.sh文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 改名</span></span><br><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2. 编辑spark-env.sh, 在底部追加如下内容</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 设置JAVA安装目录</span></span></span><br><span class="line">JAVA_HOME=/export/server/jdk</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span></span></span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 指定spark老大Master的IP和提交任务的通信端口</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">告知Spark的master运行在哪个机器上</span></span><br><span class="line">export SPARK_MASTER_HOST=node1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">告知sparkmaster的通讯端口</span></span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">告知spark master的 webui端口</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker cpu可用核数</span></span><br><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker可用内存</span></span><br><span class="line">SPARK_WORKER_MEMORY=1g</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker的工作通讯地址</span></span><br><span class="line">SPARK_WORKER_PORT=7078</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker的 webui地址</span></span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 设置历史服务器</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中</span></span><br><span class="line">SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span><br></pre></td></tr></table></figure>


<p>注意, 上面的配置的路径 要根据你自己机器实际的路径来写</p>
<p>在HDFS上创建程序运行历史记录存放的文件夹:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /sparklog</span><br><span class="line">hadoop fs -chmod 777 /sparklog</span><br></pre></td></tr></table></figure>


<p>配置spark-defaults.conf文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 改名</span></span><br><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2. 修改内容, 追加如下内容</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">开启spark的日期记录功能</span></span><br><span class="line">spark.eventLog.enabled 	true</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置spark日志记录的路径</span></span><br><span class="line">spark.eventLog.dir	 hdfs://node1:8020/sparklog/ </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置spark日志是否启动压缩</span></span><br><span class="line">spark.eventLog.compress 	true</span><br></pre></td></tr></table></figure>


<p>配置log4j.properties 文件 [可选配置]</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 改名</span></span><br><span class="line">mv log4j.properties.template log4j.properties</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2. 修改内容 参考下图</span></span><br></pre></td></tr></table></figure>
<p><img src="/./md%E5%9B%BE/spark.assets/10.jpg"></p>
<blockquote>
<p>这个文件的修改不是必须的,  为什么修改为WARN. 因为Spark是个话痨</p>
<p>会疯狂输出日志, 设置级别为WARN 只输出警告和错误日志, 不要输出一堆废话.</p>
</blockquote>
<h3 id="将Spark安装文件夹-分发到其它的服务器上"><a href="#将Spark安装文件夹-分发到其它的服务器上" class="headerlink" title="将Spark安装文件夹  分发到其它的服务器上"></a>将Spark安装文件夹  分发到其它的服务器上</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r spark-3.1.2-bin-hadoop3.2 node2:/export/server/</span><br><span class="line">scp -r spark-3.1.2-bin-hadoop3.2 node3:/export/server/</span><br></pre></td></tr></table></figure>


<p>不要忘记, 在node2和node3上 给spark安装目录增加软链接</p>
<p><code>ln -s /export/server/spark-3.1.2-bin-hadoop3.2 /export/server/spark</code></p>
<h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h3><p>检查每台机器的:</p>
<p>JAVA_HOME</p>
<p>SPARK_HOME</p>
<p>PYSPARK_PYTHON</p>
<p>等等 环境变量是否正常指向正确的目录</p>
<h3 id="启动历史服务器"><a href="#启动历史服务器" class="headerlink" title="启动历史服务器"></a>启动历史服务器</h3><p><code>sbin/start-history-server.sh</code></p>
<h3 id="启动Spark的Master和Worker进程"><a href="#启动Spark的Master和Worker进程" class="headerlink" title="启动Spark的Master和Worker进程"></a>启动Spark的Master和Worker进程</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动全部master和worker</span></span><br><span class="line">sbin/start-all.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">或者可以一个个启动:</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动当前机器的master</span></span><br><span class="line">sbin/start-master.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动当前机器的worker</span></span><br><span class="line">sbin/start-worker.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">停止全部</span></span><br><span class="line">sbin/stop-all.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">停止当前机器的master</span></span><br><span class="line">sbin/stop-master.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">停止当前机器的worker</span></span><br><span class="line">sbin/stop-worker.sh</span><br></pre></td></tr></table></figure>


<h3 id="查看Master的WEB-UI"><a href="#查看Master的WEB-UI" class="headerlink" title="查看Master的WEB UI"></a>查看Master的WEB UI</h3><p>默认端口master我们设置到了8080</p>
<p>如果端口被占用, 会顺延到8081 …;8082… 8083… 直到申请到端口为止</p>
<p>可以在日志中查看, 具体顺延到哪个端口上:</p>
<p><code>Service &#39;MasterUI&#39; could not bind on port 8080. Attempting port 8081.</code><br>​</p>
<p><img src="/./md%E5%9B%BE/spark.assets/11.jpg"></p>
<h3 id="连接到StandAlone集群"><a href="#连接到StandAlone集群" class="headerlink" title="连接到StandAlone集群"></a>连接到StandAlone集群</h3><h4 id="bin-x2F-pyspark-1"><a href="#bin-x2F-pyspark-1" class="headerlink" title="bin&#x2F;pyspark"></a>bin&#x2F;pyspark</h4><p>执行:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/pyspark --master spark://node1:7077</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">通过--master选项来连接到 StandAlone集群</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果不写--master选项, 默认是<span class="built_in">local</span>模式运行</span></span><br></pre></td></tr></table></figure>
<p><img src="/./md%E5%9B%BE/spark.assets/12.jpg"></p>
<h4 id="bin-x2F-spark-shell"><a href="#bin-x2F-spark-shell" class="headerlink" title="bin&#x2F;spark-shell"></a>bin&#x2F;spark-shell</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --master spark://node1:7077</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">同样适用--master来连接到集群使用</span></span><br></pre></td></tr></table></figure>


<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 测试代码</span></span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)).map(x=&gt; x + <span class="number">1</span>).collect()</span><br></pre></td></tr></table></figure>


<h4 id="bin-x2F-spark-submit-PI-1"><a href="#bin-x2F-spark-submit-PI-1" class="headerlink" title="bin&#x2F;spark-submit (PI)"></a>bin&#x2F;spark-submit (PI)</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master spark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">同样使用--master来指定将任务提交到集群运行</span></span><br></pre></td></tr></table></figure>


<h3 id="查看历史服务器WEB-UI"><a href="#查看历史服务器WEB-UI" class="headerlink" title="查看历史服务器WEB UI"></a>查看历史服务器WEB UI</h3><p>历史服务器的默认端口是: 18080</p>
<p>我们启动在node1上, 可以在浏览器打开:</p>
<p><code>node1:18080</code>来进入到历史服务器的WEB UI上.<br><img src="/./md%E5%9B%BE/spark.assets/13.jpg"></p>
<h1 id="Spark-StandAlone-HA-环境搭建"><a href="#Spark-StandAlone-HA-环境搭建" class="headerlink" title="Spark StandAlone HA 环境搭建"></a>Spark StandAlone HA 环境搭建</h1><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><blockquote>
<p>前提: 确保Zookeeper 和 HDFS 均已经启动</p>
</blockquote>
<p>先在<code>spark-env.sh</code>中, 删除: <code>SPARK_MASTER_HOST=node1</code></p>
<p>原因: 配置文件中固定master是谁, 那么就无法用到zk的动态切换master功能了.</p>
<p>在<code>spark-env.sh</code>中, 增加:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir=/spark-ha&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定Zookeeper的连接地址</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定在Zookeeper中注册临时节点的路径</span></span><br></pre></td></tr></table></figure>


<p>将spark-env.sh 分发到每一台服务器上</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp spark-env.sh node2:/export/server/spark/conf/</span><br><span class="line">scp spark-env.sh node3:/export/server/spark/conf/</span><br></pre></td></tr></table></figure>


<p>停止当前StandAlone集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/stop-all.sh</span><br></pre></td></tr></table></figure>


<p>启动集群:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在node1上 启动一个master 和全部worker</span></span><br><span class="line">sbin/start-all.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">注意, 下面命令在node2上执行</span></span><br><span class="line">sbin/start-master.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在node2上启动一个备用的master进程</span></span><br></pre></td></tr></table></figure>
<p><img src="/./md%E5%9B%BE/spark.assets/14.jpg"><br><img src="/./md%E5%9B%BE/spark.assets/15.jpg"></p>
<h2 id="master主备切换"><a href="#master主备切换" class="headerlink" title="master主备切换"></a>master主备切换</h2><p>提交一个spark任务到当前<code>alive</code>master上:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master spark://node1:7077 /export/server/spark/examples/src/main/python/pi.py 1000</span><br></pre></td></tr></table></figure>


<p>在提交成功后, 将alivemaster直接kill掉</p>
<p>不会影响程序运行:<br><img src="/./md%E5%9B%BE/spark.assets/16.jpg"><br>当新的master接收集群后, 程序继续运行, 正常得到结果.</p>
<blockquote>
<p>结论 HA模式下, 主备切换 不会影响到正在运行的程序.</p>
<p>最大的影响是 会让它中断大约30秒左右.</p>
</blockquote>
<h1 id="Spark-On-YARN-环境搭建"><a href="#Spark-On-YARN-环境搭建" class="headerlink" title="Spark On YARN 环境搭建"></a>Spark On YARN 环境搭建</h1><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>确保:</p>
<ul>
<li>HADOOP_CONF_DIR</li>
<li>YARN_CONF_DIR</li>
</ul>
<p>在spark-env.sh 以及 环境变量配置文件中即可<br>​</p>
<h2 id="连接到YARN中"><a href="#连接到YARN中" class="headerlink" title="连接到YARN中"></a>连接到YARN中</h2><h3 id="bin-x2F-pyspark-2"><a href="#bin-x2F-pyspark-2" class="headerlink" title="bin&#x2F;pyspark"></a>bin&#x2F;pyspark</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/pyspark --master yarn --deploy-mode client|cluster</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">--deploy-mode 选项是指定部署模式, 默认是 客户端模式</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">client就是客户端模式</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">cluster就是集群模式</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">--deploy-mode 仅可以用在YARN模式下</span></span><br></pre></td></tr></table></figure>


<blockquote>
<p>注意: 交互式环境 pyspark  和 spark-shell  无法运行 cluster模式</p>
</blockquote>
<h3 id="bin-x2F-spark-shell-1"><a href="#bin-x2F-spark-shell-1" class="headerlink" title="bin&#x2F;spark-shell"></a>bin&#x2F;spark-shell</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --master yarn --deploy-mode client|cluster</span><br></pre></td></tr></table></figure>


<blockquote>
<p>注意: 交互式环境 pyspark  和 spark-shell  无法运行 cluster模式</p>
</blockquote>
<h3 id="bin-x2F-spark-submit-PI-2"><a href="#bin-x2F-spark-submit-PI-2" class="headerlink" title="bin&#x2F;spark-submit (PI)"></a>bin&#x2F;spark-submit (PI)</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master yarn --deploy-mode client|cluster /xxx/xxx/xxx.py 参数</span><br></pre></td></tr></table></figure>


<h2 id="spark-submit-和-spark-shell-和-pyspark的相关参数"><a href="#spark-submit-和-spark-shell-和-pyspark的相关参数" class="headerlink" title="spark-submit 和 spark-shell 和 pyspark的相关参数"></a>spark-submit 和 spark-shell 和 pyspark的相关参数</h2><p>参见: 附2<br>​</p>
<h1 id="附1-Anaconda-On-Linux-安装-单台服务器"><a href="#附1-Anaconda-On-Linux-安装-单台服务器" class="headerlink" title="附1 Anaconda On Linux 安装 (单台服务器)"></a>附1 Anaconda On Linux 安装 (单台服务器)</h1><h2 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h2><p>上传安装包:</p>
<p>上传: 资料中提供的<code>Anaconda3-2021.05-Linux-x86_64.sh</code>文件到Linux服务器上</p>
<p>安装:</p>
<p><code>sh ./Anaconda3-2021.05-Linux-x86_64.sh</code><br><img src="/./md%E5%9B%BE/spark.assets/17.jpg"><br><img src="/./md%E5%9B%BE/spark.assets/18.jpg"><br><img src="/./md%E5%9B%BE/spark.assets/19.jpg"><br><img src="/./md%E5%9B%BE/spark.assets/20.jpg"><br><img src="/./md%E5%9B%BE/spark.assets/21.jpg"><br>输入yes后就安装完成了.</p>
<p>安装完成后, <code>退出finalshell 重新进来</code>:<br><img src="/./md%E5%9B%BE/spark.assets/22.jpg"></p>
<p>看到这个Base开头表明安装好了.</p>
<p>base是默认的虚拟环境.<br>​</p>
<h2 id="国内源"><a href="#国内源" class="headerlink" title="国内源"></a>国内源</h2><p>如果你安装好后, 没有出现base, 可以打开:&#x2F;root&#x2F;.condarc这个文件, 追加如下内容:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">default_channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure>


<h1 id="附2-spark-submit和pyspark相关参数"><a href="#附2-spark-submit和pyspark相关参数" class="headerlink" title="附2 spark-submit和pyspark相关参数"></a>附2 spark-submit和pyspark相关参数</h1><p>客户端工具我们可以用的有:</p>
<ul>
<li>bin&#x2F;pyspark: pyspark解释器spark环境</li>
<li>bin&#x2F;spark-shell: scala解释器spark环境</li>
<li>bin&#x2F;spark-submit: 提交jar包或Python文件执行的工具</li>
<li>bin&#x2F;spark-sql: sparksql客户端工具</li>
</ul>
<p>这4个客户端工具的参数基本通用.</p>
<p>以spark-submit 为例:</p>
<p><code>bin/spark-submit --master spark://node1:7077 xxx.py</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]</span><br><span class="line">Usage: spark-submit --kill [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit --status [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit run-example [options] example-class [example args]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,</span><br><span class="line">                              k8s://https://host:port, or local (Default: local[*]).</span><br><span class="line">  --deploy-mode DEPLOY_MODE   部署模式 client 或者 cluster 默认是client</span><br><span class="line">  --class CLASS_NAME          运行java或者scala class(for Java / Scala apps).</span><br><span class="line">  --name NAME                 程序的名字</span><br><span class="line">  --jars JARS                 Comma-separated list of jars to include on the driver</span><br><span class="line">                              and executor classpaths.</span><br><span class="line">  --packages                  Comma-separated list of maven coordinates of jars to include</span><br><span class="line">                              on the driver and executor classpaths. Will search the local</span><br><span class="line">                              maven repo, then maven central and any additional remote</span><br><span class="line">                              repositories given by --repositories. The format for the</span><br><span class="line">                              coordinates should be groupId:artifactId:version.</span><br><span class="line">  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while</span><br><span class="line">                              resolving the dependencies provided in --packages to avoid</span><br><span class="line">                              dependency conflicts.</span><br><span class="line">  --repositories              Comma-separated list of additional remote repositories to</span><br><span class="line">                              search for the maven coordinates given with --packages.</span><br><span class="line">  --py-files PY_FILES         指定Python程序依赖的其它python文件</span><br><span class="line">  --files FILES               Comma-separated list of files to be placed in the working</span><br><span class="line">                              directory of each executor. File paths of these files</span><br><span class="line">                              in executors can be accessed via SparkFiles.get(fileName).</span><br><span class="line">  --archives ARCHIVES         Comma-separated list of archives to be extracted into the</span><br><span class="line">                              working directory of each executor.</span><br><span class="line"></span><br><span class="line">  --conf, -c PROP=VALUE       手动指定配置</span><br><span class="line">  --properties-file FILE      Path to a file from which to load extra properties. If not</span><br><span class="line">                              specified, this will look for conf/spark-defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM         Driver的可用内存(Default: 1024M).</span><br><span class="line">  --driver-java-options       Driver的一些Java选项</span><br><span class="line">  --driver-library-path       Extra library path entries to pass to the driver.</span><br><span class="line">  --driver-class-path         Extra class path entries to pass to the driver. Note that</span><br><span class="line">                              jars added with --jars are automatically included in the</span><br><span class="line">                              classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM       Executor的内存 (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME           User to impersonate when submitting the application.</span><br><span class="line">                              This argument does not work with --principal / --keytab.</span><br><span class="line"></span><br><span class="line">  --help, -h                  显示帮助文件</span><br><span class="line">  --verbose, -v               Print additional debug output.</span><br><span class="line">  --version,                  打印版本</span><br><span class="line"></span><br><span class="line"> Cluster deploy mode only(集群模式专属):</span><br><span class="line">  --driver-cores NUM          Driver可用的的CPU核数(Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                 如果给定, 可以尝试重启Driver</span><br><span class="line"></span><br><span class="line"> Spark standalone, Mesos or K8s with cluster deploy mode only:</span><br><span class="line">  --kill SUBMISSION_ID        指定程序ID kill</span><br><span class="line">  --status SUBMISSION_ID      指定程序ID 查看运行状态</span><br><span class="line"></span><br><span class="line"> Spark standalone, Mesos and Kubernetes only:</span><br><span class="line">  --total-executor-cores NUM  整个任务可以给Executor多少个CPU核心用</span><br><span class="line"></span><br><span class="line"> Spark standalone, YARN and Kubernetes only:</span><br><span class="line">  --executor-cores NUM        单个Executor能使用多少CPU核心</span><br><span class="line"></span><br><span class="line"> Spark on YARN and Kubernetes only(YARN模式下):</span><br><span class="line">  --num-executors NUM         Executor应该开启几个</span><br><span class="line">  --principal PRINCIPAL       Principal to be used to login to KDC.</span><br><span class="line">  --keytab KEYTAB             The full path to the file that contains the keytab for the</span><br><span class="line">                              principal specified above.</span><br><span class="line"></span><br><span class="line"> Spark on YARN only:</span><br><span class="line">  --queue QUEUE_NAME          指定运行的YARN队列(Default: &quot;default&quot;).</span><br></pre></td></tr></table></figure>




<h1 id="附3-Windows系统配置Anaconda"><a href="#附3-Windows系统配置Anaconda" class="headerlink" title="附3 Windows系统配置Anaconda"></a>附3 Windows系统配置Anaconda</h1><h2 id="安装-2"><a href="#安装-2" class="headerlink" title="安装"></a>安装</h2><p>打开资料中提供的:Anaconda3-2021.05-Windows-x86_64.exe文件,或者去官网下载:[<a target="_blank" rel="noopener" href="https://www.anaconda.com/products/individual#Downloads]">https://www.anaconda.com/products/individual#Downloads]</a><br>​</p>
<p>打开后,一直点击<code>Next</code>下一步即可:<br><img src="/./md%E5%9B%BE/spark.assets/23.jpg" alt="image.png"><br><img src="/./md%E5%9B%BE/spark.assets/24.jpg" alt="image.png"><br>如果想要修改安装路径, 可以修改<br><img src="/./md%E5%9B%BE/spark.assets/25.jpg" alt="image.png"><br>不必勾选<br><img src="/./md%E5%9B%BE/spark.assets/26.jpg" alt="image.png"><br>最终点击Finish完成安装</p>
<p>打开开始菜单, 搜索Anaconda<br><img src="/./md%E5%9B%BE/spark.assets/27.jpg" alt="image.png"><br>出现如图的程序, 安装成功.</p>
<p>打开 <code>Anaconda Prompt</code>程序:<br><img src="/./md%E5%9B%BE/spark.assets/28.jpg" alt="image.png"><br>出现<code>base</code>说明安装正确.</p>
<h2 id="配置国内源"><a href="#配置国内源" class="headerlink" title="配置国内源"></a>配置国内源</h2><p>Anaconda默认源服务器在国外, 网速比较慢, 配置国内源加速网络下载.<br>​</p>
<p>打开上图中的 <code>Anaconda Prompt</code>程序:<br>执行:<br><code>conda config --set show_channel_urls yes</code><br>​</p>
<p>然后用记事本打开:<br><code>C:\Users\用户名\.condarc</code>文件, 将如下内容替换进文件内,保存即可:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">default_channels:</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span><br><span class="line">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br><span class="line">  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span><br></pre></td></tr></table></figure>


<h2 id="创建虚拟环境"><a href="#创建虚拟环境" class="headerlink" title="创建虚拟环境"></a>创建虚拟环境</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建虚拟环境 pyspark, 基于Python 3.8</span></span><br><span class="line">conda create -n pyspark python=3.8</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">切换到虚拟环境内</span></span><br><span class="line">conda activate pyspark</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在虚拟环境内安装包</span></span><br><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple </span><br></pre></td></tr></table></figure>










      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/19/Spark%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/" data-id="clj25kfyc0005n0urcgsvc5lz" data-title="Spark部署" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Kafka" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/19/Kafka/" class="article-date">
  <time class="dt-published" datetime="2023-06-19T00:49:49.449Z" itemprop="datePublished">2023-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/19/Kafka/">Kafka</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="一、Kafka简介"><a href="#一、Kafka简介" class="headerlink" title="一、Kafka简介"></a>一、Kafka简介</h2><h3 id="1-1-什么是kafka"><a href="#1-1-什么是kafka" class="headerlink" title="1.1 什么是kafka"></a>1.1 什么是kafka</h3><p>Kafka 它最初由LinkedIn 公司开发，之后成为Apache 项目的一部分。<br>Kafka 是一个<code>分布式消息中间件,支持分区的、多副本的、多订阅者的</code>、<code>基于zookeeper 协调的</code>分布式<br>消息系统。<br>通俗来说：<code> kafka 就是一个存储系统，存储的数据形式为“消息&quot;；</code><br>它的主要作用类似于蓄水池，起到一个<code>缓冲</code>作用；</p>
<img src=".\md图\kafka.assets\image-20230204221620734.png" alt="image-20230204221620734" style="zoom:50%;" />

<p>Kafka是由Apache软件基金会开发的一个开源流平台，由Scala和Java编写。Kafka的Apache官网是这样介绍Kakfa的。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Apache Kafka是一个分布式流平台。一个分布式的流平台应该包含3点关键的能力：</span><br><span class="line">1. 发布和订阅流数据流，类似于消息队列或者是企业消息传递系统</span><br><span class="line">2. 以容错的持久化方式存储数据流</span><br><span class="line">3. 处理数据流</span><br><span class="line"></span><br><span class="line">英文原版：</span><br><span class="line">-Publish and subscribe to streams of records, similar to a message queue or enterprise </span><br><span class="line">messaging system.</span><br><span class="line">-Store streams of records in a fault-tolerant durable way.</span><br><span class="line">-Process streams of records as they occur.</span><br><span class="line"></span><br><span class="line">我们重点关键三个部分的关键词：</span><br><span class="line">1. Publish and subscribe：发布与订阅</span><br><span class="line">2. Store：存储</span><br><span class="line">3. Process：处理</span><br><span class="line">后续我们的课程主要围绕这三点来讲解。</span><br></pre></td></tr></table></figure>

<h4 id="1-1-1-消息队列"><a href="#1-1-1-消息队列" class="headerlink" title="1.1.1 消息队列"></a>1.1.1 消息队列</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">消息队列常用于两个系统之间的数据传递；</span><br><span class="line"></span><br><span class="line">-消息队列——用于存放消息的组件</span><br><span class="line"></span><br><span class="line">-程序员可以将消息放入到队列中，也可以从消息队列中获取消息</span><br><span class="line"></span><br><span class="line">-很多时候消息队列不是一个永久性的存储，是作为临时存储存在的（设定一个期限：设置消息在MQ中保存10天）</span><br><span class="line"></span><br><span class="line">-消息队列中间件：常见的消息队列组件，例如：Kafka、Active MQ、RabbitMQ、RocketMQ、ZeroMQ</span><br></pre></td></tr></table></figure>



<h3 id="1-2Kafka的应用场景"><a href="#1-2Kafka的应用场景" class="headerlink" title="1.2Kafka的应用场景"></a>1.2Kafka的应用场景</h3><h4 id="（1）应用场景"><a href="#（1）应用场景" class="headerlink" title="（1）应用场景"></a>（1）应用场景</h4><p>我们通常将Apache Kafka用在两类程序：</p>
<ol>
<li><p>建立实时数据管道，以可靠地在系统或应用程序之间获取数据</p>
</li>
<li><p>构建实时流应用程序，以转换或响应数据流</p>
</li>
</ol>
<img src=".\md图\kafka.assets\image-20230204221832864.png" alt="image-20230204221832864" style="zoom:80%;" />

<p>上图，我们可以看到：</p>
<ol>
<li><p><code>Producers：</code>可以有很多的应用程序，将消息数据放入到Kafka集群中。</p>
</li>
<li><p><code>Consumers：</code>可以有很多的应用程序，将消息数据从Kafka集群中拉取出来。</p>
</li>
<li><p><code>Connectors：</code>Kafka的连接器可以将数据库中的数据导入到Kafka，也可以将Kafka的数据导出到</p>
</li>
</ol>
<p>数据库中。</p>
<ol start="4">
<li><code>Stream Processors：</code>流处理器可以Kafka中拉取数据，也可以将数据写入到Kafka中。</li>
</ol>
<h4 id="（2）kafka诞生背景"><a href="#（2）kafka诞生背景" class="headerlink" title="（2）kafka诞生背景"></a>（2）kafka诞生背景</h4><p>kafka的诞生，是为了解决<code>linkedin的数据管道问题</code>，起初linkedin采用了<code>ActiveMQ来进行数据交换</code>，大约是在2010年前后，那时的ActiveMQ还<code>远远无法满足linkedin对数据传递系统的要求</code>，经常由于各种缺陷而导致<code>消息阻塞</code>或者<code>服务无法正常访问</code>，为了能够解决这个问题，linkedin决定研发自己的消息传递系统，当时linkedin的首席架构师jay kreps便开始组织团队进行<code>消息传递系统</code>的研发。</p>
<p>提示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. Linkedin还是挺厉害的</span><br><span class="line">2. Kafka比ActiveMQ厉害得多</span><br></pre></td></tr></table></figure>



<ul>
<li><code>异步处理</code><ul>
<li>可以将一些比较耗时的操作放在其他系统中，通过消息队列将需要进行处理的消息进行存储，其他系统可以消费消息队列中的数据</li>
<li>比较常见的：发送短信验证码、发送邮件</li>
</ul>
</li>
</ul>
<p><img src="/md%E5%9B%BE/kafka.assets/image-20200916093856262.png" alt="image-20200916093856262"></p>
<ul>
<li><code>系统解耦</code><ul>
<li>原先一个微服务是通过接口（HTTP）调用另一个微服务，这时候耦合很严重，只要接口发生变化就会导致系统不可用</li>
<li>使用消息队列可以将系统进行解耦合，现在第一个微服务可以将消息放入到消息队列中，另一个微服务可以从消息队列中把消息取出来进行处理。进行系统解耦</li>
</ul>
</li>
</ul>
<p><img src="/md%E5%9B%BE/kafka.assets/image-20200916093908261.png" alt="image-20200916093908261"></p>
<ul>
<li><code>流量削峰</code><ul>
<li>因为消息队列是低延迟、高可靠、高吞吐的，可以应对大量并发</li>
</ul>
</li>
</ul>
<p><img src="/md%E5%9B%BE/kafka.assets/image-20200916093919754.png" alt="image-20200916093919754"></p>
<ul>
<li><code>日志处理</code><ul>
<li>可以使用消息队列作为临时存储，或者一种通信管道</li>
</ul>
</li>
</ul>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203213716547.png" alt="image-20230203213716547"></p>
<h3 id="1-3消息队列的两种模型"><a href="#1-3消息队列的两种模型" class="headerlink" title="1.3消息队列的两种模型"></a>1.3消息队列的两种模型</h3><ul>
<li>生产者、消费者模型<ul>
<li>生产者负责将消息生产到MQ中</li>
<li>消费者负责从MQ中获取消息</li>
<li>生产者和消费者是解耦的，可能是生产者一个程序、消费者是另外一个程序</li>
</ul>
</li>
<li>消息队列的模式<ul>
<li>点对点：一个消费者消费一个消息</li>
<li>发布订阅：多个消费者可以消费一个消息</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">分布式消息传递基于可靠的消息队列，在客户端应用和消息系统之间异步传递消息。</span><br><span class="line"></span><br><span class="line">有两种主要的消息传递模式：点对点传递模式、发布-订阅模式。</span><br></pre></td></tr></table></figure>

<img src=".\md图\kafka.assets\image-20230203212920000.png" alt="image-20230203212920000" style="zoom:80%;" />

<img src=".\md图\kafka.assets\image-20230203213006782.png" alt="image-20230203213006782" style="zoom:80%;" />

<p><strong>大部分的消息系统选用发布-订阅模式。</strong><br><strong>kafka 是发布-订阅模式。</strong></p>
<h3 id="1-4-kafka的特点与优势"><a href="#1-4-kafka的特点与优势" class="headerlink" title="1.4 kafka的特点与优势"></a>1.4 kafka的特点与优势</h3><h4 id="（1）特点"><a href="#（1）特点" class="headerlink" title="（1）特点"></a>（1）特点</h4><ol>
<li>高吞吐量、低延迟：kafka 每秒可以处理几十万条消息，它的延迟最低只有几毫秒，每个topic 可</li>
<li>以分多个partition, 由多个consumer group 对partition 进行consume 操作。</li>
<li>可扩展性：kafka 集群支持热扩展</li>
<li>持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失</li>
<li>容错性：允许集群中节点失败（若副本数量为n,则允许n-1 个节点失败）</li>
<li>高并发：支持数千个客户端同时读写</li>
</ol>
<h4 id="（2）优势"><a href="#（2）优势" class="headerlink" title="（2）优势"></a>（2）优势</h4><p>前面我们了解到，消息队列中间件有很多，为什么我们要选择Kafka？</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>ActiveMQ</th>
<th>RabbitMQ</th>
<th>Kafka</th>
<th>RocketMQ</th>
</tr>
</thead>
<tbody><tr>
<td>所属社区&#x2F;公司</td>
<td>Apache</td>
<td>Mozilla Public License</td>
<td>Apache</td>
<td>Apache&#x2F;Ali</td>
</tr>
<tr>
<td>成熟度</td>
<td>成熟</td>
<td>成熟</td>
<td>成熟</td>
<td>比较成熟</td>
</tr>
<tr>
<td>生产者-消费者模式</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>发布-订阅</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>REQUEST-REPLY</td>
<td>支持</td>
<td>支持</td>
<td>-</td>
<td>支持</td>
</tr>
<tr>
<td>API完备性</td>
<td>高</td>
<td>高</td>
<td>高</td>
<td>低（静态配置）</td>
</tr>
<tr>
<td>多语言支持</td>
<td>支持JAVA优先</td>
<td>语言无关</td>
<td>支持，JAVA优先</td>
<td>支持</td>
</tr>
<tr>
<td>单机呑吐量</td>
<td>万级（最差）</td>
<td>万级</td>
<td><strong>十万级</strong></td>
<td>十万级（最高）</td>
</tr>
<tr>
<td>消息延迟</td>
<td>-</td>
<td>微秒级</td>
<td><strong>毫秒级</strong></td>
<td>-</td>
</tr>
<tr>
<td>可用性</td>
<td>高（主从）</td>
<td>高（主从）</td>
<td><strong>非常高（分布式）</strong></td>
<td>高</td>
</tr>
<tr>
<td>消息丢失</td>
<td>-</td>
<td>低</td>
<td><strong>理论上不会丢失</strong></td>
<td>-</td>
</tr>
<tr>
<td>消息重复</td>
<td>-</td>
<td>可控制</td>
<td>理论上会有重复</td>
<td>-</td>
</tr>
<tr>
<td>事务</td>
<td>支持</td>
<td>不支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>文档的完备性</td>
<td>高</td>
<td>高</td>
<td>高</td>
<td>中</td>
</tr>
<tr>
<td>提供快速入门</td>
<td>有</td>
<td>有</td>
<td>有</td>
<td>无</td>
</tr>
<tr>
<td>首次部署难度</td>
<td>-</td>
<td>低</td>
<td>中</td>
<td>高</td>
</tr>
</tbody></table>
<h3 id="1-5kafka的使用场景"><a href="#1-5kafka的使用场景" class="headerlink" title="1.5kafka的使用场景"></a>1.5kafka的使用场景</h3><h4 id="（1）使用场景"><a href="#（1）使用场景" class="headerlink" title="（1）使用场景"></a>（1）使用场景</h4><p><strong>主要用于数据处理系统中的缓冲！（尤其是实时流式数据处理）</strong></p>
<ol>
<li><p><code>日志收集</code>：一个公司可以用kafka 可以收集各种服务的log，通过kafka 以统一接口服务的方式开</p>
<p>放给各种consumer，例如hadoop、HBase、Solr 等。</p>
</li>
<li><p><code>消息系统：</code>解耦和生产者和消费者、缓存消息等。</p>
</li>
<li><p><code>用户活动跟踪：</code>kafka 经常被用来记录web 用户或者app 用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka 的topic 中，然后订阅者通过订阅这些topic</p>
<p>来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。</p>
</li>
<li><p><code>运营指标：</code>kafka 也经常用来记录运维监控数据。包括收集各种分布式应用的数据，各种操作的</p>
<p>集中反馈，比如报警和报告。</p>
</li>
<li><p><code>流式数据处理：</code>比如spark streaming 和Flink</p>
</li>
</ol>
<h4 id="（2）哪些公司在使用kafka"><a href="#（2）哪些公司在使用kafka" class="headerlink" title="（2）哪些公司在使用kafka"></a>（2）哪些公司在使用kafka</h4><img src=".\md图\kafka.assets\image-20230204222605927.png" alt="image-20230204222605927" style="zoom: 50%;" />

<img src=".\md图\kafka.assets\image-20230204222623936.png" alt="image-20230204222623936" style="zoom:50%;" />

<h4 id="（3）kafka生态圈"><a href="#（3）kafka生态圈" class="headerlink" title="（3）kafka生态圈"></a>（3）kafka生态圈</h4><p>Apache Kafka这么多年的发展，目前也有一个较庞大的生态圈。</p>
<p>Kafka生态圈官网地址：<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem">https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem</a></p>
<img src=".\md图\kafka.assets\image-20230204222917611.png" alt="image-20230204222917611" style="zoom: 67%;" />



<h2 id="二、Kafka系统架构（基础重点）"><a href="#二、Kafka系统架构（基础重点）" class="headerlink" title="二、Kafka系统架构（基础重点）"></a>二、Kafka系统架构（基础重点）</h2><p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203214337255.png" alt="image-20230203214337255"></p>
<p><strong>Kafka 架构分为以下几个部分</strong></p>
<h3 id="2-1-producer"><a href="#2-1-producer" class="headerlink" title="2.1 producer"></a>2.1 producer</h3><p>消息生产者，就是向kafka broker 发消息的客户端。</p>
<h3 id="2-2-consumer"><a href="#2-2-consumer" class="headerlink" title="2.2 consumer"></a>2.2 consumer</h3><p>consumer ：消息消费者，从kafka broker 取消息的客户端。<br>consumer group：单个或多个consumer 可以组成一个consumer group；这是kafka 用来实现消息的广<br>播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic 可以有多个ConsumerGroup。</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203214453794.png" alt="image-20230203214453794"></p>
<h3 id="2-3-topic"><a href="#2-3-topic" class="headerlink" title="2.3 topic"></a>2.3 topic</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据的逻辑分类；</span><br><span class="line">可以理解为数据库中“表&quot;的概念；</span><br></pre></td></tr></table></figure>

<h4 id="1-partition"><a href="#1-partition" class="headerlink" title="1.partition"></a>1.<strong>partition</strong></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">topic 中数据的具体管理单元；（可以理解为hbase 中表的“region&quot;概念）</span><br><span class="line"></span><br><span class="line">一个topic 可以划分为多个partition，分布到多个broker 上管理；</span><br><span class="line">每个partition 由一个kafka broker 服务器管理；</span><br><span class="line">partition 中的每条消息都会被分配一个递增的id（offset）；</span><br><span class="line">每个partition 是一个有序的队列，kafka 只保证按一个partition 中的消息的顺序，不保证一个topic的整体（多个partition 间）的顺序。</span><br><span class="line">每个partition 都可以有多个副本；</span><br></pre></td></tr></table></figure>

<h4 id="2-broker"><a href="#2-broker" class="headerlink" title="2.broker"></a>2.<strong>broker</strong></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">一台kafka 服务器就是一个broker。</span><br><span class="line">一个kafka 集群由多个broker 组成。</span><br><span class="line">一个broker 可以容纳多个topic 的多个partition。</span><br><span class="line">分区对于kafka 集群的好处是：实现topic 数据的负载均衡。分区对于消费者来说，可以提高并发度，</span><br><span class="line">提高效率。</span><br></pre></td></tr></table></figure>

<h4 id="3-offset"><a href="#3-offset" class="headerlink" title="3.offset"></a>3.offset</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">消息在底层存储中的索引位置，kafka 底层的存储文件就是以文件中第一条消息的offset 来命名的，</span><br><span class="line">通过offset 可以快速定位到消息的具体存储位置；</span><br></pre></td></tr></table></figure>

<h3 id="2-4-Leader"><a href="#2-4-Leader" class="headerlink" title="2.4 Leader"></a>2.4 Leader</h3><p><code>partition replica</code> 中的一个角色，producer 和consumer 只跟leader 交互（负责读写）。</p>
<h3 id="2-5-副本Replica"><a href="#2-5-副本Replica" class="headerlink" title="2.5 副本Replica"></a>2.5 副本Replica</h3><p>partition 的副本，保障partition 的高可用（replica 副本数目不能大于kafka broker 节点的数目，否则报<br>错。<br>每个partition 的所有副本中，必包括一个leader 副本，其他的就是follower 副本</p>
<h3 id="2-6-Follower"><a href="#2-6-Follower" class="headerlink" title="2.6 Follower"></a>2.6 Follower</h3><p>partition replica 中的一个角色，从leader 中拉取复制数据（只负责备份）。<br>如果leader 所在节点宕机，follower 中会选举出新的leader；</p>
<h3 id="2-7-偏移量Offset"><a href="#2-7-偏移量Offset" class="headerlink" title="2.7 偏移量Offset"></a>2.7 偏移量Offset</h3><p>每一条数据都有一个offset，是数据在该partition 中的唯一标识（其实就是消息的索引号）。<br>各个consumer 会保存其消费到的offset 位置，这样下次可以从该offset 位置开始继续消费；<br>consumer 的消费offset 保存在一个专门的topic（__consumer_offsets）中；（0.10.x 版本以前是保存在zk 中）</p>
<h3 id="2-8-消息Message"><a href="#2-8-消息Message" class="headerlink" title="2.8 消息Message"></a>2.8 消息Message</h3><p>在客户端编程代码中，消息的类叫做ProducerRecord； ConsumerRecord；<br>简单来说，kafka 中的每个massage 由一对key-value 构成<br>Kafka 中的message 格式经历了3 个版本的变化了：version0 、version1 、version2</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203215042738.png" alt="image-20230203215042738"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">各个字段的含义介绍如下：</span><br><span class="line">crc：占用4 个字节，主要用于校验消息的内容；</span><br><span class="line"></span><br><span class="line">magic：这个占用1 个字节，主要用于标识Kafka 版本。Kafka 0.10.x magic 默认值为1</span><br><span class="line"></span><br><span class="line">attributes：占用1 个字节，这里面存储了消息压缩使用的编码以及Timestamp 类型。目前Kafka 支</span><br><span class="line">持gzip、snappy 以及lz4（0.8.2 引入） 三种压缩格式；后四位如果是0001 则表示gzip 压缩，</span><br><span class="line">如果是0010 则是snappy 压缩，如果是0011 则是lz4 压缩，如果是0000 则表示没有使用压缩。</span><br><span class="line">第4 个bit 位如果为0，代表使用create time；如果为1 代表append time；其余位（第5~8 位）保</span><br><span class="line">留；</span><br><span class="line"></span><br><span class="line">key length：占用4 个字节。主要标识Key 的内容的长度；</span><br><span class="line"></span><br><span class="line">key：占用N 个字节，存储的是key 的具体内容；</span><br><span class="line"></span><br><span class="line">value length：占用4 个字节。主要标识value 的内容的长度；</span><br><span class="line"></span><br><span class="line">value：value 即是消息的真实内容，在Kafka 中这个也叫做payload。</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203215333470.png" alt="image-20230203215333470"></p>
<h2 id="三、kafka-的数据存储结构"><a href="#三、kafka-的数据存储结构" class="headerlink" title="三、kafka 的数据存储结构"></a>三、kafka 的数据存储结构</h2><h3 id="3-1-kafka-的整体存储结构"><a href="#3-1-kafka-的整体存储结构" class="headerlink" title="3.1 kafka 的整体存储结构"></a>3.1 kafka 的整体存储结构</h3><p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203215459084.png" alt="image-20230203215459084"></p>
<h3 id="3-2-服务器存储结构示例"><a href="#3-2-服务器存储结构示例" class="headerlink" title="3.2 服务器存储结构示例"></a>3.2 服务器存储结构示例</h3><p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203215550273.png" alt="image-20230203215550273"></p>
<p><strong>注：“t1”即为一个topic 的名称；</strong><br><strong>而“t1-0 &#x2F; t1-1”则表明这个目录是t1 这个topic 的哪个partition；</strong></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203215645133.png" alt="image-20230203215645133"></p>
<p>由于生产者生产的消息会不断追加到log 文件末尾，为防止log 文件过大导致数据定位效率低下，<br>Kafka 采取了分片和索引机制，将每个partition 分为多个segment。每个segment 对应两个文件：<br>“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic 名称-<br>分区序号。</p>
<p>index 和log 文件以当前segment 的第一条消息的offset 命名。</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203215709598.png" alt="image-20230203215709598"></p>
<p>“.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中message 的物理偏移地址。</p>
<p>Kafka 中的索引文件以稀疏索引（ sparse index ）的方式构造消息的索引，它并不保证每个消息在<br>索引文件中都有对应的索引；每当写入一定量（由broker 端参数log.index.interval.bytes 指定，<br>默认值为4096 ，即4KB ）的消息时，偏移量索引文件和时间戳索引文件分别增加一个偏移量索引<br>项和时间戳索引项，增大或减小log.index.interval.bytes 的值，对应地可以增加或缩小索引项的<br>密度；<br>偏移量索引文件中的偏移量是单调递增的，查询指定偏移量时，使用二分查找法来快速定位偏移量的<br>位置。</p>
<h2 id="四、Kafka集群搭建"><a href="#四、Kafka集群搭建" class="headerlink" title="四、Kafka集群搭建"></a>四、Kafka集群搭建</h2><h3 id="4-0-kafka版本"><a href="#4-0-kafka版本" class="headerlink" title="4.0 kafka版本"></a>4.0 kafka版本</h3><p>本次课程使用的Kafka版本为2.4.1，是2020年3月12日发布的版本。</p>
<p>可以注意到Kafka的版本号为：kafka_2.12-2.4.1，因为kafka主要是使用scala语言开发的，2.12为scala的版本号。<a target="_blank" rel="noopener" href="http://kafka.apache.org/downloads%E5%8F%AF%E4%BB%A5%E6%9F%A5%E7%9C%8B%E5%88%B0%E6%AF%8F%E4%B8%AA%E7%89%88%E6%9C%AC%E7%9A%84%E5%8F%91%E5%B8%83%E6%97%B6%E9%97%B4%E3%80%82">http://kafka.apache.org/downloads可以查看到每个版本的发布时间。</a></p>
<h3 id="4-1-安装zookeeper-集群"><a href="#4-1-安装zookeeper-集群" class="headerlink" title="4.1 安装zookeeper 集群"></a>4.1 安装zookeeper 集群</h3><ul>
<li>Kafka集群是必须要有<code>ZooKeeper</code>的</li>
</ul>
<p>注意：</p>
<ul>
<li>每一个Kafka的节点都需要修改<code>broker.id</code>（每个节点的标识，不能重复）</li>
<li><code>log.dir</code>数据存储目录需要配置</li>
</ul>
<p>详见<code>zookeeper部署文档.md</code></p>
<h3 id="4-2-安装kafka-集群"><a href="#4-2-安装kafka-集群" class="headerlink" title="4.2 安装kafka 集群"></a>4.2 安装kafka 集群</h3><p><strong>上传安装包</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">移动到指定文件夹</span><br><span class="line"></span><br><span class="line">mv kafka_2.12-2.4.1.tgz /export/server</span><br><span class="line"></span><br><span class="line">tar -zxvf kafka_2.12-2.4.1.tgz</span><br></pre></td></tr></table></figure>

<p><strong>目录结构分析</strong></p>
<table>
<thead>
<tr>
<th>目录名称</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>bin</td>
<td>Kafka的所有执行脚本都在这里。例如：启动Kafka服务器、创建Topic、生产者、消费者程序等等</td>
</tr>
<tr>
<td>config</td>
<td>Kafka的所有配置文件</td>
</tr>
<tr>
<td>libs</td>
<td>运行Kafka所需要的所有JAR包</td>
</tr>
<tr>
<td>logs</td>
<td>Kafka的所有日志文件，如果Kafka出现一些问题，需要到该目录中去查看异常信息</td>
</tr>
<tr>
<td>site-docs</td>
<td>Kafka的网站帮助文件</td>
</tr>
</tbody></table>
<p><strong>修改配置文件</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(1)进入配置文件目录</span></span><br><span class="line">cd /export/server/kafka_2.11-2.0.0/config</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(2)编辑配置文件</span></span><br><span class="line">vi server.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#为依次增长的:0、1、2、3、4,集群中唯一 id --》从0开始，每台不能重复，第一块要改的</span></span><br><span class="line"><span class="attr">broker.id</span>=<span class="string">0 </span></span><br><span class="line"></span><br><span class="line"><span class="attr">----Logbasic------</span></span><br><span class="line"><span class="comment">#数据存储的目录，第二块要改的</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/export/data/kafka-logs  </span></span><br><span class="line"></span><br><span class="line"><span class="attr">---zookeeper----</span></span><br><span class="line"><span class="comment">#指定 zk 集群地址，第四块要改的</span></span><br><span class="line"><span class="attr">zookeeper.connect</span>=<span class="string">node1:2181,node2:2181,node3:2181</span></span><br></pre></td></tr></table></figure>



<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">分发kafka</span></span><br><span class="line">cd /export/server/</span><br><span class="line">syncfile /export/server/kafka_2.12-2.4.1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">配置环境变量</span></span><br><span class="line">vi /etc/profile </span><br><span class="line"></span><br><span class="line">export KAFKA_HOME=/export/server/kafka </span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin </span><br><span class="line">source /etc/profile </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">注意:还需要分发环境变量</span></span><br><span class="line">syncfile /etc/profile</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">分别在node2和node3上修改配置文件</span></span><br><span class="line"></span><br><span class="line">vim /export/server/kafka/config/server.propertie</span><br><span class="line">broker.id=1 </span><br><span class="line">broker.id=2</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(broker.id 不能重复)</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启停集群(在各个节点上启动)</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启动集群</span></span><br><span class="line">kafka-server-start.sh -daemon /export/server/kafka/config/server.properties </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">停止集群</span></span><br><span class="line">kafka-server-stop.sh stop</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kafka一键启停脚本</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ $# -eq 0 ]</span><br><span class="line">then</span><br><span class="line">echo &quot;please input param:start stop&quot;</span><br><span class="line">else</span><br><span class="line"></span><br><span class="line">if [ $1 = start  ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;$&#123;1&#125;ing node$&#123;i&#125;&quot;</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/kafka/bin/kafka-server-start.sh -daemon /export/server/kafka/config/server.properties&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $1 = stop ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/kafka/bin/kafka-server-stop.sh&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>



<h3 id="4-3-kafka命令行操作"><a href="#4-3-kafka命令行操作" class="headerlink" title="4.3 kafka命令行操作"></a>4.3 kafka命令行操作</h3><p>Kafka 中提供了许多命令行工具（位于$KAFKA HOME&#x2F;bin 目录下）用于管理集群的变更。</p>
<table>
<thead>
<tr>
<th align="center">命令</th>
<th align="center">用途</th>
</tr>
</thead>
<tbody><tr>
<td align="center">kafka-configs.sh</td>
<td align="center">用于配置管理</td>
</tr>
<tr>
<td align="center">kafka-console-consumer.sh</td>
<td align="center">用于消费消息</td>
</tr>
<tr>
<td align="center">kafka-console-producer.sh</td>
<td align="center">用于生产消息</td>
</tr>
<tr>
<td align="center">kafka-consumer-perf-test.sh</td>
<td align="center">用于测试消费性能</td>
</tr>
<tr>
<td align="center">kafka-topics.sh</td>
<td align="center">用于管理主题</td>
</tr>
<tr>
<td align="center">kafka-dump-log.sh</td>
<td align="center">用于查看日志内容</td>
</tr>
<tr>
<td align="center">kafka-server-stop.sh</td>
<td align="center">用于关闭Kafka服务</td>
</tr>
<tr>
<td align="center">kafka-preferred-replica-election.sh</td>
<td align="center">用于优先副本的选举</td>
</tr>
<tr>
<td align="center">kafka-server-start.sh</td>
<td align="center">用于启动Kafka服务</td>
</tr>
<tr>
<td align="center">kafka-producer-perf-test.sh</td>
<td align="center">用于测试生产性能</td>
</tr>
<tr>
<td align="center">kafka-reassign-partitions.sh</td>
<td align="center">用于分区重分配</td>
</tr>
</tbody></table>
<h4 id="（1）创建topic"><a href="#（1）创建topic" class="headerlink" title="（1）创建topic"></a>（1）创建topic</h4><p>创建一个topic（主题）。Kafka中所有的消息都是保存在主题中，要生产消息到Kafka，首先必须要有一个确定的主题。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">基本方式</span></span><br><span class="line">./kafka-topics.sh --create --topic tpc_1 --partitions 2 --replication-factor 2 --zookeeper node1:2181</span><br><span class="line"></span><br><span class="line">--replication-factor 副本数量</span><br><span class="line">--partitions 分区数量</span><br><span class="line">--topic topic 名称</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">手动指定副本的存储位置</span></span><br><span class="line">bin/kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line">该方式下,命令会自动判断所要创建的 topic 的分区数及副本数</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">bootstrap方式</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建名为<span class="built_in">test</span>的主题</span></span><br><span class="line">bin/kafka-topics.sh --create --bootstrap-server node1:9092 --topic test</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看目前Kafka中的主题</span></span><br><span class="line">bin/kafka-topics.sh --list --bootstrap-server node1:9092</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">--replica-assignment 不能同时使用--partitions --replication-factor参数指定partition的AR列表，未指定AR列表则会根据负载均衡算法将partition的replica均衡的分布在Kafka集群中。</span><br><span class="line"></span><br><span class="line">--replica-assignment 1:3,2:1,3:2，</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">逗号区分不同的partition，</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">冒号区别相同partition中的replica，</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">partition-0的AR=[1,3]，partition-1的AR=[2,1]，partition-2的AR=[3,2]。</span></span><br><span class="line"></span><br><span class="line">Eg：testMcdull222AR列表计算出来时--replica-assignment 2:3,1:3,1:2 。数字指的是broker的ID号</span><br></pre></td></tr></table></figure>

<img src=".\md图\kafka.assets\image-20230204132807408.png" alt="image-20230204132807408" style="zoom:50%;" />

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">--replica-assignment 参数一般不由用户指定，由Kafka默认分配算法保证，有两个原则：</span><br><span class="line"></span><br><span class="line">（1）使Topic的所有Partition Replica能够均匀地分配至各个Kafka Broker（负载均衡）；</span><br><span class="line">（2）Partition 内的replica能够均匀地分配在不同Kafka Broker。</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如果Partition的第一个Replica分配至某一个Kafka Broker，那么这个Partition的其它Replica则需要分配至其它的Kafka Brokers，即Partition Replica分配至不同的Broker；</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">分配原则</span></span><br><span class="line">1、从Broker随机位置开始按照轮询方式选择每个Partition的第一个replica</span><br><span class="line">2、不同Partition剩余replica按照一定的偏移量紧跟着各自的第一个replica</span><br></pre></td></tr></table></figure>

<img src=".\md图\kafka.assets\image-20230204133027944.png" alt="image-20230204133027944" style="zoom: 50%;" />

<h4 id="（2）删除topic"><a href="#（2）删除topic" class="headerlink" title="（2）删除topic"></a>（2）删除topic</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh  --delete --topic tpc_1 --zookeeper node1：2181</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">（异步线程去删除）删除 topic,需要一个参数处于启用状态: delete.topic.enable = <span class="literal">true</span>,否则删不掉</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">使用 kafka-topics.sh 脚本删除主题的行为本质上只是在 ZooKeeper 中的 /admin/delete_topics 路径下 建一个与待删除主题同名的节点,以标记该主题为待删除的状态。与创建主题相同的是,真正删除主题的动作也是由 Kafka 的控制器负责完成的。</span></span><br></pre></td></tr></table></figure>

<h4 id="（3）查看topic"><a href="#（3）查看topic" class="headerlink" title="（3）查看topic"></a>（3）查看topic</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(1)列出当前系统中的所有 topic</span> </span><br><span class="line">bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 –list</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(2)查看 topic 详细信息</span></span><br><span class="line">bin/kafka-topics.sh --create --topic tpc_1   --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line">bin/kafka-topics.sh --describe --topic tpc_1 --zookeper node1:2181 </span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204133351092.png" alt="image-20230204133351092"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">	 Topic: tpc_1 PartitionCount:2 ReplicationFactor:2 Configs: </span><br><span class="line">	 Topic: tpc_1 Partition: 0 Leader: 0 Replicas: 0,1 Isr: 0,1</span><br><span class="line">	 Topic: tpc_1 Partition: 1 Leader: 1 Replicas: 1,2 Isr: 1,2</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">从上面的结果中, 可以看出, topic 的分区数量, 以及每个分区的副本数量, 以及每个副本所在的 broker 节点,以及每个分区的 leader 副本所在 broker 节点,以及每个分区的 ISR 副本列表;</span> </span><br><span class="line"></span><br><span class="line">AR=ISR+OSR</span><br><span class="line"></span><br><span class="line">ISR: in sync replicas 同步副本(当然也包含 leader 自身) -&gt;follower去找leader同步数据</span><br><span class="line"></span><br><span class="line">OSR:out of sync replicas 失去同步的副本(数据与 leader 之间的差距超过配置的阈值)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kafka不是完全同步，也不是完全异步</span></span><br><span class="line">1.leader会维持一个与其保持同步的replica集合，该集合就是ISR，每一个partition都有一个ISR，它是有leader动态维护。</span><br><span class="line"></span><br><span class="line">2.我们要保证kafka不丢失message，就要保证ISR这组集合存活（至少有一个存活），并且消息commit成功。</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">英文简称</span></span><br><span class="line">--AR：  Assigned Replicas的缩写，是每个partition下所有副本（replicas）的统称；</span><br><span class="line">--ISR： In-Sync Replicas的缩写，是指副本同步队列，ISR是AR中的一个子集；</span><br><span class="line">--LEO：LogEndOffset的缩写，表示每个partition的log最后一条Message的位置。</span><br><span class="line">--HW： HighWatermark的缩写，是指consumer能够看到的此partition的位置。 取一个partition对应的ISR中最小的LEO作为HW，consumer最多只能消费到HW所在的位置。</span><br></pre></td></tr></table></figure>

<h4 id="（4）增加分区数"><a href="#（4）增加分区数" class="headerlink" title="（4）增加分区数"></a>（4）增加分区数</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --alter --topic tpc_1 --partitions 3 --zookeeper node1:2181</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">Kafka 只支持增加分区,不支持减少分区</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">原因是:减少分区,代价太大(数据的转移,日志段拼接合并)</span> </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如果真的需要实现此功能,则完全可以重新创建一个分区数较小的主题,然后将现有主题中的消息按照既定的逻辑复制过去;</span></span><br></pre></td></tr></table></figure>

<h4 id="（5）动态配置topic-参数"><a href="#（5）动态配置topic-参数" class="headerlink" title="（5）动态配置topic 参数"></a>（5）动态配置topic 参数</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">通过管理命令,可以为已创建的 topic 增加、修改、删除 topic level 参数</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">添加、修改配置参数(开启压缩发送传输种提高kafka消息吞吐量的有效办法(‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’))</span></span><br><span class="line"></span><br><span class="line">bin/kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --add-config compression.type=gzip </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">删除配置参数</span></span><br><span class="line">bin/kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --delete-config compression.type</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（6）生产消息到Kafka并进行消费"><a href="#（6）生产消息到Kafka并进行消费" class="headerlink" title="（6）生产消息到Kafka并进行消费"></a>（6）生产消息到Kafka并进行消费</h4><p>使用Kafka内置的测试程序，生产一些消息到Kafka的tpc_1主题中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example1-kafka-console-producer</span></span><br><span class="line">bin/kafka-console-producer.sh --broker-list node1:9092, node2:9092, node3:9092 --topic tpc_1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello word</span> </span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">kafka</span> </span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">nihao</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example2-kafka-console-consumer</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(1)消费消息(从头开始)</span></span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server node1:9092, node2:9092, node1:9092 --topic tpc_1 --from-beginning</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(2)指定要消费的分区,和要消费的起始 offset</span> </span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server node1:9092,node2:9092,node3:9092 --topic tcp_1 --offset 2 --partition 0</span><br></pre></td></tr></table></figure>

<h4 id="（7）配置管理-kafka-configs"><a href="#（7）配置管理-kafka-configs" class="headerlink" title="（7）配置管理 kafka-configs"></a>（7）<strong>配置管理</strong> <strong>kafka</strong>-configs</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">kafka-configs.sh 脚本是专门用来对配置进行操作的, 这里的操作是运行状态修改原有的配置, 如此可以达到动态变更的目; </span><br><span class="line"></span><br><span class="line">kafka-configs.sh 脚本包含:</span><br><span class="line">-变更 alter</span><br><span class="line">-查看 describe 这两种指令类型。</span><br><span class="line">同使用 kafka-topics.sh 脚本变更配置一样,增、删、改的行为都可以看做变更操作,不过 kafka-configs.sh 脚本不仅可支持操作主题相关的配置,还支持操 broker 、用户和客户端这 3 个类型的配置。实体entity</span><br><span class="line"></span><br><span class="line">kafka-configs.sh 脚本使用 entity-type 参数来指定操作配置的类型, 并且使 entity-name 参数来指定操作配置的名称。</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">比如查看 topic 的配置可以按如下方式执行:</span></span><br><span class="line">bin/kafka-configs.sh zookeeper node1: 2181 --describe --entity-type topics --entity-name tpc_2 </span><br><span class="line"></span><br><span class="line">比如查看 broker 的动态配置可以按如下方式执行:</span><br><span class="line">bin/kafka-configs.sh zookeeper node1: 2181 --describe --entity-type brokers --entity-name 0 --zookeeper node1:2181</span><br></pre></td></tr></table></figure>

<p><strong>entity-type 和entity-name 的对应关系</strong><br><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204213257261.png" alt="image-20230204213257261"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">示例:添加 topic 级别参数</span></span><br><span class="line">bin/kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name tpc_2 --add-config cleanup.policy=compact , max.message.bytes=10000</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">使用 kafka-configs.sh 脚本来变更( alter )配置时,会在 ZooKeeper 中创建一个命名形式为: /config/&lt;entity-type&gt;/&lt;ent ity name &gt;的节点,并将变更的配置写入这个节点</span></span><br></pre></td></tr></table></figure>



<h4 id="（8）Kafka的生产者-x2F-消费者-x2F-工具"><a href="#（8）Kafka的生产者-x2F-消费者-x2F-工具" class="headerlink" title="（8）Kafka的生产者&#x2F;消费者&#x2F;工具"></a>（8）Kafka的生产者&#x2F;消费者&#x2F;工具</h4><ul>
<li>安装Kafka集群，可以测试以下<ul>
<li>创建一个topic主题（消息都是存放在topic中，类似mysql建表的过程）</li>
<li>基于kafka的内置测试生产者脚本来读取标准输入（键盘输入）的数据，并放入到topic中</li>
<li>基于kafka的内置测试消费者脚本来消费topic中的数据</li>
</ul>
</li>
<li>推荐大家开发的使用Kafka Tool<ul>
<li>浏览Kafka集群节点、多少个topic、多少个分区</li>
<li>创建topic&#x2F;删除topic</li>
<li>浏览ZooKeeper中的数据</li>
</ul>
</li>
</ul>
<img src=".\md图\kafka.assets\image-20230204213731698.png" alt="image-20230204213731698"  />

<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204213753683.png" alt="image-20230204213753683"></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204213812492.png" alt="image-20230204213812492"></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204213824345.png" alt="image-20230204213824345"></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204213832931.png" alt="image-20230204213832931"></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204213840932.png" alt="image-20230204213840932"></p>
<h4 id="（9）Kafka的基准测试"><a href="#（9）Kafka的基准测试" class="headerlink" title="（9）Kafka的基准测试"></a>（9）Kafka的基准测试</h4><p>基准<a target="_blank" rel="noopener" href="http://www.blogjava.net/qileilove/archive/2012/07/05/382241.html">测试</a>（benchmark testing）是一种测量和评估软件性能指标的活动。我们可以通过基准测试，了解到软件、硬件的性能水平。主要测试负载的执行时间、传输速度、吞吐量、资源占用率等。</p>
<ul>
<li><p>Kafka中提供了内置的性能测试工具</p>
<ul>
<li><p>生产者：测试生产每秒传输的数据量（多少条数据、多少M的数据）</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5000000 records sent, 11825.446943 records/sec (11.28 MB/sec), 2757.61 ms avg latency</span><br></pre></td></tr></table></figure>
</li>
<li><p>消费者：测试消费每条拉取的数据量</p>
</li>
</ul>
</li>
<li><p>对比生产者和消费者：消费者的速度更快</p>
</li>
</ul>
<h2 id="五、Kafka-Java-API开发"><a href="#五、Kafka-Java-API开发" class="headerlink" title="五、Kafka Java API开发"></a>五、Kafka Java API开发</h2><h3 id="5-1-API-开发：producer-生产者"><a href="#5-1-API-开发：producer-生产者" class="headerlink" title="5.1 API 开发：producer 生产者"></a>5.1 API 开发：producer 生产者</h3><h4 id="（1）生产者api示例"><a href="#（1）生产者api示例" class="headerlink" title="（1）生产者api示例"></a>（1）生产者api示例</h4><blockquote>
<p>一个正常的生产逻辑需要具备以下几个步骤</p>
</blockquote>
<p><code>(1)配置生产者客户端参数</code></p>
<p><code>(2)创建相应的生产者实例</code></p>
<p><code>(3)构建待发送的消息</code></p>
<p><code>(4)发送消息</code></p>
<p><code>(5)关闭生产者实例</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//首先,引入 maven 依赖</span></span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">	&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; </span><br><span class="line">	&lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; </span><br><span class="line">	&lt;version&gt;<span class="number">2.0</span><span class="number">.0</span>&lt;/version&gt; </span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//采用默认分区方式将消息散列的发送到各个分区当中</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer; </span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer; </span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord; </span><br><span class="line"><span class="keyword">import</span> java.util.Properties; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyProducer</span> &#123; </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[ ] args)</span> <span class="keyword">throws</span> InterruptedException &#123; </span><br><span class="line">		<span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line">		<span class="comment">//设置 kafka 集群的地址</span></span><br><span class="line">		props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>);</span><br><span class="line">		<span class="comment">//ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，</span></span><br><span class="line">		<span class="comment">//0-&gt;不等响应就继续发（可靠性低），</span></span><br><span class="line">		<span class="comment">//1-&gt;leader会写到本地日志后，然后给响应，producer拿到响应才继续发（follwer还没同步）</span></span><br><span class="line">		props.put(“acks”, “all”); <span class="comment">//--》很重要</span></span><br><span class="line"></span><br><span class="line">		props.put(“retries”, <span class="number">3</span>); <span class="comment">//失败重试次数-&gt;失败会自动重试（可恢复/不可恢复）--&gt;(有可能会造成数据的乱序)</span></span><br><span class="line">		props.put(“batch.size”, <span class="number">10</span>); <span class="comment">//数据发送的批次大小提高效率/吞吐量太大会数据延迟</span></span><br><span class="line">		props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">10000</span>); <span class="comment">//消息在缓冲区保留的时间,超过设置的值就会被提交到服务端</span></span><br><span class="line">		props.put(<span class="string">&quot;max.request.size&quot;</span>,<span class="number">10</span>); <span class="comment">//数据发送请求的最大缓存数</span></span><br><span class="line">		props.put(“buffer.memory”, <span class="number">10240</span>); <span class="comment">//整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端</span></span><br><span class="line">		<span class="comment">//buffer.memory 要大于 batch.size,否则会报申请内存不足的错误降低阻塞的可能性</span></span><br><span class="line">		props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>); <span class="comment">//key-value序列化器</span></span><br><span class="line">		props.put(<span class="string">&quot;value.serializer&quot;</span>, 	<span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<span class="comment">//字符串最好</span></span><br><span class="line">		Producer&lt;String, String&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(props); </span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) </span><br><span class="line">			producer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;String, String&gt;(<span class="string">&quot;test&quot;</span>, Integer.toString(i), <span class="string">&quot;dd:&quot;</span>+i)); </span><br><span class="line">        <span class="comment">//Thread.sleep(1000000); </span></span><br><span class="line">        producer.close(); </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//消息对象 ProducerRecord,它并不是单纯意义上的消息,它包含了多个属性,原本需要发送的与业务关的消息体只是其中的一个 value 属性 ,比“ Hello, rgzn!&quot;只是 ProducerRecord 对象的一个属性。 ProducerRecord 类的定义如下:</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProducerRecord</span>&lt;K, V&gt; &#123; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> String topic; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Integer partition;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Headers headers; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> K key; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> V value; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Long timestamp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="（2）必要的参数配置"><a href="#（2）必要的参数配置" class="headerlink" title="（2）必要的参数配置"></a>（2）必要的参数配置</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//在创建真正的生产者实例前需要配置相应的参数,比如需要连接的 Kafka 集群地址。在 Kafka 生产者客户端 KatkaProducer 中有 3 个参数是必填的。</span></span><br><span class="line">-bootstrap.servers </span><br><span class="line">-key.serializer </span><br><span class="line">-value.serializer</span><br><span class="line">    </span><br><span class="line"><span class="comment">//为了防止参数名字符串书写错误,可以使用如下方式进行设置: </span></span><br><span class="line">props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,ProducerInterceptorPrefix.class.getName());</span><br><span class="line">props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;node1:9092,node2:9092&quot;</span>); </span><br><span class="line">props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName()); props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());</span><br></pre></td></tr></table></figure>

<h4 id="（3）发送消息"><a href="#（3）发送消息" class="headerlink" title="（3）发送消息"></a>（3）发送消息</h4><p>创建生产者实例和构建消息之后就可以开始发送消息了。发送消息主要有3 种模式：</p>
<h5 id="1-发后即忘-fire-and-forget）"><a href="#1-发后即忘-fire-and-forget）" class="headerlink" title="1.发后即忘( fire-and-forget）"></a>1.发后即忘( fire-and-forget）</h5><p><code>天文领域--&gt;三体--&gt;叶文洁红岸</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#发后即忘,producer只管往 Kafka 发送,并不关心消息是否正确到达。</span></span><br><span class="line"></span><br><span class="line">在大多数情况下,这种发送方式没有问题; </span><br><span class="line">不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。</span><br><span class="line">这种发送方式的性能最高,可靠性最差。</span><br><span class="line"></span><br><span class="line">ack--&gt;作用在broker </span><br><span class="line">Future&lt;RecordMetadata&gt; send = producer.send(rcd);-》也是异步</span><br><span class="line"></span><br><span class="line">没成功的话，producer也不管了</span><br></pre></td></tr></table></figure>

<h5 id="2-同步发送（sync-）"><a href="#2-同步发送（sync-）" class="headerlink" title="2.同步发送（sync ）"></a>2.同步发送（sync ）</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">	producer.send(rcd).get( ); <span class="comment">//--》一旦调用get方法，就会阻塞</span></span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123; </span><br><span class="line">	e.printStackTrace( );</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0.8.x 前,有一个参数 `producer.type=sycn|asycn` 来决定生产者的发送模式;<span class="comment">#-&gt;取消了</span></span><br><span class="line"></span><br><span class="line">现已失效(其实，新版中,producer 在底层只有异步方式，若想同步，发送一次，get一次就可实现)</span><br><span class="line"></span><br><span class="line">Future  future = Callable.run( ) <span class="comment">#-&gt; 有返回值，future.get（）</span></span><br><span class="line">runnable.run（）<span class="comment">#-&gt;无返回值</span></span><br><span class="line">多线程，new thread，然后new一个runnable<span class="comment">#-&gt;线程干活去了-&gt;没有返回值（拿不到）</span></span><br><span class="line">Future future =  Callable.run()<span class="comment">#-&gt; future.get()-&gt;可以有同步的实现方式了-&gt;使用.get()方法，就可以实现同步了</span></span><br></pre></td></tr></table></figure>

<h5 id="3-异步发送-async"><a href="#3-异步发送-async" class="headerlink" title="3.异步发送(async )"></a>3.异步发送(async )</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">回调函数会在 producer 收到 ack 时调用,为异步调用,该方法有两个参数,分别是 `RecordMetadata` 和`Exception`,如果 `Exception 为 null`,说明消息`发送成功`,如果 `Exception 不为 null`,说明消息`发送失败`。同时，则recordMetadata是有值的</span><br><span class="line"></span><br><span class="line"><span class="comment">#注意:消息发送失败会自动重试,不需要我们在回调函数中手动重试。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//代码示例</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*; </span><br><span class="line"><span class="keyword">import</span> java.util.Properties; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyProducer</span> &#123; </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123; </span><br><span class="line">		<span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); <span class="comment">// Kafka 服务端的主机名和端口号</span></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>); </span><br><span class="line">		<span class="comment">// 等待所有副本节点的应答</span></span><br><span class="line">		props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>); <span class="comment">// 消息发送最大尝试次数</span></span><br><span class="line">		props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">0</span>); <span class="comment">// 一批消息处理大小</span></span><br><span class="line">		props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>); <span class="comment">// 增加服务端请求延时</span></span><br><span class="line">		props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>); <span class="comment">// 发送缓存区内存大小</span></span><br><span class="line">		props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>); <span class="comment">// key 序列化</span></span><br><span class="line">		props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>); </span><br><span class="line">		<span class="comment">// value 序列化</span></span><br><span class="line">		props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>); 				  			KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(props); </span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">50</span>; i++) &#123; </span><br><span class="line">			kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;String, String&gt;(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;hello&quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123; </span><br><span class="line">			<span class="meta">@Override</span> </span><br><span class="line">			<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123; </span><br><span class="line">				<span class="keyword">if</span> (metadata != <span class="literal">null</span>) &#123; </span><br><span class="line">					System.out.println(metadata.partition() + <span class="string">&quot;---&quot;</span> + metadata.offset()); </span><br><span class="line">                &#125; </span><br><span class="line">            &#125; </span><br><span class="line">            &#125;); </span><br><span class="line">        &#125; </span><br><span class="line">        kafkaProducer.close(); </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（4）生产者原理解析"><a href="#（4）生产者原理解析" class="headerlink" title="（4）生产者原理解析"></a>（4）生产者原理解析</h4><p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cproducer%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg" alt="producer流程图"></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">1.一个生产者客户端由两个线程协调运行,这两个线程分别为`主线程`和 `Sender 线程 。</span><br><span class="line"></span><br><span class="line">2.在主线程中由` kafkaProducer `创建消息,然后通过可能的`拦截器`、`序列化器`和`分区器`的作用之后缓存到消息累加器(`RecordAccumulator`, 也称为消息收集器)中。</span><br><span class="line"></span><br><span class="line">3.`Sender线程`负责从 `RecordAccumulator` 获取消息并将其发送到 Kafka 中; </span><br><span class="line"></span><br><span class="line">4.`RecordAccumulator `主要用来`缓存消息`以便 Sender 线程可以批量发送, 进而减少网络传输的资源消耗以提升性能。 </span><br><span class="line"></span><br><span class="line">5.`RecordAccumulator `缓存的大小可以通过生产者客户端参数 `buffer.memory` 配置, 默认值为 33554432B ,即 32M。</span><br><span class="line">如果`生产者发送消息的速度超过发送到服务器的速度`,则会导致生产者空间不足,这个时候 `KafkaProducer.send()`方法调用要么被阻塞,要么抛出异常,这个取决于参数`max.block.ms` 的配置,此参数的默认值为 60000,即 60 秒。</span><br><span class="line"></span><br><span class="line">6.主线程中发送过来的消息都会被迫加到 `RecordAccumulator `的某个`双端队列( Deque )`中, `RecordAccumulator `内部为每个分区都维护了一个双端队列,即 `Deque&lt;ProducerBatch&gt;``。</span><br><span class="line">消息写入缓存时,`追加到双端队列的尾部`;</span><br><span class="line"></span><br><span class="line">7.`Sender `读取消息时,`从双端队列的头部读取`。</span><br><span class="line"></span><br><span class="line">8.注意:`ProducerBatch `是指一个消息批次; </span><br><span class="line">与此同时,会将较小的 `ProducerBatch `凑成一个较大 `ProducerBatch` ,也可以减少网络请求的次数以提升整体的吞吐量。</span><br><span class="line"></span><br><span class="line"><span class="comment">#问题：什么情况下，消息累加器中的分区会增多？</span></span><br><span class="line">9.`ProducerBatch` 大小和 `batch.size` 参数也有着密切的关系。</span><br><span class="line"></span><br><span class="line">10.当一条消息(`ProducerRecord `) 流入`RecordAccumulator` 时,会先寻找与消息分区所对应的双端队列(如果没有则新建),再从这个双端队列的尾部获取一个 `ProducerBatch` (如果没有则新建),查看 `ProducerBatch` 中是否还可以写入这个 `ProducerRecord`,如果可以写入,如果不可以则需要创建一个新的 `Producer Batch`。</span><br><span class="line"></span><br><span class="line">11.在新建`ProducerBatch `时评估这条消息的大小是否超过 `batch.size` 参数大小, 如果不超过, 那么就以 `batch.size` 参数的大小来创建 `ProducerBatch`。</span><br><span class="line"></span><br><span class="line"><span class="comment">#如果生产者客户端需要向很多分区发送消息, 则可以将 buffer.memory 参数适当调大以增加整体的吞吐量。</span></span><br><span class="line"></span><br><span class="line">12.`Sender `从 `RecordAccumulator` 获取缓存的消息之后,会进一步将`&lt;分区,Deque&lt;Producer Batch&gt;&gt;`的形式转变成`&lt;Node,List&lt; ProducerBatch&gt;`的形式,其中 Node 表示 Kafka 集群 broker 节点。</span><br><span class="line"></span><br><span class="line">13.对于网络连接来说,生产者客户端是与具体 `broker` 节点建立的连接,也就是向具体的` broker `节点发送消息,而并不关心消息属于哪一个分区;</span><br><span class="line"></span><br><span class="line">14.而对于 `KafkaProducer `的应用逻辑而言,我们只关注向哪个分区中发送哪些消息,所以在这里需要做一个应用逻辑层面到网络 I/O 层面的转换。</span><br><span class="line"></span><br><span class="line">15.在转换成`&lt;Node, List&lt;ProducerBatch&gt;&gt;`的形式之后, Sender 会进一步封装成`&lt;Node,Request&gt; `的形式, 这样就可以将 `Request `请求发往各个 Node 了,这里的 `Request `是 Kafka 各种协议请求;</span><br><span class="line"></span><br><span class="line">16.请求在从 sender 线程发往 Kafka 之前还会保存到 `InFlightRequests `中,`InFlightRequests` 保存对象的具体形式为 `Map&lt;Nodeld, Deque&lt;request&gt;&gt;`,它的主要作用是缓存了已经发出去但还没有收到服务端响应的请求(Nodeld 是一个 String 类型,表示节点的 <span class="built_in">id</span> 编号)。</span><br><span class="line"></span><br><span class="line">17.与此同时,`InFlightRequests` 还提供了许多管理类的方法,并且通过配置参数还可以限制每个连接(也就是客户端与 Node 之间的连接) 最多缓存的请求数。</span><br><span class="line"></span><br><span class="line">18.这个配置参数为 `max.in.flight.request.per.connection` ,默认值为 5,即每个连接最多只能缓存 5 个未响应的请求,超过该数值之后就不能再向这个连接发送更多的请求了,除非有缓存的请求收到了响应( Response )。</span><br><span class="line"></span><br><span class="line">19.通过比较 `Deque&lt;Request&gt;` 的 `size` 与这个`参数的大小`来判断对应的 Node 中是否己经堆积了很多未响应的消息, 如果真是如此, 那么`说明这个 Node 节点负载较大或网络连接有问题,再继其发送请求会增大请求超时的可能`。</span><br></pre></td></tr></table></figure>

<h4 id="（5）重要的生产者参数"><a href="#（5）重要的生产者参数" class="headerlink" title="（5）重要的生产者参数"></a>（5）重要的生产者参数</h4><h5 id="1-acks"><a href="#1-acks" class="headerlink" title="1.acks"></a>1.acks</h5><table>
<thead>
<tr>
<th>acks</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>Producer 往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。</td>
</tr>
<tr>
<td>1</td>
<td>Producer 往集群发送数据只要Leader 成功写入消息就可以发送下一条，只确保Leader 接收成功。</td>
</tr>
<tr>
<td>-1或all</td>
<td>Producer 往集群发送数据需要所有的ISR Follower 都完成从Leader 的同步才会发送下一条，确保<br/>Leader 发送成功和所有的副本都成功接收。安全性最高，但是效率最低。</td>
</tr>
</tbody></table>
<h5 id="2-max-request-size"><a href="#2-max-request-size" class="headerlink" title="2.max.request.size"></a>2.max.request.size</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">这个参数用来限制生产者客户端能发送的消息的最大值,默认值为 1048576B ,即 1MB 一般情况下,这个默认值就可以满足大多数的应用场景了。</span><br><span class="line"></span><br><span class="line">这个参数还涉及一些其它参数的联动,比如 broker 端的` message.max.bytes` 参数,如果配置错误可能会引起一些不必要的异常 ; </span><br><span class="line"></span><br><span class="line">比如将broker端的`message.max.bytes`参数配置为10 , 而`max.request.size` 参数配置为 20,那么当发送一条大小为 15B 的消息时,生产者客户端就会报出异常</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="3-compression-type"><a href="#3-compression-type" class="headerlink" title="3.compression.type"></a>3.compression.type</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这个参数用来`指定消息的压缩方式,默认值为“none <span class="string">&quot;`,即默认情况下,消息不会被压缩。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">该参数还可以配置为 &quot;</span>gzip<span class="string">&quot;,&quot;</span>snappy<span class="string">&quot; 和 &quot;</span>lz4<span class="string">&quot;。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">`（服务端也有压缩参数，先解压，再压缩）`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">对消息进行压缩可以极大地减少网络传输、降低网络 I/O,从而提高整体的性能 。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">消息压缩是一种以时间换空间的优化方式,如果对时延有一定的要求,则不推荐对消息进行压缩;</span></span><br><span class="line"><span class="string">`没有必要，不需要压缩`</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>

<h5 id="4-retries-和-retry-backoff-ms"><a href="#4-retries-和-retry-backoff-ms" class="headerlink" title="4.retries 和 retry.backoff.ms"></a>4.retries 和 retry.backoff.ms</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">retries 参数用来`配置生产者重试的次数`,默认值为 0,即在发生异常的时候不进行任何重试动作。</span><br><span class="line"></span><br><span class="line">消息在从生产者发出到成功写入服务器之前可能发生一些`临时性的异常`,比如`网络抖动`、 `leader 副本的选举`等,这种异常往往是可以自行恢复的,生产者可以`通过配置 retries 大于 0 的值`,以此通过内部重试来恢复而不是一味地将异常抛给生产者的应用程序。如果重试达到设定的次数,那么生产者就会放弃重试并返回异常。</span><br><span class="line"></span><br><span class="line">重试还和另一个参数 `retry.backoff.ms` 有关,这个参数的默认值为 100,它用来`设定两次重试之间的时间间隔,避免无效的频繁重试`。</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Kafka `可以保证同一个分区中的消息是有序的`。如果生产者按照一定的顺序发送消息,那么这些消息也会顺序地写入分区,进而消费者也可以按照同样的顺序消费它们。</span><br><span class="line"></span><br><span class="line">对于某些应用来，顺序性非常重要 ，比如 `MySQL binlog` 的传输,如果出现错误就会造成非常严重的后果; </span><br><span class="line"></span><br><span class="line"><span class="comment">#MySQL binlog --》mysql插入数据--》操作结果体会在表中--》mysql为了提高可靠性会把操作记录在日志中--》为了以后的主从同步（mysql集群，主表，子表）--》读写分离--》binlog（mysql自己设计的格式，二进制形式）</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Canal--》监听binlog--》解析为json--》写入kafka--》flume就可以读到---》在kafka中的顺序不能错</span></span><br><span class="line"></span><br><span class="line">如果将` acks 参数`配置为非零值,并且 `max.flight.requests.per.connection `参数配置为大于 1 的值,那可能会出现错序的现象:</span><br><span class="line"><span class="comment">#如果第一批次消息写入失败,而第二批次消息写入成功,那么生产者会重试发送第一批次的消息,此时如果第一次的消息写入成功,那么这两个批次的消息就出现了错序。</span></span><br><span class="line"></span><br><span class="line">一般而言,在需要保证消息顺序的场合建议把参数`max.in.flight.requests.per.connection `配置为 1 ,而不是把 acks 配置为 0,不过这样也会影响整体的吞吐。<span class="comment">#--》吞吐量降低</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="5-batch-size"><a href="#5-batch-size" class="headerlink" title="5.batch.size"></a>5.batch.size</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">每个 Batch 要存放` batch.size `大小的数据后,才可以发送出去。比如说 `batch.size` 默认值是 16KB,那么里面凑够 16KB 的数据才会发送。</span><br><span class="line"></span><br><span class="line">理论上来说, 提升 `batch.size` 的大小, 可以允许更多的数据缓冲在里面, 那么一次 `Request` 发送出去的数据量就更多了,这样吞吐量可能会有所提升。</span><br><span class="line"></span><br><span class="line">但是 `batch.size` 也不能过大,要是数据老是缓冲在 `Batch `里迟迟不发送出去,那么发送消息的延迟就会很高。</span><br><span class="line"></span><br><span class="line">一般可以尝试把这个参数调节大些,利用生产环境发消息负载测试一下。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="6-linger-ms-和batchsize有联系"><a href="#6-linger-ms-和batchsize有联系" class="headerlink" title="6.linger.ms(和batchsize有联系)"></a>6.linger.ms(和batchsize有联系)</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">这个参数用来指定生产者发送 `ProducerBatch` 之前等待更多消息( `ProducerRecord` )加入`ProducerBatch` 时间,默认值为 0。</span><br><span class="line"></span><br><span class="line">生产者客户端会在 `ProducerBatch `填满或等待时间超过 `linger.ms `值时发送出去。</span><br><span class="line"></span><br><span class="line">增大这个参数的值会增加消息的延迟,但是同时能提升一定的吞吐量。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="7-enable-idempotence-gt-true-x2F-false"><a href="#7-enable-idempotence-gt-true-x2F-false" class="headerlink" title="7.enable.idempotence  -&gt;true&#x2F;false"></a>7.enable.idempotence  -&gt;true&#x2F;false</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">是否开启幂等性功能,详见后续原理加强;</span><br><span class="line"></span><br><span class="line">幂等性,就是一个操作重复做,每次的结果都一样，x*1=1，x*1=1，x*1=1，</span><br><span class="line"></span><br><span class="line">在 kafka 中，就是生产者生产的一条消息，如果多次重复发送，在服务器中的结果还是只有一条</span><br><span class="line"></span><br><span class="line">Kafka很难实现幂等性，如果重复发，kafka肯定有多条消息---》需要有机制判断曾经是否发送过--》各种手段判断--》事务管理的概念----》加入幂等性，吞吐量会急剧下降</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="8-partitioner-classes"><a href="#8-partitioner-classes" class="headerlink" title="8.partitioner.classes"></a>8.partitioner.classes</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">用来指定分区器,默认:`org.apache.kafka.internals.DefaultPartitioner` --》用`hashcode`分</span><br><span class="line"></span><br><span class="line">自定义 partitioner 需要实现 `org.apache.kafka.clients.producer.Partitioner` 接口--》自己写很简单</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="5-2-API-开发：consumer生产者"><a href="#5-2-API-开发：consumer生产者" class="headerlink" title="5.2 API 开发：consumer生产者"></a>5.2 API 开发：consumer生产者</h3><h4 id="（1）消费者Api-示例"><a href="#（1）消费者Api-示例" class="headerlink" title="（1）消费者Api 示例"></a>（1）消费者Api 示例</h4><blockquote>
<p>一个正常的消费逻辑需要具备以下几个步骤: </p>
</blockquote>
<p><code>(1)配置消费者客户端参数</code></p>
<p><code>(2)创建相应的消费者实例; </code></p>
<p><code>(3)订阅主题; </code></p>
<p><code>(4)拉取消息并消费; </code></p>
<p><code>(5)提交消费位移 offset;</code></p>
<p><code>(6)关闭消费者实例。</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//消费者实例代码</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*; </span><br><span class="line"><span class="keyword">import</span> java.util.Arrays; </span><br><span class="line"><span class="keyword">import</span> java.util.Properties; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyConsumer</span> &#123; </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123; </span><br><span class="line">		<span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line">		<span class="comment">// 定义 kakfa 服务的地址,不需要将所有 broker 指定上</span></span><br><span class="line">		props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1:9092&quot;</span>); </span><br><span class="line">		<span class="comment">// 指定 consumer group </span></span><br><span class="line">		props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;g1&quot;</span>); </span><br><span class="line">		<span class="comment">// 是否自动提交 offset </span></span><br><span class="line">		props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>); </span><br><span class="line">		<span class="comment">// 自动提交 offset 的时间间隔</span></span><br><span class="line">		props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line">		<span class="comment">// key 的反序列化类</span></span><br><span class="line">		props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>); </span><br><span class="line">		<span class="comment">// value 的反序列化类</span></span><br><span class="line">		props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>); </span><br><span class="line">		<span class="comment">// 如果没有消费偏移量记录,则自动重设为起始 offset:latest, earliest, none</span></span><br><span class="line">		<span class="comment">//Earliest-&gt;目前状态下最前面的一条消息（日志在一定保存时间后会自动清空）</span></span><br><span class="line">		<span class="comment">//none（上次记录的偏移量，如果没有，会抛异常） </span></span><br><span class="line"></span><br><span class="line">		props.put(<span class="string">&quot;auto.offset.reset&quot;</span>,<span class="string">&quot;earliest&quot;</span>); </span><br><span class="line">		<span class="comment">// 定义 consumer </span></span><br><span class="line">		KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props); </span><br><span class="line">		<span class="comment">// 消费者订阅的 topic, 可同时订阅多个</span></span><br><span class="line">		consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;test&quot;</span>,<span class="string">&quot;test1&quot;</span>)); </span><br><span class="line">		<span class="keyword">while</span> (<span class="literal">true</span>) &#123; </span><br><span class="line">		<span class="comment">// 读取数据,读取超时时间为 100ms </span></span><br><span class="line">			ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>); </span><br><span class="line">			<span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) </span><br><span class="line">				System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, 		record.offset(), record.key(), record.value()); </span><br><span class="line">        &#125; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="（2）必要参数配置"><a href="#（2）必要参数配置" class="headerlink" title="（2）必要参数配置"></a>（2）必要参数配置</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//也可以使用如下形式:</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,brokerList);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.GROUP_ID_CONFIG,groupid);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.CLIENT_ID_CONFIG,clientid);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（3）subscribe-订阅主题"><a href="#（3）subscribe-订阅主题" class="headerlink" title="（3）subscribe 订阅主题"></a>（3）subscribe 订阅主题</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//subscribe 有如下重载方法: </span></span><br><span class="line"><span class="comment">//(1)前面两种是通过集合的方式订阅一到多个topic</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Collection&lt;String&gt; topics,ConsumerRebalanceListener listener)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Collection&lt;String&gt; topics)</span></span><br><span class="line"><span class="comment">//(2)后两种主要是采用正则的方式订阅一到多个topic</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Pattern pattern, ConsumerRebalanceListener listener)</span> </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Pattern pattern)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//指定集合方式订阅主题</span></span><br><span class="line">consumer.subscribe(Arrays.asList(topic1)); </span><br><span class="line">consumer <span class="title function_">subscribe</span><span class="params">(Arrays.asList(topic2)</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//正则方式订阅主题</span></span><br><span class="line">如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中,如果有人又创建了新的主题,并且主题名字与正表达式相匹配,那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题,并且可以处理不同的类型,那么这种订阅方式就很有效。</span><br><span class="line"></span><br><span class="line">正则表达式的方式订阅的示例如下</span><br><span class="line">consumer.subscribe(Pattern.compile (<span class="string">&quot;topic.*&quot;</span> )); </span><br><span class="line">利用正则表达式订阅主题,可实现动态订阅;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ConsumerRebalanceListener listener</span><br><span class="line"><span class="comment">//一般同一个消费组中，一旦有触发消费者的增减变化，都会触发消费组的rebalance再均衡，如果消费者a消费一批消息后还没来得及提交偏移量offset，而它所负责的分区在rebalance中转移给了消费者b，则有可能发生消息的重复消费，那么此时可以通过再均衡器做一定程度的补救。</span></span><br><span class="line">consumer.subscribe(Arrays.asList(”tpc_1<span class="string">&quot;), new ConsumerRebalanceListener()&#123; @Override </span></span><br><span class="line"><span class="string">public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; </span></span><br><span class="line"><span class="string">log.info(&quot;</span>&lt;&gt;&lt;&gt; Before start consume the message &lt;&gt;&lt;&gt;<span class="string">&quot;); &#125; </span></span><br><span class="line"><span class="string">@Override </span></span><br><span class="line"><span class="string">public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; </span></span><br><span class="line"><span class="string">log.info(&quot;</span>&lt;&gt;&lt;&gt; After stop consume the message &lt;&gt;&lt;&gt;<span class="string">&quot;); &#125; &#125;);</span></span><br></pre></td></tr></table></figure>



<h4 id="（4）assign订阅主题"><a href="#（4）assign订阅主题" class="headerlink" title="（4）assign订阅主题"></a>（4）assign订阅主题</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">消费者不仅可以通过 KafkaConsumer.subscribe() 方法订阅主题,还可直接订阅某些主题的指定分区; </span><br><span class="line"></span><br><span class="line">在 KafkaConsumer 中提供了 assign() 方法来实现这些功能,此方法的具体定义如下: </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">//这个方法只接受参数 partitions,用来指定需要订阅的分区集合。</span></span><br><span class="line"></span><br><span class="line">示例如下: </span><br><span class="line">consumer.assign(Arrays.asList(<span class="keyword">new</span> <span class="title class_">TopicPartition</span> (<span class="string">&quot;tpc_1&quot;</span> , <span class="number">0</span>),<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(“tpc_2”,<span class="number">1</span>))) ;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（5）subscribe-与assign-的区别"><a href="#（5）subscribe-与assign-的区别" class="headerlink" title="（5）subscribe 与assign 的区别"></a>（5）subscribe 与assign 的区别</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(1)通过 `subscribe()`方法订阅主题具有消费者自动再均衡功能 ; </span><br><span class="line"></span><br><span class="line">在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。 </span><br><span class="line">当消费组的消费者增加或减少时,分区分配关系会自动调整,以实现消费负载均衡及故障自动转移。</span><br><span class="line"></span><br><span class="line">(2)`assign() `方法订阅分区时,是不具备消费者自动均衡的功能的; </span><br><span class="line"></span><br><span class="line">其实这一点从 `assign()`方法参数可以看出端倪,两种类型 `subscribe()`都有 `ConsumerRebalanceListener` 类型参数的方法,而 `assign()`方法却没有。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（6）取消订阅"><a href="#（6）取消订阅" class="headerlink" title="（6）取消订阅"></a>（6）取消订阅</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//既然有订阅,那么就有取消订阅; </span></span><br><span class="line"></span><br><span class="line">可以使用 KafkaConsumer 中的 unsubscribe()方法采取消主题的订阅,这个方法既可以取消通过subscribe( Collection)方式实现的订阅; </span><br><span class="line"></span><br><span class="line">也可以取消通过 subscribe(Pattem)方式实现的订阅,还可以取消通过 assign( Collection)方式实现的订阅。示例码如下: </span><br><span class="line"></span><br><span class="line">consumer.unsubscribe(); </span><br><span class="line"></span><br><span class="line">如果将 subscribe(Collection )或 assign(Collection)集合参数设置为空集合,作用与 unsubscribe()方法相同,如下示例中三行代码的效果相同: </span><br><span class="line"></span><br><span class="line"><span class="number">1.</span>consumer.unsubscribe(  ); </span><br><span class="line"><span class="number">2.</span>consumer.subscribe(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;(  )) ; </span><br><span class="line"><span class="number">3.</span>consumer.assign(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;TopicPartition&gt;(  ));</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（7）消息的消费模式"><a href="#（7）消息的消费模式" class="headerlink" title="（7）消息的消费模式"></a>（7）消息的消费模式</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。</span></span><br><span class="line"><span class="comment">//推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息。</span></span><br><span class="line"></span><br><span class="line">Kafka 中的消息消费是一个不断轮询的过程,消费者所要做的就是重复地调用 poll() 方法,poll()方法返回的是所订阅的主题(分区)上的一组消息。</span><br><span class="line"></span><br><span class="line">对于 poll ()方法而言,如果某些分区中没有可供消费的消息,那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息,那么 poll()方法返回为空的消息集; poll () 方法具体定义如下: </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> ConsumerRecords&lt;K, V&gt; <span class="title function_">poll</span><span class="params">(<span class="keyword">final</span> Duration timeout)</span> </span><br><span class="line">    </span><br><span class="line">超时时间参数 timeout , 用来控制 poll() 方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率,可以把 timeout 设置为Long.MAX_VALUE;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//消费者消费到的每条消息的类型为 ConsumerRecord</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConsumerRecord</span>&lt;K, V&gt; &#123; </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">NO_TIMESTAMP</span> <span class="operator">=</span> RecordBatch.NO_TIMESTAMP; </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">NULL_SIZE</span> <span class="operator">=</span> -<span class="number">1</span>; </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">NULL_CHECKSUM</span> <span class="operator">=</span> -<span class="number">1</span>; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> String topic; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> partition; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> offset;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> timestamp; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> TimestampType timestampType; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedKeySize; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedValueSize; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Headers headers; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> K key; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> V value; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">volatile</span> Long checksum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-topic -partition 这两个字段分别代表消息所属主题的名称和所在分区的编号。</span><br><span class="line">-offsset 表示消息在所属分区的偏移量。</span><br><span class="line">-timestamp 表示时间戳,与此对应的 timestampType 表示时间戳的类型。</span><br><span class="line">-timestampType 有两种类型 CreateTime 和 LogAppendTime , 分别代表消息创建的时间戳和消息追加到日志的时间戳。</span><br><span class="line">-headers 表示消息的头部内容。</span><br><span class="line">-key value 分别表示消息的键和消息的值,一般业务应用要读取的就是 value ; </span><br><span class="line">-serializedKeySize、serializedValueSize 分别表示 key、value 经过序列化之后的大小,如果 key 为空, 则 serializedKeySize 值为 -1,同样,如果 value 为空,则 serializedValueSize 的值也会为 -1; </span><br><span class="line">-checksum 是 CRC32 的校验值。</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//示例代码片段</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 订阅与消费方式2</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="type">TopicPartition</span> <span class="variable">tp1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;x&quot;</span>, <span class="number">0</span>);</span><br><span class="line"><span class="type">TopicPartition</span> <span class="variable">tp2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;y&quot;</span>, <span class="number">0</span>);</span><br><span class="line"><span class="type">TopicPartition</span> <span class="variable">tp3</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;z&quot;</span>, <span class="number">0</span>);</span><br><span class="line">List&lt;TopicPartition&gt; tps = Arrays.asList(tp1, tp2, tp3);</span><br><span class="line">consumer.assign(tps);</span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">	ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">	<span class="keyword">for</span> (TopicPartition tp : tps) &#123;</span><br><span class="line">		List&lt;ConsumerRecord&lt;String, String&gt;&gt; rList = records.records(tp);</span><br><span class="line">		<span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; r : rList) &#123;</span><br><span class="line">			r.topic();</span><br><span class="line">			r.partition();</span><br><span class="line">			r.offset();</span><br><span class="line">			r.value();</span><br><span class="line">			<span class="comment">//do something to process record.</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="（8）指定位移消费"><a href="#（8）指定位移消费" class="headerlink" title="（8）指定位移消费"></a>（8）指定位移消费</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">有些时候,我们需要一种更细粒度的掌控,可以让我们从特定的位移处开始拉取消息,而KafkaConsumer 中的 seek() 方法正好提供了这个功能,让我们可以追前消费或回溯消费。</span><br><span class="line"></span><br><span class="line">seek()方法的具体定义如下: </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">seek</span><span class="params">(TopicPartiton partition,<span class="type">long</span> offset)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//代码示例</span></span><br><span class="line"><span class="comment">// 在调用seek 方法之前，需要先调用一次poll，以分配到分区</span></span><br><span class="line">consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所分配到的分区信息</span></span><br><span class="line">Set&lt;TopicPartition&gt; assignment = consumer.assignment();</span><br><span class="line"><span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">	<span class="comment">// 为指定partition 设置读取起始offset</span></span><br><span class="line">	consumer.seek(topicPartition, <span class="number">80</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 开始正式消费</span></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">	ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">	<span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">	<span class="comment">// do some process</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="（9）再均衡监听器"><a href="#（9）再均衡监听器" class="headerlink" title="（9）再均衡监听器"></a>（9）再均衡监听器</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡; </span><br><span class="line">如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//代码示例</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 再均衡处理</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">consumer.subscribe(Collections.singletonList(<span class="string">&quot;tpc_5&quot;</span>), <span class="keyword">new</span> <span class="title class_">ConsumerRebalanceListener</span>() &#123;</span><br><span class="line">	<span class="comment">// 再均衡开始前和消费者停止读取消息之后，被调用</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; collection)</span> &#123;</span><br><span class="line">	<span class="comment">// store the current offset to db</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// 重新分配到分区后和消费者开始读取消息之前，被调用</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; collection)</span> &#123;</span><br><span class="line">	<span class="comment">// fetch the current offset from db</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<h4 id="（10）自动位移提交"><a href="#（10）自动位移提交" class="headerlink" title="（10）自动位移提交"></a>（10）自动位移提交</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Kafka 中默认的消费位移的提交方式是自动提交,这个由消费者客户端参数 `enable.auto.commit `配置, 默认值为 <span class="literal">true</span> 。</span><br><span class="line"></span><br><span class="line">当然这个默认的自动提交不是每消费一条消息就提交一次,而是定期提交,这个定期的周期时间由客户端参数 `auto.commit.interval.ms` 配置, 默认值为 5 秒, 此参数生效的前提是 <span class="built_in">enable</span>.</span><br><span class="line">`auto.commit` 参数为 <span class="literal">true</span>。</span><br><span class="line"></span><br><span class="line">在默认的方式下,消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在 `poll() `方法的逻辑里完成的,在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交,如果可以,那么就会提交上一次轮询的位移。</span><br><span class="line"></span><br><span class="line"><span class="comment">#Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重复消费</span></span><br><span class="line">假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。</span><br><span class="line"><span class="comment">#丢失消息</span></span><br><span class="line">按照一般思维逻辑而言,自动提交是延时提交,重复消费可以理解,那么消息丢失又是在什么情形下会发生的呢?我们来看下图中的情形: 拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第 y+1次拉取,以及第 m 次位移提交的时候,也就是x+6 之前的位移己经确认提交了, 处理线程却还正在处理 x+3 的消息; 此时如果处理线程发生了异常, 待其恢复之后会从第 m 次位移提交处,也就是 x+6 的位置开始拉取消息,那么 x+3 至 x+6 之间的消息就没有得到相应的处理,这样便发生消息丢失的现象。</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据丢失不会发生在kafka集群里</span></span><br><span class="line"><span class="comment">#往往发生在处理具体业务逻辑的缓存里</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<img src=".\md图\kafka.assets\image-20230221221618036.png" alt="image-20230221221618036" style="zoom:50%;" />

<h4 id="（11）手动位移提交（调用kafka-api）"><a href="#（11）手动位移提交（调用kafka-api）" class="headerlink" title="（11）手动位移提交（调用kafka api）"></a>（11）手动位移提交（调用kafka api）</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象, 但是在编程的世界里异常无可避免; 同时, 自动位移提交也无法做到精确的位移管理。 </span><br><span class="line"></span><br><span class="line">在 Kafka 中还提供了手动位移提交的方式, 这样可以使得开发人员对消费位移的管理控制更加灵活。</span><br><span class="line">    </span><br><span class="line">很多时候并不是说拉取到消息就算消费完成,而是需要将消息写入数据库、写入本地缓存,或者是更加复杂的业务处理。在这些场景下,所有的业务处理完成才能认为消息被成功消费; 手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。 </span><br><span class="line">开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 <span class="literal">false</span> ,示例如下</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>); </span><br><span class="line"></span><br><span class="line">手动提交可以细分为同步提交和异步提交,对应于 KafkaConsumer 中的 commitSync()和commitAsync()两种类型的方法。</span><br></pre></td></tr></table></figure>

<p><code>1.同步提交的方式</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//commitSync()方法的定义如下:</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 手动提交offset</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">	ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">	<span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; r : records) &#123;</span><br><span class="line"><span class="comment">//do something to process record.</span></span><br><span class="line">	&#125;</span><br><span class="line">	consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//对于采用 commitSync()的无参方法,它提交消费位移的频率和拉取批次消息、处理批次消息的频率是一样的, 如果想寻求更细粒度的、 更精准的提交, 那么就需要使用 commitSync()的另一个有参方法,具体定义如下：</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitSync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition，OffsetAndMetadata&gt; offsets)</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">//示例代码如下：</span></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">	ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">	<span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; r : records) &#123;</span><br><span class="line">		<span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> r.offset();</span><br><span class="line">		<span class="comment">//do something to process record.</span></span><br><span class="line">		<span class="type">TopicPartition</span> <span class="variable">topicPartition</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(r.topic(), 	r.partition());</span><br><span class="line">		consumer.commitSync(Collections.singletonMap(topicPartition,<span class="keyword">new</span> 		<span class="title class_">OffsetAndMetadata</span>(offset+<span class="number">1</span>)));</span><br><span class="line">&#125;</span><br><span class="line">&#125;   </span><br><span class="line"><span class="comment">//提交的偏移量= 消费完的 record 的偏移量+ 1 因为,__consumer_offsets 中记录的消费偏移量,代表的是,消费者下一次要读取的位置</span></span><br></pre></td></tr></table></figure>

<p><code>2.异步提交方式</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//commitSync()方法相反,异步提交的方式( commitAsync())在执行的时候消费者线程不会被阻塞;可能在提交消费位移的结果还未返回之前就开始了新一次的拉取操 。异步提交以便消费者的性能得到一定的增强。 commitAsync 方法有一个不同的重载方法,具体定义如下</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitAsync</span><span class="params">()</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitAsync</span><span class="params">(0ffsetCommitcallback callback)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitAsync</span> <span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition,OffsetAndMetadata&gt; offsets,</span></span><br><span class="line"><span class="params">OffsetCommitCallback callback)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//示例代码</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 异步提交offset</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">	ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">	<span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; r : records) &#123;</span><br><span class="line">		<span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> r.offset();</span><br><span class="line">		<span class="comment">//do something to process record.</span></span><br><span class="line">		<span class="type">TopicPartition</span> <span class="variable">topicPartition</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(r.topic(), r.partition());</span><br><span class="line">		consumer.commitSync(Collections.singletonMap(topicPartition,<span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(offset+<span class="number">1</span>)));</span><br><span class="line">		consumer.commitAsync(Collections.singletonMap(topicPartition, <span class="keyword">new</span> 	<span class="title class_">OffsetAndMetadata</span>(offset + <span class="number">1</span>)), <span class="keyword">new</span></span><br><span class="line"><span class="title class_">OffsetCommitCallback</span>() &#123;</span><br><span class="line">		<span class="meta">@Override</span></span><br><span class="line">		<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; map, Exception e)</span> &#123;</span><br><span class="line">			<span class="keyword">if</span>(e == <span class="literal">null</span> )&#123;</span><br><span class="line">				System.out.println(map);</span><br><span class="line">			&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">				System.out.println(<span class="string">&quot;error commit offset&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;   </span><br></pre></td></tr></table></figure>

<h4 id="（12）其他重要参数"><a href="#（12）其他重要参数" class="headerlink" title="（12）其他重要参数"></a>（12）其他重要参数</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">fetch.min.bytes</span>=<span class="string">1B		 		一次拉取的最小字节数</span></span><br><span class="line"></span><br><span class="line"><span class="attr">fetch.max.bytes</span>=<span class="string">50M 				一次拉取的最大数据量</span></span><br><span class="line"></span><br><span class="line"><span class="attr">fetch.max.wait.ms</span>=<span class="string">500ms 			拉取时的最大等待时长</span></span><br><span class="line"></span><br><span class="line"><span class="attr">max.partition.fetch.bytes</span> = <span class="string">1MB 		每个分区一次拉取的最大数据量</span></span><br><span class="line"></span><br><span class="line"><span class="attr">max.poll.records</span>=<span class="string">500				一次拉取的最大条数</span></span><br><span class="line"><span class="attr">connections.max.idle.ms</span>=<span class="string">540000ms 	网络连接的最大闲置时长</span></span><br><span class="line"></span><br><span class="line"><span class="attr">request.timeout.ms</span>=<span class="string">30000ms 一次请求等待响应的最大超时时间consumer 等待请求响应的最长时间</span></span><br><span class="line"></span><br><span class="line"><span class="attr">metadata.max.age.ms</span>=<span class="string">300000 	元数据在限定时间内没有进行更新,则会被强制更新</span></span><br><span class="line"></span><br><span class="line"><span class="attr">reconnect.backoff.ms</span>=<span class="string">50ms 		尝试重新连接指定主机之前的退避时间</span></span><br><span class="line"></span><br><span class="line"><span class="attr">retry.backoff.ms</span>=<span class="string">100ms 		尝试重新拉取数据的重试间隔</span></span><br><span class="line"><span class="attr">isolation.level</span>=<span class="string">read_uncommitted 		隔离级别! 决定消费者能读到什么样的数据</span></span><br><span class="line"></span><br><span class="line"><span class="attr">read_uncommitted</span>:				<span class="string">可以消费到 LSO(LastStableOffset)位置; </span></span><br><span class="line"></span><br><span class="line"><span class="attr">read_committed</span>:				<span class="string">可以消费到 HW(High Watermark)位置</span></span><br><span class="line"></span><br><span class="line"><span class="attr">max.poll.interval.ms</span> 			<span class="string">超过时限没有发起 poll 操作,则消费组认为该消费者已离开消费组</span></span><br><span class="line"></span><br><span class="line"><span class="attr">enable.auto.commit</span>=<span class="string">true 			开启消费位移的自动提交</span></span><br><span class="line"></span><br><span class="line"><span class="attr">auto.commit.interval.ms</span>=<span class="string">5000 		自动提交消费位移的时间间隔</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>











<h3 id="5-3-API-开发：topic-管理"><a href="#5-3-API-开发：topic-管理" class="headerlink" title="5.3 API 开发：topic 管理"></a>5.3 API 开发：topic 管理</h3><p>一般情况下,我们都习惯使用 kafka-topic.sh 本来管理主题,如果希望将管理类的功能集成到公司内部的系统中,打造集管理、监控、运维、告警为一体的生态平台,那么就需要以程序调用 API 方式去实现。</p>
<p>这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类</p>
<p>KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">-创建主题:CreateTopicsResult <span class="title function_">createTopics</span><span class="params">(Collection&lt;NewTopic&gt; newTopics)</span>。</span><br><span class="line"></span><br><span class="line">-删除主题:DeleteTopicsResult <span class="title function_">deleteTopics</span><span class="params">(Collection&lt;String&gt; topics)</span>。</span><br><span class="line">    </span><br><span class="line">-列出所有可用的主题:ListTopicsResult <span class="title function_">listTopics</span><span class="params">()</span>。</span><br><span class="line">    </span><br><span class="line">-查看主题的信息:DescribeTopicsResult <span class="title function_">describeTopics</span><span class="params">(Collection&lt;String&gt; topicNames)</span>。</span><br><span class="line"></span><br><span class="line">-查询配置信息:DescribeConfigsResult <span class="title function_">describeConfigs</span><span class="params">(Collection&lt;ConfigResource&gt;resources)</span>。</span><br><span class="line">    </span><br><span class="line">-修改配置信息:AlterConfigsResult <span class="title function_">alterConfigs</span><span class="params">(Map&lt;ConfigResource, Config&gt; configs)</span>。</span><br><span class="line"></span><br><span class="line">-增加分区:CreatePartitionsResult <span class="title function_">createPartitions</span><span class="params">(Map&lt;String, NewPartitions&gt; newPartitions)</span>。</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">构造一个 KafkaAdminClient </span><br><span class="line"><span class="type">AdminClient</span> <span class="variable">adminClient</span> <span class="operator">=</span> KafkaAdminClient.create(props);</span><br></pre></td></tr></table></figure>

<h4 id="（1）列出主题"><a href="#（1）列出主题" class="headerlink" title="（1）列出主题"></a>（1）列出主题</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ListTopicsResult</span> <span class="variable">listTopicsResult</span> <span class="operator">=</span> adminClient.listTopics(); </span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; topics = listTopicsResult.names().get(); </span><br><span class="line"></span><br><span class="line">System.out.println(topics);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（2）查看主题信息"><a href="#（2）查看主题信息" class="headerlink" title="（2）查看主题信息"></a>（2）查看主题信息</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DescribeTopicsResult</span> <span class="variable">describeTopicsResult</span> <span class="operator">=</span> adminClient.describeTopics(Arrays.asList(<span class="string">&quot;tpc_4&quot;</span>, <span class="string">&quot;tpc_3&quot;</span>)); </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Map&lt;String, TopicDescription&gt; res = describeTopicsResult.all().get();</span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; ksets = res.keySet(); </span><br><span class="line"><span class="keyword">for</span> (String k : ksets) &#123; </span><br><span class="line">	System.out.println(res.get(k)); </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（3）创建主题"><a href="#（3）创建主题" class="headerlink" title="（3）创建主题"></a>（3）创建主题</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//代码示例:</span></span><br><span class="line"><span class="comment">// 参数配置</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line">props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>); </span><br><span class="line">props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,<span class="number">3000</span>); </span><br><span class="line"><span class="comment">// 创建 admin client 对象</span></span><br><span class="line"><span class="type">AdminClient</span> <span class="variable">adminClient</span> <span class="operator">=</span> KafkaAdminClient.create(props); </span><br><span class="line"><span class="comment">// 由服务端 controller 自行分配分区及副本所在 broker </span></span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_3</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_3&quot;</span>, <span class="number">2</span>, (<span class="type">short</span>) <span class="number">1</span>); </span><br><span class="line"><span class="comment">// 手动指定分区及副本的 broker 分配</span></span><br><span class="line">HashMap&lt;Integer, List&lt;Integer&gt;&gt; replicaAssignments = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;(); </span><br><span class="line"><span class="comment">// 分区 0,分配到 broker0,broker1 replicaAssignments.put(0,Arrays.asList(0,1)); </span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 分区 1,分配到 broker0,broker2 </span></span><br><span class="line">replicaAssignments.put(<span class="number">0</span>,Arrays.asList(<span class="number">0</span>,<span class="number">1</span>));</span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_4</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_4&quot;</span>, replicaAssignments); </span><br><span class="line"><span class="type">CreateTopicsResult</span> <span class="variable">result</span> <span class="operator">=</span> adminClient.createTopics(Arrays.asList(tpc_3,tpc_4)); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 从 future 中等待服务端返回</span></span><br><span class="line"><span class="keyword">try</span> &#123; </span><br><span class="line">	result.all().get(); </span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123; </span><br><span class="line">e.printStackTrace(); </span><br><span class="line">&#125; </span><br><span class="line">adminClient.close();</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="（4）删除主题"><a href="#（4）删除主题" class="headerlink" title="（4）删除主题"></a>（4）删除主题</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//代码示例: </span></span><br><span class="line"><span class="type">DeleteTopicsResult</span> <span class="variable">deleteTopicsResult</span> <span class="operator">=</span> adminClient.deleteTopics(Arrays.asList(<span class="string">&quot;tpc_1&quot;</span>, <span class="string">&quot;tpc_1&quot;</span>)); </span><br><span class="line"></span><br><span class="line">Map&lt;String, KafkaFuture&lt;Void&gt;&gt; values = deleteTopicsResult.values();</span><br><span class="line"></span><br><span class="line">System.out.println(values);</span><br></pre></td></tr></table></figure>

<h4 id="（5）其他管理"><a href="#（5）其他管理" class="headerlink" title="（5）其他管理"></a>（5）其他管理</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">除了进行 topic 管理之外,KafkaAdminClient 也可以进行诸如动态参数管理,分区管理等各类管理操作;</span><br></pre></td></tr></table></figure>

<h3 id="5-4-复习"><a href="#5-4-复习" class="headerlink" title="5.4 复习"></a>5.4 复习</h3><h4 id="1-生产者程序开发"><a href="#1-生产者程序开发" class="headerlink" title="1.生产者程序开发"></a>1.生产者程序开发</h4><ol>
<li>创建连接<ul>
<li>bootstrap.servers：Kafka的服务器地址</li>
<li>acks：表示当生产者生产数据到Kafka中，Kafka中会以什么样的策略返回</li>
<li>key.serializer：Kafka中的消息是以key、value键值对存储的，而且生产者生产的消息是需要在网络上传到的，这里指定的是StringSerializer方式，就是以字符串方式发送（将来还可以使用其他的一些序列化框架：Google ProtoBuf、Avro）</li>
<li>value.serializer：同上</li>
</ul>
</li>
<li>创建一个生产者对象KafkaProducer</li>
<li>调用send方法发送消息（ProducerRecor，封装是key-value键值对）</li>
<li>调用Future.get表示等带服务端的响应</li>
<li>关闭生产者</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaProducerTest</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1. 创建用于连接Kafka的Properties配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1.itcast.cn:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 创建一个生产者对象KafkaProducer</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 发送1-100的消息到指定的topic中</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; ++i) &#123;</span><br><span class="line">            <span class="comment">// 构建一条消息，直接new ProducerRecord</span></span><br><span class="line">            ProducerRecord&lt;String, String&gt; producerRecord = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;test&quot;</span>, <span class="literal">null</span>, i + <span class="string">&quot;&quot;</span>);</span><br><span class="line">            Future&lt;RecordMetadata&gt; future = kafkaProducer.send(producerRecord);</span><br><span class="line">            <span class="comment">// 调用Future的get方法等待响应</span></span><br><span class="line">            future.get();</span><br><span class="line">            System.out.println(<span class="string">&quot;第&quot;</span> + i + <span class="string">&quot;条消息写入成功！&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.关闭生产者</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-消费者程序开发"><a href="#2-消费者程序开发" class="headerlink" title="2.消费者程序开发"></a>2.消费者程序开发</h4><ul>
<li>group.id：消费者组的概念，可以在一个消费组中包含多个消费者。如果若干个消费者的group.id是一样的，表示它们就在一个组中，一个组中的消费者是共同消费Kafka中topic的数据。</li>
<li>Kafka是一种拉消息模式的消息队列，在消费者中会有一个offset，表示从哪条消息开始拉取数据</li>
<li>kafkaConsumer.poll：Kafka的消费者API是一批一批数据的拉取</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 消费者程序</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 1.创建Kafka消费者配置</span></span><br><span class="line"><span class="comment"> * Properties props = new Properties();</span></span><br><span class="line"><span class="comment"> * props.setProperty(&quot;bootstrap.servers&quot;, &quot;node1.itcast.cn:9092&quot;);</span></span><br><span class="line"><span class="comment"> * props.setProperty(&quot;group.id&quot;, &quot;test&quot;);</span></span><br><span class="line"><span class="comment"> * props.setProperty(&quot;enable.auto.commit&quot;, &quot;true&quot;);</span></span><br><span class="line"><span class="comment"> * props.setProperty(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);</span></span><br><span class="line"><span class="comment"> * props.setProperty(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span></span><br><span class="line"><span class="comment"> * props.setProperty(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 2.创建Kafka消费者</span></span><br><span class="line"><span class="comment"> * 3.订阅要消费的主题</span></span><br><span class="line"><span class="comment"> * 4.使用一个while循环，不断从Kafka的topic中拉取消息</span></span><br><span class="line"><span class="comment"> * 5.将将记录（record）的offset、key、value都打印出来</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConsumerTest</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建Kafka消费者配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1.itcast.cn:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// 消费者组（可以使用消费者组将若干个消费者组织到一起），共同消费Kafka中topic的数据</span></span><br><span class="line">        <span class="comment">// 每一个消费者需要指定一个消费者组，如果消费者的组名是一样的，表示这几个消费者是一个组中的</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">        <span class="comment">// 自动提交offset</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">        <span class="comment">// 自动提交offset的时间间隔</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line">        <span class="comment">// 拉取的key、value数据的</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.创建Kafka消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 订阅要消费的主题</span></span><br><span class="line">        <span class="comment">// 指定消费者从哪个topic中拉取数据</span></span><br><span class="line">        kafkaConsumer.subscribe(Arrays.asList(<span class="string">&quot;test&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.使用一个while循环，不断从Kafka的topic中拉取消息</span></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// Kafka的消费者一次拉取一批的数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">5</span>));</span><br><span class="line">            <span class="comment">// 5.将将记录（record）的offset、key、value都打印出来</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                <span class="comment">// 主题</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> consumerRecord.topic();</span><br><span class="line">                <span class="comment">// offset：这条消息处于Kafka分区中的哪个位置</span></span><br><span class="line">                <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> consumerRecord.offset();</span><br><span class="line">                <span class="comment">// key\value</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> consumerRecord.key();</span><br><span class="line">                <span class="type">String</span> <span class="variable">value</span> <span class="operator">=</span> consumerRecord.value();</span><br><span class="line"></span><br><span class="line">                System.out.println(<span class="string">&quot;topic: &quot;</span> + topic + <span class="string">&quot; offset:&quot;</span> + offset + <span class="string">&quot; key:&quot;</span> + key + <span class="string">&quot; value:&quot;</span> + value);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="3-生产者使用异步方式生产消息"><a href="#3-生产者使用异步方式生产消息" class="headerlink" title="3.生产者使用异步方式生产消息"></a>3.生产者使用异步方式生产消息</h4><ul>
<li>使用匿名内部类实现Callback接口，该接口中表示Kafka服务器响应给客户端，会自动调用onCompletion方法<ul>
<li>metadata：消息的元数据（属于哪个topic、属于哪个partition、对应的offset是什么）</li>
<li>exception：这个对象Kafka生产消息封装了出现的异常，如果为null，表示发送成功，如果不为null，表示出现异常。</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 二、使用异步回调的方式发送消息</span></span><br><span class="line">ProducerRecord&lt;String, String&gt; producerRecord = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;test&quot;</span>, <span class="literal">null</span>, i + <span class="string">&quot;&quot;</span>);</span><br><span class="line">kafkaProducer.send(producerRecord, <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 判断发送消息是否成功</span></span><br><span class="line">        <span class="keyword">if</span>(exception == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="comment">// 发送成功</span></span><br><span class="line">            <span class="comment">// 主题</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> metadata.topic();</span><br><span class="line">            <span class="comment">// 分区id</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">partition</span> <span class="operator">=</span> metadata.partition();</span><br><span class="line">            <span class="comment">// 偏移量</span></span><br><span class="line">            <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> metadata.offset();</span><br><span class="line">            System.out.println(<span class="string">&quot;topic:&quot;</span> + topic + <span class="string">&quot; 分区id：&quot;</span> + partition + <span class="string">&quot; 偏移量：&quot;</span> + offset);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 发送出现错误</span></span><br><span class="line">            System.out.println(<span class="string">&quot;生产消息出现异常！&quot;</span>);</span><br><span class="line">            <span class="comment">// 打印异常消息</span></span><br><span class="line">            System.out.println(exception.getMessage());</span><br><span class="line">            <span class="comment">// 打印调用栈</span></span><br><span class="line">            System.out.println(exception.getStackTrace());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<h2 id="六、Kafka整合"><a href="#六、Kafka整合" class="headerlink" title="六、Kafka整合"></a>六、Kafka整合</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Kafka 和flume 的整合有3 种方式：</span><br><span class="line">1.把kafka 当做source 的数据源</span><br><span class="line">2.把kafka 当做channel</span><br><span class="line">3.把kafka 作为sink 的目标存储</span><br></pre></td></tr></table></figure>

<h3 id="6-1-Kafka-Flume"><a href="#6-1-Kafka-Flume" class="headerlink" title="6.1 Kafka+Flume"></a>6.1 Kafka+Flume</h3><h4 id="6-1-1-Flume-从kafka-source-中读取数据"><a href="#6-1-1-Flume-从kafka-source-中读取数据" class="headerlink" title="6.1.1 Flume 从kafka-source  中读取数据"></a>6.1.1 Flume 从kafka-source  中读取数据</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">org.apache.flume.source.kafka.KafkaSource</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.consumer.group.id</span> = <span class="string">g00001</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.topics</span> = <span class="string">tpc_2</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.consumer.auto.offset.reset</span> = <span class="string">earliest</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">2000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">org.apache.flume.sink.kafka.KafkaSink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.topic</span> = <span class="string">tpc_3</span></span><br><span class="line"><span class="attr">a1.sinks.k1.flumeBatchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.acks</span> = <span class="string">-1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.allowTopicOverride</span> = <span class="string">false</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.linger.ms</span> = <span class="string">1000</span></span><br></pre></td></tr></table></figure>

<h4 id="6-1-2-Flume-把kafka-作为channel"><a href="#6-1-2-Flume-把kafka-作为channel" class="headerlink" title="6.1.2 Flume 把kafka 作为channel"></a>6.1.2 Flume 把kafka 作为channel</h4><p>有两种方式：<br>（1）配了一个source+channel</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="attr">a1.sources.r1.command</span> = <span class="string">tail -F /root/abc.log</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">org.apache.flume.channel.kafka.KafkaChannel</span></span><br><span class="line"><span class="attr">a1.channels.c1.kafka.topic</span> = <span class="string">flume-channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br></pre></td></tr></table></figure>

<p>（2）配了一个channle+sink</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">org.apache.flume.channel.kafka.KafkaChannel</span></span><br><span class="line"><span class="attr">a1.channels.c1.kafka.topic</span> = <span class="string">flume-channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.path</span> = <span class="string">hdfs://node1:8020/logdata/%Y-%m-%d/%H/</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.filePrefix</span> = <span class="string">logdata_</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileSuffix</span> = <span class="string">.log</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollInterval</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollSize</span> = <span class="string">268435456</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollCount</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.codeC</span> = <span class="string">gzip</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileType</span> = <span class="string">CompressedStream</span></span><br></pre></td></tr></table></figure>

<h4 id="6-1-3-Flume-用kafka-sink-把数据写入kafka"><a href="#6-1-3-Flume-用kafka-sink-把数据写入kafka" class="headerlink" title="6.1.3 Flume 用kafka-sink 把数据写入kafka"></a>6.1.3 Flume 用kafka-sink 把数据写入kafka</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">org.apache.flume.source.kafka.KafkaSource</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.consumer.group.id</span> = <span class="string">g00001</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.topics</span> = <span class="string">tpc_2</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.consumer.auto.offset.reset</span> = <span class="string">earliest</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">2000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">org.apache.flume.sink.kafka.KafkaSink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.topic</span> = <span class="string">tpc_3</span></span><br><span class="line"><span class="attr">a1.sinks.k1.flumeBatchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.acks</span> = <span class="string">-1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.allowTopicOverride</span> = <span class="string">false</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.linger.ms</span> = <span class="string">1000</span></span><br></pre></td></tr></table></figure>

<h4 id="6-1-4-案例"><a href="#6-1-4-案例" class="headerlink" title="6.1.4 案例"></a>6.1.4 案例</h4><p>（1）配置flume</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="comment"># source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="attr">a1.sources.r1.command</span> = <span class="string">tail -F -c +0 /root/flume.log</span></span><br><span class="line"><span class="attr">a1.sources.r1.shell</span> = <span class="string">/bin/bash -c</span></span><br><span class="line"><span class="comment"># sink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">org.apache.flume.sink.kafka.KafkaSink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.topic</span> = <span class="string">test</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.flumeBatchSize</span> = <span class="string">20</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.acks</span> = <span class="string">1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.linger.ms</span> = <span class="string">1</span></span><br><span class="line"><span class="comment"># channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"><span class="comment"># bind</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>

<p>（2）启动客户端消费者<br>（3）启动flume，进入flume bin 目录下<br>.&#x2F;flume-ng agent -c conf&#x2F; -n a1 -f jobs&#x2F;flume-kafka.conf<br>（4）向日志文件增加数据查看消费情况<br>echo hello &gt;&gt; &#x2F;root&#x2F;flume.log</p>
<h3 id="6-2-Kafka-sparkStreaming"><a href="#6-2-Kafka-sparkStreaming" class="headerlink" title="6.2 Kafka+sparkStreaming"></a>6.2 Kafka+sparkStreaming</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//以workCount 示意：</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">		<span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;spark streaming 整合kafka&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        </span><br><span class="line">		<span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line">		<span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">			<span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>,</span><br><span class="line">			<span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">			<span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">			<span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;use_a_separate_group_id_for_each_stream&quot;</span>,</span><br><span class="line">			<span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;earliest&quot;</span>,</span><br><span class="line">			<span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">			)</span><br><span class="line">		<span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">		<span class="comment">// 获取数据</span></span><br><span class="line">		<span class="keyword">val</span> stream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = 		<span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">ssc,</span><br><span class="line"><span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>, <span class="comment">// 如果计算节点和Broker 是同一台节点可以使用PreferBrokers</span></span><br><span class="line"><span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">)</span><br><span class="line">		stream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words: <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = rdd.map(_.value()).map(line =&gt; line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">			<span class="keyword">val</span> wordAndOne: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = words.flatMap(arr =&gt; arr.map(word =&gt; (word, <span class="number">1</span>)))</span><br><span class="line">		<span class="comment">// RDD[(K, V)]</span></span><br><span class="line">			<span class="keyword">val</span> wordCountResult = wordAndOne.reduceByKey(_ + _)</span><br><span class="line">wordCountResult.foreach(println)</span><br><span class="line">        &#125;)</span><br><span class="line">	ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="七、kafka原理加强"><a href="#七、kafka原理加强" class="headerlink" title="七、kafka原理加强"></a>七、kafka原理加强</h2><h3 id="7-1-Kafka中的重要概念"><a href="#7-1-Kafka中的重要概念" class="headerlink" title="7.1 Kafka中的重要概念"></a>7.1 Kafka中的重要概念</h3><h4 id="（1）架构组件"><a href="#（1）架构组件" class="headerlink" title="（1）架构组件"></a>（1）架构组件</h4><ul>
<li>broker<ul>
<li>Kafka服务器进程，生产者、消费者都要连接broker</li>
<li>一个集群由多个broker组成，功能实现Kafka集群的负载均衡、容错</li>
</ul>
</li>
<li>producer：生产者</li>
<li>consumer：消费者</li>
<li>topic：主题，一个Kafka集群中，可以包含多个topic。一个topic可以包含多个分区<ul>
<li>是一个逻辑结构，生产、消费消息都需要指定topic</li>
</ul>
</li>
<li>partition：Kafka集群的分布式就是由分区来实现的。一个topic中的消息可以分布在topic中的不同partition中</li>
<li>replica：副本，实现Kafkaf集群的容错，实现partition的容错。一个topic至少应该包含大于1个的副本</li>
<li>consumer group：消费者组，一个消费者组中的消费者可以共同消费topic中的分区数据。每一个消费者组都一个唯一的名字。配置group.id一样的消费者是属于同一个组中</li>
<li>offset：偏移量。相对消费者、partition来说，可以通过offset来拉取数据</li>
</ul>
<h4 id="（2）消费者组"><a href="#（2）消费者组" class="headerlink" title="（2）消费者组"></a>（2）消费者组</h4><ul>
<li>一个消费者组中可以包含多个消费者，共同来消费topic中的数据</li>
<li>一个topic中如果只有一个分区，那么这个分区只能被某个组中的一个消费者消费</li>
<li>有多少个分区，那么就可以被同一个组内的多少个消费者消费</li>
</ul>
<h4 id="（3）幂等性"><a href="#（3）幂等性" class="headerlink" title="（3）幂等性"></a>（3）幂等性</h4><ul>
<li><p>生产者消息重复问题</p>
<ul>
<li>Kafka生产者生产消息到partition，如果直接发送消息，kafka会将消息保存到分区中，但Kafka会返回一个ack给生产者，表示当前操作是否成功，是否已经保存了这条消息。如果ack响应的过程失败了，此时生产者会重试，继续发送没有发送成功的消息，Kafka又会保存一条一模一样的消息</li>
</ul>
</li>
<li><p>在Kafka中可以开启幂等性</p>
<ul>
<li>当Kafka的生产者生产消息时，会增加一个pid（生产者的唯一编号）和sequence number（针对消息的一个递增序列）</li>
<li>发送消息，会连着pid和sequence number一块发送</li>
<li>kafka接收到消息，会将消息和pid、sequence number一并保存下来</li>
<li>如果ack响应失败，生产者重试，再次发送消息时，Kafka会根据pid、sequence number是否需要再保存一条消息</li>
<li>判断条件：生产者发送过来的sequence number 是否小于等于 partition中消息对应的sequence</li>
</ul>
</li>
</ul>
<h3 id="7-2-事务编程"><a href="#7-2-事务编程" class="headerlink" title="7.2 事务编程"></a>7.2 事务编程</h3><ul>
<li><p>开启事务的条件</p>
<ul>
<li><p>生产者</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启事务必须要配置事务的ID</span></span><br><span class="line">props.put(<span class="string">&quot;transactional.id&quot;</span>, <span class="string">&quot;dwd_user&quot;</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>消费者</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 配置事务的隔离级别</span></span><br><span class="line">props.put(<span class="string">&quot;isolation.level&quot;</span>,<span class="string">&quot;read_committed&quot;</span>);</span><br><span class="line"><span class="comment">// 关闭自动提交，一会我们需要手动来提交offset，通过事务来维护offset</span></span><br><span class="line">props.setProperty(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>生产者</p>
<ul>
<li>初始化事务</li>
<li>开启事务</li>
<li>需要使用producer来将消费者的offset提交到事务中</li>
<li>提交事务</li>
<li>如果出现异常回滚事务</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>如果使用了事务，不要使用异步发送</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TransactionProgram</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 调用之前实现的方法，创建消费者、生产者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = createConsumer();</span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = createProducer();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 生产者调用initTransactions初始化事务</span></span><br><span class="line">        producer.initTransactions();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 编写一个while死循环，在while循环中不断拉取数据，进行处理后，再写入到指定的topic</span></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// (1)	生产者开启事务</span></span><br><span class="line">                producer.beginTransaction();</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 这个Map保存了topic对应的partition的偏移量</span></span><br><span class="line">                Map&lt;TopicPartition, OffsetAndMetadata&gt; offsetMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 从topic中拉取一批的数据</span></span><br><span class="line">                <span class="comment">// (2)	消费者拉取消息</span></span><br><span class="line">                ConsumerRecords&lt;String, String&gt; concumserRecordArray = consumer.poll(Duration.ofSeconds(<span class="number">5</span>));</span><br><span class="line">                <span class="comment">// (3)	遍历拉取到的消息，并进行预处理</span></span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; cr : concumserRecordArray) &#123;</span><br><span class="line">                    <span class="comment">// 将1转换为男，0转换为女</span></span><br><span class="line">                    <span class="type">String</span> <span class="variable">msg</span> <span class="operator">=</span> cr.value();</span><br><span class="line">                    String[] fieldArray = msg.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 将消息的偏移量保存</span></span><br><span class="line">                    <span class="comment">// 消费的是ods_user中的数据</span></span><br><span class="line">                    <span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> cr.topic();</span><br><span class="line">                    <span class="type">int</span> <span class="variable">partition</span> <span class="operator">=</span> cr.partition();</span><br><span class="line">                    <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> cr.offset();</span><br><span class="line"></span><br><span class="line">                	<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span> / <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// offset + 1：offset是当前消费的记录（消息）对应在partition中的offset，而我们希望下一次能继续从下一个消息消息</span></span><br><span class="line">                    <span class="comment">// 必须要+1，从能消费下一条消息</span></span><br><span class="line">                    offsetMap.put(<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(topic, partition), <span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(offset + <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 将字段进行替换</span></span><br><span class="line">                    <span class="keyword">if</span>(fieldArray != <span class="literal">null</span> &amp;&amp; fieldArray.length &gt; <span class="number">2</span>) &#123;</span><br><span class="line">                        <span class="type">String</span> <span class="variable">sexField</span> <span class="operator">=</span> fieldArray[<span class="number">1</span>];</span><br><span class="line">                        <span class="keyword">if</span>(sexField.equals(<span class="string">&quot;1&quot;</span>)) &#123;</span><br><span class="line">                            fieldArray[<span class="number">1</span>] = <span class="string">&quot;男&quot;</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">else</span> <span class="keyword">if</span>(sexField.equals(<span class="string">&quot;0&quot;</span>))&#123;</span><br><span class="line">                            fieldArray[<span class="number">1</span>] = <span class="string">&quot;女&quot;</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 重新拼接字段</span></span><br><span class="line">                    msg = fieldArray[<span class="number">0</span>] + <span class="string">&quot;,&quot;</span> + fieldArray[<span class="number">1</span>] + <span class="string">&quot;,&quot;</span> + fieldArray[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// (4)	生产消息到dwd_user topic中</span></span><br><span class="line">                    ProducerRecord&lt;String, String&gt; dwdMsg = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;dwd_user&quot;</span>, msg);</span><br><span class="line">                    <span class="comment">// 发送消息</span></span><br><span class="line">                    Future&lt;RecordMetadata&gt; future = producer.send(dwdMsg);</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        future.get();</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                        producer.abortTransaction();</span><br><span class="line">                    &#125;</span><br><span class="line"><span class="comment">//                            new Callback()</span></span><br><span class="line"><span class="comment">//                    &#123;</span></span><br><span class="line"><span class="comment">//                        @Override</span></span><br><span class="line"><span class="comment">//                        public void onCompletion(RecordMetadata metadata, Exception exception) &#123;</span></span><br><span class="line"><span class="comment">//                            // 生产消息没有问题</span></span><br><span class="line"><span class="comment">//                            if(exception == null) &#123;</span></span><br><span class="line"><span class="comment">//                                System.out.println(&quot;发送成功:&quot; + dwdMsg);</span></span><br><span class="line"><span class="comment">//                            &#125;</span></span><br><span class="line"><span class="comment">//                            else &#123;</span></span><br><span class="line"><span class="comment">//                                System.out.println(&quot;生产消息失败:&quot;);</span></span><br><span class="line"><span class="comment">//                                System.out.println(exception.getMessage());</span></span><br><span class="line"><span class="comment">//                                System.out.println(exception.getStackTrace());</span></span><br><span class="line"><span class="comment">//                            &#125;</span></span><br><span class="line"><span class="comment">//                        &#125;</span></span><br><span class="line"><span class="comment">//                    &#125;);</span></span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                producer.sendOffsetsToTransaction(offsetMap, <span class="string">&quot;ods_user&quot;</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// (6)	提交事务</span></span><br><span class="line">                producer.commitTransaction();</span><br><span class="line">            &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">                <span class="comment">// (7)	捕获异常，如果出现异常，则取消事务</span></span><br><span class="line">                producer.abortTransaction();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 一、创建一个消费者来消费ods_user中的数据</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> KafkaConsumer&lt;String, String&gt; <span class="title function_">createConsumer</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 配置消费者的属性（添加对事务的支持）</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1.itcast.cn:9092&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;ods_user&quot;</span>);</span><br><span class="line">        <span class="comment">// 配置事务的隔离级别</span></span><br><span class="line">        props.put(<span class="string">&quot;isolation.level&quot;</span>,<span class="string">&quot;read_committed&quot;</span>);</span><br><span class="line">        <span class="comment">// 关闭自动提交，一会我们需要手动来提交offset，通过事务来维护offset</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br><span class="line">        <span class="comment">// 反序列化器</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 构建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 订阅一个topic</span></span><br><span class="line">        kafkaConsumer.subscribe(Arrays.asList(<span class="string">&quot;ods_user&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> kafkaConsumer;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 二、编写createProducer方法，用来创建一个带有事务配置的生产者</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> KafkaProducer&lt;String, String&gt; <span class="title function_">createProducer</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 配置生产者带有事务配置的属性</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1.itcast.cn:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// 开启事务必须要配置事务的ID</span></span><br><span class="line">        props.put(<span class="string">&quot;transactional.id&quot;</span>, <span class="string">&quot;dwd_user&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 构建生产者</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> kafkaProducer;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="7-3-Kafka中的分区副本机制"><a href="#7-3-Kafka中的分区副本机制" class="headerlink" title="7.3 Kafka中的分区副本机制"></a>7.3 Kafka中的分区副本机制</h3><h4 id="（1）生产者的分区写入策略"><a href="#（1）生产者的分区写入策略" class="headerlink" title="（1）生产者的分区写入策略"></a>（1）生产者的分区写入策略</h4><ul>
<li>轮询（按照消息尽量保证每个分区的负载）策略，消息会均匀地分布到每个partition<ul>
<li>写入消息的时候，key为null的时候，默认使用的是轮询策略</li>
</ul>
</li>
<li>随机策略（不使用）</li>
<li>按key写入策略，key.hash() % 分区的数量</li>
<li>自定义分区策略（类似于MapReduce指定分区）</li>
</ul>
<blockquote>
<p>乱序问题</p>
<ul>
<li>在Kafka中生产者是有写入策略，如果topic有多个分区，就会将数据分散在不同的partition中存储</li>
<li>当partition数量大于1的时候，数据（消息）会打散分布在不同的partition中</li>
<li>如果只有一个分区，消息是有序的</li>
</ul>
</blockquote>
<h4 id="（2）消费组Consumer-Group-Rebalance机制"><a href="#（2）消费组Consumer-Group-Rebalance机制" class="headerlink" title="（2）消费组Consumer Group Rebalance机制"></a>（2）消费组Consumer Group Rebalance机制</h4><ul>
<li>再均衡：在某些情况下，消费者组中的消费者消费的分区会产生变化，会导致消费者分配不均匀（例如：有两个消费者消费3个，因为某个partition崩溃了，还有一个消费者当前没有分区要削峰），Kafka Consumer Group就会启用rebalance机制，重新平衡这个Consumer Group内的消费者消费的分区分配。</li>
<li>触发时机<ul>
<li>消费者数量发生变化<ul>
<li>某个消费者crash</li>
<li>新增消费者</li>
</ul>
</li>
<li>topic的数量发生变化<ul>
<li>某个topic被删除</li>
</ul>
</li>
<li>partition的数量发生变化<ul>
<li>删除partition</li>
<li>新增partition</li>
</ul>
</li>
</ul>
</li>
<li>不良影响<ul>
<li>发生rebalance，所有的consumer将不再工作，共同来参与再均衡，直到每个消费者都已经被成功分配所需要消费的分区为止（rebalance结束）</li>
</ul>
</li>
</ul>
<h4 id="（3）消费者的分区分配策略"><a href="#（3）消费者的分区分配策略" class="headerlink" title="（3）消费者的分区分配策略"></a>（3）消费者的分区分配策略</h4><p>分区分配策略：保障每个消费者尽量能够均衡地消费分区的数据，不能出现某个消费者消费分区的数量特别多，某个消费者消费的分区特别少</p>
<ul>
<li>Range分配策略（范围分配策略）：Kafka默认的分配策略<ul>
<li>n：分区的数量 &#x2F; 消费者数量</li>
<li>m：分区的数量 % 消费者数量</li>
<li>前m个消费者消费n+1个分区</li>
<li>剩余的消费者消费n个分区</li>
</ul>
</li>
<li>RoundRobin分配策略（轮询分配策略）<ul>
<li>消费者挨个分配消费的分区</li>
</ul>
</li>
<li>Striky粘性分配策略<ul>
<li>在没有发生rebalance跟轮询分配策略是一致的</li>
<li>发生了rebalance，轮询分配策略，重新走一遍轮询分配的过程。而粘性会保证跟上一次的尽量一致，只是将新的需要分配的分区，均匀的分配到现有可用的消费者中即可</li>
<li>减少上下文的切换</li>
</ul>
</li>
</ul>
<h4 id="（4）副本的ACK机制"><a href="#（4）副本的ACK机制" class="headerlink" title="（4）副本的ACK机制"></a>（4）副本的ACK机制</h4><p>producer是不断地往Kafka中写入数据，写入数据会有一个返回结果，表示是否写入成功。这里对应有一个ACKs的配置。</p>
<ul>
<li>acks &#x3D; 0：生产者只管写入，不管是否写入成功，可能会数据丢失。性能是最好的</li>
<li>acks &#x3D; 1：生产者会等到leader分区写入成功后，返回成功，接着发送下一条</li>
<li>acks &#x3D; -1&#x2F;all：确保消息写入到leader分区、还确保消息写入到对应副本都成功后，接着发送下一条，性能是最差的</li>
</ul>
<p>根据业务情况来选择ack机制，是要求性能最高，一部分数据丢失影响不大，可以选择0&#x2F;1。如果要求数据一定不能丢失，就得配置为-1&#x2F;all。</p>
<p>分区中是有leader和follower的概念，为了确保消费者消费的数据是一致的，只能从分区leader去读写消息，follower做的事情就是同步数据，Backup。</p>
<h4 id="（5）高级API（High-Level-API）、低级API（Low-Level-API）"><a href="#（5）高级API（High-Level-API）、低级API（Low-Level-API）" class="headerlink" title="（5）高级API（High-Level API）、低级API（Low-Level API）"></a>（5）高级API（High-Level API）、低级API（Low-Level API）</h4><ul>
<li>高级API就是直接让Kafka帮助管理、处理分配、数据<ul>
<li>offset存储在ZK中</li>
<li>由kafka的rebalance来控制消费者分配的分区</li>
<li>开发起来比较简单，无需开发者关注底层细节</li>
<li>无法做到细粒度的控制</li>
</ul>
</li>
<li>低级API：由编写的程序自己控制逻辑<ul>
<li>自己来管理Offset，可以将offset存储在ZK、MySQL、Redis、HBase、Flink的状态存储</li>
<li>指定消费者拉取某个分区的数据</li>
<li>可以做到细粒度的控制</li>
<li>原有的Kafka的策略会失效，需要我们自己来实现消费机制</li>
</ul>
</li>
</ul>
<h3 id="7-4-Kafka原理"><a href="#7-4-Kafka原理" class="headerlink" title="7.4 Kafka原理"></a>7.4 Kafka原理</h3><h4 id="（1）leader和follower"><a href="#（1）leader和follower" class="headerlink" title="（1）leader和follower"></a>（1）leader和follower</h4><ul>
<li>Kafka中的leader和follower是相对分区有意义，不是相对broker</li>
<li>Kafka在创建topic的时候，会尽量分配分区的leader在不同的broker中，其实就是负载均衡</li>
<li>leader职责：读写数据</li>
<li>follower职责：同步数据、参与选举（leader crash之后，会选举一个follower重新成为分区的leader</li>
<li>注意和ZooKeeper区分<ul>
<li>ZK的leader负责读、写，follower可以读取</li>
<li>Kafka的leader负责读写、follower不能读写数据（确保每个消费者消费的数据是一致的），Kafka一个topic有多个分区leader，一样可以实现数据操作的负载均衡</li>
</ul>
</li>
</ul>
<h4 id="（2）AR-ISR-OSR"><a href="#（2）AR-ISR-OSR" class="headerlink" title="（2）AR\ISR\OSR"></a>（2）AR\ISR\OSR</h4><ul>
<li>AR表示一个topic下的所有副本</li>
<li>ISR：In Sync Replicas，正在同步的副本（可以理解为当前有几个follower是存活的）</li>
<li>OSR：Out of Sync Replicas，不再同步的副本</li>
<li>AR &#x3D; ISR + OSR</li>
</ul>
<h4 id="（3）leader选举"><a href="#（3）leader选举" class="headerlink" title="（3）leader选举"></a>（3）leader选举</h4><ul>
<li><p>Controller：controller是kafka集群的老大，是针对Broker的一个角色</p>
<ul>
<li>Controller是高可用的，是用过ZK来进行选举</li>
</ul>
</li>
<li><p>Leader：是针对partition的一个角色</p>
<ul>
<li>Leader是通过ISR来进行快速选举</li>
</ul>
</li>
<li><p>如果Kafka是基于ZK来进行选举，ZK的压力可能会比较大。例如：某个节点崩溃，这个节点上不仅仅只有一个leader，是有不少的leader需要选举。通过ISR快速进行选举。</p>
</li>
<li><p>leader的负载均衡</p>
<ul>
<li>如果某个broker crash之后，就可能会导致partition的leader分布不均匀，就是一个broker上存在一个topic下不同partition的leader</li>
<li>通过以下指令，可以将leader分配到优先的leader对应的broker，确保leader是均匀分配的</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-leader-election.sh --bootstrap-server node1.itcast.cn:9092 --topic test --partition=2 --election-type preferred</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="（4）Kafka读写流程"><a href="#（4）Kafka读写流程" class="headerlink" title="（4）Kafka读写流程"></a>（4）Kafka读写流程</h4><ul>
<li>写流程<ul>
<li>通过ZooKeeper找partition对应的leader，leader是负责写的</li>
<li>producer开始写入数据</li>
<li>ISR里面的follower开始同步数据，并返回给leader ACK</li>
<li>返回给producer ACK</li>
</ul>
</li>
<li>读流程<ul>
<li>通过ZooKeeper找partition对应的leader，leader是负责读的</li>
<li>通过ZooKeeper找到消费者对应的offset</li>
<li>然后开始从offset往后顺序拉取数据</li>
<li>提交offset（自动提交——每隔多少秒提交一次offset、手动提交——放入到事务中提交）</li>
</ul>
</li>
</ul>
<h4 id="（5）Kafka的物理存储"><a href="#（5）Kafka的物理存储" class="headerlink" title="（5）Kafka的物理存储"></a>（5）Kafka的物理存储</h4><ul>
<li>Kafka的数据组织结构<ul>
<li>topic</li>
<li>partition</li>
<li>segment<ul>
<li>.log数据文件</li>
<li>.index（稀疏索引）</li>
<li>.timeindex（根据时间做的索引）</li>
</ul>
</li>
</ul>
</li>
<li>深入了解读数据的流程<ul>
<li>消费者的offset是一个针对partition全局offset</li>
<li>可以根据这个offset找到segment段</li>
<li>接着需要将全局的offset转换成segment的局部offset</li>
<li>根据局部的offset，就可以从（.index稀疏索引）找到对应的数据位置</li>
<li>开始顺序读取</li>
</ul>
</li>
</ul>
<h4 id="（6）消息传递的语义性"><a href="#（6）消息传递的语义性" class="headerlink" title="（6）消息传递的语义性"></a>（6）消息传递的语义性</h4><p>Flink里面有对应的每种不同机制的保证，提供Exactly-Once保障（二阶段事务提交方式）</p>
<ul>
<li>At-most once：最多一次（只管把数据消费到，不管有没有成功，可能会有数据丢失）</li>
<li>At-least once：最少一次（有可能会出现重复消费）</li>
<li>Exactly-Once：仅有一次（事务性性的保障，保证消息有且仅被处理一次）</li>
</ul>
<h4 id="（7）Kafka的消息不丢失"><a href="#（7）Kafka的消息不丢失" class="headerlink" title="（7）Kafka的消息不丢失"></a>（7）Kafka的消息不丢失</h4><ul>
<li>broker消息不丢失：因为有副本relicas的存在，会不断地从leader中同步副本，所以，一个broker crash，不会导致数据丢失，除非是只有一个副本。</li>
<li>生产者消息不丢失：ACK机制（配置成ALL&#x2F;-1）、配置0或者1有可能会存在丢失</li>
<li>消费者消费不丢失：重点控制offset<ul>
<li>At-least once：一种数据可能会重复消费</li>
<li>Exactly-Once：仅被一次消费</li>
</ul>
</li>
</ul>
<h4 id="（8）数据积压"><a href="#（8）数据积压" class="headerlink" title="（8）数据积压"></a>（8）数据积压</h4><ul>
<li>数据积压指的是消费者因为有一些外部的IO、一些比较耗时的操作（Full GC——Stop the world），就会造成消息在partition中一直存在得不到消费，就会产生数据积压</li>
<li>在企业中，我们要有监控系统，如果出现这种情况，需要尽快处理。虽然后续的Spark Streaming&#x2F;Flink可以实现背压机制，但是数据累积太多一定对实时系统它的实时性是有说影响的</li>
</ul>
<h4 id="（9）数据清理-amp-配额限速"><a href="#（9）数据清理-amp-配额限速" class="headerlink" title="（9）数据清理&amp;配额限速"></a>（9）数据清理&amp;配额限速</h4><ul>
<li>数据清理<ul>
<li>Log Deletion（日志删除）：如果消息达到一定的条件（时间、日志大小、offset大小），Kafka就会自动将日志设置为待删除（segment端的后缀名会以 .delete结尾），日志管理程序会定期清理这些日志<ul>
<li>默认是7天过期</li>
</ul>
</li>
<li>Log Compaction（日志合并）<ul>
<li>如果在一些key-value数据中，一个key可以对应多个不同版本的value</li>
<li>经过日志合并，就会只保留最新的一个版本</li>
</ul>
</li>
</ul>
</li>
<li>配额限速<ul>
<li>可以限制Producer、Consumer的速率</li>
<li>防止Kafka的速度过快，占用整个服务器（broker）的所有IO资源</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/19/Kafka/" data-id="clj25kfye0009n0urdmypddmd" data-title="Kafka" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Hive3安装" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/19/Hive3%E5%AE%89%E8%A3%85/" class="article-date">
  <time class="dt-published" datetime="2023-06-19T00:49:49.447Z" itemprop="datePublished">2023-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/19/Hive3%E5%AE%89%E8%A3%85/">Hive</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Hive3安装"><a href="#Hive3安装" class="headerlink" title="Hive3安装"></a>Hive3安装</h1><h2 id="一、Mysql安装"><a href="#一、Mysql安装" class="headerlink" title="一、Mysql安装"></a>一、Mysql安装</h2><ul>
<li><p>卸载Centos7自带的mariadb</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# rpm -qa|grep mariadb</span><br><span class="line">mariadb-libs-5.5.64-1.el7.x86_64</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# rpm -e mariadb-libs-5.5.64-1.el7.x86_64 --nodeps</span><br><span class="line">[root@node1 ~]# rpm -qa|grep mariadb                            </span><br><span class="line">[root@node1 ~]# </span><br></pre></td></tr></table></figure>
</li>
<li><p>安装mysql</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">mkdir /export/server/mysql</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">上传mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar 到上述文件夹下  解压</span></span><br><span class="line">tar xvf mysql-5.7.29-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">执行安装</span></span><br><span class="line">yum -y install libaio</span><br><span class="line"></span><br><span class="line">[root@node1 mysql]rpm -ivh mysql-community-common-5.7.29-1.el7.x86_64.rpm mysql-community-libs-5.7.29-1.el7.x86_64.rpm mysql-community-client-5.7.29-1.el7.x86_64.rpm mysql-community-server-5.7.29-1.el7.x86_64.rpm </span><br><span class="line"></span><br><span class="line">warning: mysql-community-common-5.7.29-1.el7.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEY</span><br><span class="line">Preparing...                          ################################# [100%]</span><br><span class="line">Updating / installing...</span><br><span class="line">   1:mysql-community-common-5.7.29-1.e################################# [ 25%]</span><br><span class="line">   2:mysql-community-libs-5.7.29-1.el7################################# [ 50%]</span><br><span class="line">   3:mysql-community-client-5.7.29-1.e################################# [ 75%]</span><br><span class="line">   4:mysql-community-server-5.7.29-1.e################                  ( 49%)</span><br></pre></td></tr></table></figure>
</li>
<li><p>mysql初始化设置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">初始化</span></span><br><span class="line">mysqld --initialize</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">更改所属组</span></span><br><span class="line">chown mysql:mysql /var/lib/mysql -R</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启动mysql</span></span><br><span class="line">systemctl start mysqld.service</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查看生成的临时root密码</span></span><br><span class="line">cat  /var/log/mysqld.log</span><br><span class="line"></span><br><span class="line">[Note] A temporary password is generated for root@localhost: o+TU+KDOm004</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改root密码 授权远程访问 设置开机自启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# mysql -u root -p</span><br><span class="line">Enter password:     #这里输入在日志中生成的临时密码</span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 3</span><br><span class="line">Server version: 5.7.29</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"></span><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">更新root密码  设置为hadoop</span></span><br><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash">alter user user() identified by <span class="string">&quot;hadoop&quot;</span>;</span></span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">授权</span></span><br><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash">use mysql;</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash">GRANT ALL PRIVILEGES ON *.* TO <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED BY <span class="string">&#x27;hadoop&#x27;</span> WITH GRANT OPTION;</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">mysql&gt; </span><span class="language-bash">FLUSH PRIVILEGES;</span> </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">mysql的启动和关闭 状态查看 （这几个命令必须记住）</span></span><br><span class="line">systemctl stop mysqld</span><br><span class="line">systemctl status mysqld</span><br><span class="line">systemctl start mysqld</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">建议设置为开机自启动服务</span></span><br><span class="line">[root@node1 ~]# systemctl enable  mysqld                             </span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/mysqld.service to /usr/lib/systemd/system/mysqld.service.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查看是否已经设置自启动成功</span></span><br><span class="line">[root@node1 ~]# systemctl list-unit-files | grep mysqld</span><br><span class="line">mysqld.service                                enabled </span><br></pre></td></tr></table></figure>
</li>
<li><p>Centos7 干净卸载mysql 5.7</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">关闭mysql服务</span></span><br><span class="line">systemctl stop mysqld.service</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查找安装mysql的rpm包</span></span><br><span class="line">[root@node3 ~]# rpm -qa | grep -i mysql      </span><br><span class="line">mysql-community-libs-5.7.29-1.el7.x86_64</span><br><span class="line">mysql-community-common-5.7.29-1.el7.x86_64</span><br><span class="line">mysql-community-client-5.7.29-1.el7.x86_64</span><br><span class="line">mysql-community-server-5.7.29-1.el7.x86_64</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">卸载</span></span><br><span class="line">[root@node1 ~]# yum remove mysql-community-libs-5.7.29-1.el7.x86_64 mysql-community-common-5.7.29-1.el7.x86_64 mysql-community-client-5.7.29-1.el7.x86_64 mysql-community-server-5.7.29-1.el7.x86_64</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查看是否卸载干净</span></span><br><span class="line">rpm -qa | grep -i mysql</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查找mysql相关目录 删除</span></span><br><span class="line">[root@node1 ~]# find / -name mysql</span><br><span class="line">/var/lib/mysql</span><br><span class="line">/var/lib/mysql/mysql</span><br><span class="line">/usr/share/mysql</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# rm -rf /var/lib/mysql</span><br><span class="line">[root@node1 ~]# rm -rf /var/lib/mysql/mysql</span><br><span class="line">[root@node1 ~]# rm -rf /usr/share/mysql</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">删除默认配置 日志</span></span><br><span class="line">rm -rf /etc/my.cnf </span><br><span class="line">rm -rf /var/log/mysqld.log</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="二、Hive的安装"><a href="#二、Hive的安装" class="headerlink" title="二、Hive的安装"></a>二、Hive的安装</h2><ul>
<li><p>上传安装包 解压</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf apache-hive-3.1.2-bin.tar.gz</span><br><span class="line">ln -s apache-hive-3.1.2-bin hive</span><br></pre></td></tr></table></figure>
</li>
<li><p>解决Hive与Hadoop之间guava版本差异</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/hive/</span><br><span class="line">rm -rf lib/guava-19.0.jar</span><br><span class="line">cp /export/server/hadoop/share/hadoop/common/lib/guava-27.0-jre.jar ./lib/</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改配置文件</p>
<ul>
<li><p>hive-env.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/hive/conf</span><br><span class="line">mv hive-env.sh.template hive-env.sh</span><br><span class="line"></span><br><span class="line">vim hive-env.sh</span><br><span class="line">export HADOOP_HOME=/export/server/hadoop</span><br><span class="line">export HIVE_CONF_DIR=/export/server/hive/conf</span><br><span class="line">export HIVE_AUX_JARS_PATH=/export/server/hive/lib</span><br></pre></td></tr></table></figure>
</li>
<li><p>hive-site.xml</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 存储元数据mysql相关配置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node1:3306/hive3?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useSSL=false<span class="symbol">&amp;amp;</span>useUnicode=true<span class="symbol">&amp;amp;</span>characterEncoding=UTF-8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- H2S运行绑定host --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 远程模式部署metastore metastore地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://node1:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 关闭元数据存储授权  --&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>上传mysql jdbc驱动到hive安装包lib下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql-connector-java-5.1.32.jar</span><br></pre></td></tr></table></figure>
</li>
<li><p>初始化元数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/hive/</span><br><span class="line"></span><br><span class="line">bin/schematool -initSchema -dbType mysql -verbos</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">初始化成功会在mysql中创建74张表</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在hdfs创建hive存储目录（如存在则不用操作）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /tmp</span><br><span class="line">hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line">hadoop fs -chmod g+w /tmp</span><br><span class="line">hadoop fs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure>
</li>
<li><p>&#x3D;&#x3D;启动hive&#x3D;&#x3D;</p>
<ul>
<li><p>1、启动metastore服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">前台启动  关闭ctrl+c</span></span><br><span class="line">/export/server/hive/bin/hive --service metastore</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">前台启动开启debug日志</span></span><br><span class="line">/export/server/hive/bin/hive --service metastore --hiveconf hive.root.logger=DEBUG,console  </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">后台启动 进程挂起  关闭使用jps+ <span class="built_in">kill</span> -9</span></span><br><span class="line">nohup /export/server/hive/bin/hive --service metastore &amp;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>2、启动hiveserver2服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nohup /export/server/hive/bin/hive --service hiveserver2 &amp;</span><br><span class="line"><span class="meta prompt_">  </span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">注意 启动hiveserver2需要一定的时间  不要启动之后立即beeline连接 可能连接不上</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>3、beeline客户端连接</p>
<ul>
<li><p>拷贝node1安装包到beeline客户端机器上（node3）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/apache-hive-3.1.2-bin/ root@node3:/export/server/</span><br></pre></td></tr></table></figure>
</li>
<li><p>错误</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: Could not open client transport with JDBC Uri: jdbc:hive2://node1:10000: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: root is not allowed to impersonate root (state=08S01,code=0)</span><br></pre></td></tr></table></figure>

<ul>
<li><p>修改</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">  在hadoop的配置文件core-site.xml中添加如下属性：</span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>连接访问</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/export/server/hive/bin/beeline</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">beeline&gt; </span><span class="language-bash">! connect jdbc:hive2://node1:10000</span></span><br><span class="line"><span class="meta prompt_">beeline&gt; </span><span class="language-bash">root</span></span><br><span class="line"><span class="meta prompt_">beeline&gt; </span><span class="language-bash">直接回车</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
<li><p>错误解决：&#x3D;&#x3D;Hive3执行insert插入操作 statstask异常&#x3D;&#x3D;</p>
<ul>
<li><p>现象</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在执行insert + values操作的时候  虽然最终执行成功，结果正确。但是在执行日志中会出现如下的错误信息。</span><br></pre></td></tr></table></figure>

<p><img src="/md%E5%9B%BE%5CHive3%E5%AE%89%E8%A3%85.assets/image-20201109144915808.png" alt="image-20201109144915808"></p>
</li>
<li><p>开启hiveserver2执行日志。查看详细信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2020-11-09 00:37:48,963 WARN  [5ce14c58-6b36-476a-bab8-89cba7dd1706 main] metastore.RetryingMetaStoreClient: MetaStoreClient lost connection. Attempting to reconnect (1 of 1) after 1s. setPartitionColumnStatistics</span><br><span class="line"></span><br><span class="line">ERROR [5ce14c58-6b36-476a-bab8-89cba7dd1706 main] exec.StatsTask: Failed to run stats task</span><br></pre></td></tr></table></figure>

<p><img src="/md%E5%9B%BE%5CHive3%E5%AE%89%E8%A3%85.assets/image-20201109145136486.png" alt="image-20201109145136486"></p>
</li>
<li><p>但是 &#x3D;&#x3D;此错误并不影响最终的插入语句执行成功&#x3D;&#x3D;。</p>
</li>
<li><p>分析原因和解决</p>
<ul>
<li><p>statstask是一个hive中用于统计插入等操作的状态任务  其返回结果如下</p>
<p><img src="/md%E5%9B%BE%5CHive3%E5%AE%89%E8%A3%85.assets/image-20201109145304560.png" alt="image-20201109145304560"></p>
</li>
<li><p>此信息类似于计数器 用于告知用户插入数据的相关信息 但是不影响程序的正常执行。</p>
</li>
<li><p>Hive新版本中 这是一个issues  临时解决方式如下</p>
<p><a target="_blank" rel="noopener" href="https://community.cloudera.com/t5/Support-Questions/Hive-Metastore-Connection-Failure-then-Retry/td-p/151661">https://community.cloudera.com/t5/Support-Questions/Hive-Metastore-Connection-Failure-then-Retry/td-p/151661</a></p>
<p><img src="/md%E5%9B%BE%5CHive3%E5%AE%89%E8%A3%85.assets/image-20201109145621381.png" alt="image-20201109145621381"></p>
</li>
<li><p>&#x3D;&#x3D;在mysql metastore中删除 PART_COL_STATS这张表即可&#x3D;&#x3D;。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="三、hive注释信息中文乱码解决"><a href="#三、hive注释信息中文乱码解决" class="headerlink" title="三、hive注释信息中文乱码解决"></a>三、hive注释信息中文乱码解决</h2><p>–注意 下面sql语句是需要在MySQL中执行  修改Hive存储的元数据信息（metadata）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">use hivenode2;</span><br><span class="line"><span class="keyword">show</span> tables;</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> hivenode2.COLUMNS_V2 modify <span class="keyword">column</span> COMMENT <span class="type">varchar</span>(<span class="number">256</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> hivenode2.TABLE_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> hivenode2.PARTITION_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8 ;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> hivenode2.PARTITION_KEYS modify <span class="keyword">column</span> PKEY_COMMENT <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> hivenode2.INDEX_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/19/Hive3%E5%AE%89%E8%A3%85/" data-id="clj25kfyb0004n0ur3cs51lln" data-title="Hive" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Git教程" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/19/Git%E6%95%99%E7%A8%8B/" class="article-date">
  <time class="dt-published" datetime="2023-06-19T00:49:49.445Z" itemprop="datePublished">2023-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/19/Git%E6%95%99%E7%A8%8B/">Git</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="git-教程"><a href="#git-教程" class="headerlink" title="git 教程"></a>git 教程</h1><p>[TOC]</p>
<h1 id="1-版本管理工具概念"><a href="#1-版本管理工具概念" class="headerlink" title="1.版本管理工具概念"></a>1.版本管理工具概念</h1><p>我在大学毕业写论文的时候的时候碰到过如下的现象</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;&lt;毕业论文第一版.doc&gt;&gt;</span><br><span class="line">&lt;&lt;毕业论文第二版.doc&gt;&gt;</span><br><span class="line">&lt;&lt;毕业论文第三版.doc&gt;&gt;</span><br><span class="line">&lt;&lt;毕业论文最终版.doc&gt;&gt;</span><br><span class="line">&lt;&lt;毕业论文最终版2.doc&gt;&gt;</span><br></pre></td></tr></table></figure>

<p>类似的问题我曾经也碰到过很多,例如:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">领导让写文档,写好了,领导让修改,改好了,领导觉得第一版不错,改回来吧,此时内心一脸懵,第一版长啥样没存档啊</span><br></pre></td></tr></table></figure>

<p>实际上,代码开发中也需要这样的软件来管理我们的代码. 例如我们经常会碰到如下的现象:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">改之前好好的,改完就报错了,也没怎么修改啊</span><br></pre></td></tr></table></figure>

<p>在这种情况下如果不能查看修改之前的代码,查找问题是非常困难的.</p>
<p>如果有一个软件能记录我们对文档的所有修改,所有版本,那么上面的问题讲迎刃而解.而这类软件我们一般叫做版本控制工具</p>
<p>版本管理工具一般具有如下特性:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1) 能够记录历史版本,回退历史版本</span><br><span class="line">2) 团队开发,方便代码合并</span><br></pre></td></tr></table></figure>



<h1 id="2-版本管理工具介绍"><a href="#2-版本管理工具介绍" class="headerlink" title="2. 版本管理工具介绍"></a>2. 版本管理工具介绍</h1><p>现在比较流行的版本管理工具是git ,但是实际上git 是近几年才发展起来的,可能有一些老的项目,还在用一些老的软件,比如svn</p>
<h2 id="2-1版本管理发展简史-维基百科"><a href="#2-1版本管理发展简史-维基百科" class="headerlink" title="2.1版本管理发展简史(维基百科)"></a>2.1版本管理发展简史(维基百科)</h2><p><img src="/./assets/1571983065236.png" alt="1571983065236"> </p>
<h3 id="2-1-1-SVN-SubVersion"><a href="#2-1-1-SVN-SubVersion" class="headerlink" title="2.1.1 SVN(SubVersion)"></a>2.1.1 SVN(SubVersion)</h3><p>工作流程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SVN是集中式版本控制系统，版本库是集中放在中央服务器的.</span><br><span class="line">工作流程如下:</span><br><span class="line">	1.从中央服务器远程仓库下载代码</span><br><span class="line">	2.修改后将代码提交到中央服务器远程仓库</span><br></pre></td></tr></table></figure>

<p>优缺点:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">优点: 简单,易操作</span><br><span class="line">缺点:所有代码必须放在中央服务器  </span><br><span class="line"> 	   1.服务器一旦宕机无法提交代码,即容错性较差</span><br><span class="line">      2.离线无法提交代码,无法及时记录我们的提交行为</span><br></pre></td></tr></table></figure>

<p>svn流程图</p>
<p><img src="/./assets/svn.jpg"></p>
<h3 id="2-1-2-Git"><a href="#2-1-2-Git" class="headerlink" title="2.1.2 Git"></a>2.1.2 Git</h3><p>工作流程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Git是分布式版本控制系统（Distributed Version Control System，简称 DVCS），分为两种类型的仓库：</span><br><span class="line">本地仓库和远程仓库</span><br><span class="line">工作流程如下</span><br><span class="line">    1．从远程仓库中克隆或拉取代码到本地仓库(clone/pull)</span><br><span class="line">    2．从本地进行代码修改</span><br><span class="line">    3．在提交前先将代码提交到暂存区</span><br><span class="line">    4．提交到本地仓库。本地仓库中保存修改的各个历史版本</span><br><span class="line">    5．修改完成后，需要和团队成员共享代码时，将代码push到远程仓库</span><br></pre></td></tr></table></figure>

<p><img src="/./assets/git.png"></p>
<p>总结:git和svn的区别</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. svn 是集中式版本控制工具,git 是分布式版本控制工具</span><br><span class="line">2. svn 不支持离线提交,git 支持离线提交代码</span><br></pre></td></tr></table></figure>

<h1 id="3-Git-发展简史"><a href="#3-Git-发展简史" class="headerlink" title="3. Git 发展简史"></a>3. Git 发展简史</h1><p> 林纳斯·本纳第克特·托瓦兹（Linus Benedict Torvalds, 1969年~ ） </p>
<p><img src="/./assets/1571987252740.png" alt="1571987252740"></p>
<p>很多人都知道，Linus在1991年创建了开源的Linux，从此，Linux系统不断发展，已经成为最大的服务器系统软件了。</p>
<p>Linus虽然创建了Linux，但Linux的壮大是靠全世界热心的志愿者参与的，这么多人在世界各地为Linux编写代码，那Linux的代码是如何管理的呢？</p>
<p>事实是，在2002年以前，世界各地的志愿者把源代码文件通过diff的方式发给Linus，然后由Linus本人通过<strong>手工方式合并代码！</strong></p>
<p>你也许会想，为什么Linus不把Linux代码放到版本控制系统里呢？那个年代不是有CVS、SVN这些免费的版本控制系统吗？因为Linus坚定地反对CVS和SVN，这些集中式的版本控制系统不但速度慢，<strong>而且必须联网才能使用</strong>。有一些商用的版本控制系统，虽然比CVS、SVN好用，但那是<strong>付费</strong>的，和Linux的开源精神不符。</p>
<p>不过，到了2002年，Linux系统已经发展了十年了，代码库之大让Linus很难继续通过手工方式管理了，社区的弟兄们也对这种方式表达了强烈不满，于是Linus选择了一个商业的版本控制系统BitKeeper，BitKeeper的东家BitMover公司出于人道主义精神，授权Linux社区免费使用这个版本控制系统。而授权的前提是:Linux 社区的人不能开发具有相同功能的竞争产品! </p>
<p>另一方面,BitKeeper不是开源的. 显然与Linux 的开源精神不相符,所以linux 社区的很多人抱怨,不愿意使用.</p>
<p>典型的就是  Andrew Tridgell  (Samba 开发服务的创造者) 非常不满.偷偷违反了和 BitKeeper 的协议,反编译 BitKeeper 的源代码,开发了个爬虫,然后爬取信息被人发现了. BitKeeper 公司的领导非常不满意,然后开始发布消息说,(下个版本)不再为Linux 提供免费的服务. </p>
<p>Linus  本人就出面协调(几周或者几个月),但是不管用, 没办法. 估计谈判的过程感觉到了憋屈–”吃人嘴短,拿人手软”</p>
<p>Linus  本人 花了10天的时间Git 出来了,一个月之内，Linux系统的源码已经由Git管理了！</p>
<p> <img src="/./assets/1571988966446.png" alt="Linus 采访记录"></p>
<p>Git 出来以后毕竟是一个人做的,开始并不好用(刚开始只能用勉强可以用来形容), 还是很多人抱怨,发展了很多年都没有干过其他软件.</p>
<p>直到 2008年，GitHub网站上线了，它为开源项目免费提供Git存储，无数开源项目开始迁移至GitHub,从此git 迎来了飞速发展,当下git 已经成为了最流行的版本控制工具</p>
<h1 id="4-Git-的安装"><a href="#4-Git-的安装" class="headerlink" title="4. Git 的安装"></a>4. Git 的安装</h1><h2 id="4-1-git-的下载"><a href="#4-1-git-的下载" class="headerlink" title="4.1 git 的下载"></a>4.1 git 的下载</h2><p>下载地址： <a target="_blank" rel="noopener" href="https://git-scm.com/download">https://git-scm.com/download</a></p>
<p><img src="/./assets/1571990833074.png" alt="1571990833074"></p>
<p>附件</p>
<p><img src="/./assets/1571991253594.png" alt="1571991253594"></p>
<hr/>
## 4.2 安装

<ol>
<li><p>按照附件的 顺序直接下一步傻瓜式安装即可</p>
</li>
<li><p>其中安装的过程中需要填写一个邮箱和用户名(任意即可)</p>
</li>
</ol>
<p><img src="/assets/1572001054551.png" alt="1572001054551"></p>
<ol start="3">
<li><p>$\color{red}{注意: 安装完毕请重启资源管理器,或者重启电脑!!!}$ </p>
</li>
<li><p>更改语言</p>
</li>
</ol>
<p><img src="/./assets/1571992521110.png" alt="1571992521110"></p>
<h1 id="5-Git-工作流程"><a href="#5-Git-工作流程" class="headerlink" title="5. Git 工作流程"></a>5. Git 工作流程</h1><h2 id="5-1-Git-初始化"><a href="#5-1-Git-初始化" class="headerlink" title="5.1 Git 初始化"></a>5.1 Git 初始化</h2><p>我们先初始化一个本地仓</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1) 新建测试文件夹</span><br><span class="line">2) 进入文件夹,然后右键创建版本库</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1571993852280.png" alt="1571993852280"></p>
<p>此时 我们看到 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1) 文件夹上多了一个绿色图标(如果没有请看本章节 5.3小节说明)</span><br><span class="line">2) 文件夹内部生成了一个.git 隐藏文件夹(需要设置隐藏文件夹可见)</span><br></pre></td></tr></table></figure>

<h2 id="5-2-git-流程"><a href="#5-2-git-流程" class="headerlink" title="5.2  git 流程"></a>5.2  git 流程</h2><h3 id="5-2-1-流程图"><a href="#5-2-1-流程图" class="headerlink" title="5.2.1 流程图"></a>5.2.1 流程图</h3><p><img src="/assets/git%E6%B5%81%E7%A8%8B.png"></p>
<h3 id="5-2-2概念即详解"><a href="#5-2-2概念即详解" class="headerlink" title="5.2.2概念即详解"></a>5.2.2概念即详解</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">本地仓库：是在开发人员自己电脑上的Git仓库,存放我们的代码(.git 隐藏文件夹就是我们的本地仓库)		</span><br><span class="line">远程仓库：是在远程服务器上的Git仓库,存放代码(可以是github.com或者gitee.com 上的仓库,或者自己该公司的服务器)</span><br><span class="line">工作区: 我们自己写代码(文档)的地方</span><br><span class="line">暂存区: 在 本地仓库中的一个特殊的文件(index) 叫做暂存区,临时存储我们即将要提交的文件</span><br><span class="line">------------</span><br><span class="line">Clone：克隆，就是将远程仓库复制到本地仓库</span><br><span class="line">Push：推送，就是将本地仓库代码上传到远程仓库</span><br><span class="line">Pull：拉取，就是将远程仓库代码下载到本地仓库,并将代码 克隆到本地工作区</span><br></pre></td></tr></table></figure>

<p><img src="/assets/git%E6%B5%81%E7%A8%8B_%E8%AF%A6%E8%A7%A3.png"></p>
<h1 id="6-Git-的基本使用01-TortoiseGit-操作本地仓库"><a href="#6-Git-的基本使用01-TortoiseGit-操作本地仓库" class="headerlink" title="6.Git 的基本使用01-TortoiseGit 操作本地仓库"></a>6.Git 的基本使用01-TortoiseGit 操作本地仓库</h1><h2 id="6-1-初始化仓库"><a href="#6-1-初始化仓库" class="headerlink" title="6.1  初始化仓库"></a>6.1  初始化仓库</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">方法一: </span><br><span class="line">新建一个文件夹,进入文件夹内部操作</span><br><span class="line">1)右键--&gt; 在这里创建Git 版本库 </span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572161580344.png" alt="1572161580344"></p>
<p>  注意: 不要直接在桌面上操作,否则桌面就是一个仓库</p>
 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">方法二:</span><br><span class="line">2) 右键--&gt;Git GUI here</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572162906623.png" alt="1572162906623"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">方法三: 命令行模式</span><br><span class="line">2) git init </span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572163015102.png" alt="1572163015102"></p>
<p>创建完毕仓库,我们发现,此时我们创建的文件夹下有一个.git 文件已经生成了</p>
<p>并且仓库文件夹上多了一个 绿色图标</p>
<p><img src="/assets/1572164583708.png" alt="1572164583708"></p>
<h2 id="6-2-添加文件"><a href="#6-2-添加文件" class="headerlink" title="6.2 添加文件"></a>6.2 添加文件</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1)在仓库中新建一个文件</span><br><span class="line">2)选中新建的文件--&gt;右键--&gt; TortoiseGit--&gt; 添加</span><br><span class="line">3)此时我们看到文件夹上多了一个 &quot;加号&quot;</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572164475555.png" alt="1572164475555"></p>
<p><img src="/assets/1572165109016.png" alt="1572165109016"></p>
<h2 id="6-3-提交文件至本地仓库"><a href="#6-3-提交文件至本地仓库" class="headerlink" title="6.3 提交文件至本地仓库"></a>6.3 提交文件至本地仓库</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1)选中文件</span><br><span class="line">2) 右键--git提交</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572165471421.png" alt="1572165471421"></p>
<h2 id="6-4-修改文件-与再次提交文件"><a href="#6-4-修改文件-与再次提交文件" class="headerlink" title="6.4 修改文件,与再次提交文件"></a>6.4 修改文件,与再次提交文件</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">当我们修改文件以后,文件上多了一个红色感叹号,表示我们上次提交后该文件被修改过</span><br><span class="line">提交后文件图标又变成绿色</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572165676716.png" alt="1572165676716"></p>
<h2 id="6-5-文件状态讲解"><a href="#6-5-文件状态讲解" class="headerlink" title="6.5 文件状态讲解"></a>6.5 文件状态讲解</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Git工作目录下的文件存在两种状态：</span><br><span class="line">1 untracked 未跟踪（未被纳入版本控制） :  比如新建的文件(此时文件夹上没有图标或者有一个&quot;问号&quot;)</span><br><span class="line">2 tracked 已跟踪（被纳入版本控制）     </span><br><span class="line">    2.1 Staged 已暂存状态            : 添加 但未提交状态(此时文件夹上有一个&quot;加号&quot;)</span><br><span class="line">	2.2 Unmodified 未修改状态        : 已提交(此时文件夹上有一个&quot;对号&quot;)</span><br><span class="line">	2.3 Modified 已修改状态          : 修改了,但是还没有提交 (此时文件夹上有一个&quot;红色感叹号&quot;)</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1569293793902.png"></p>
<p>这些文件的状态会随着我们执行Git的命令发生变化</p>
<p><img src="/assets/1566627767390.png"></p>
<h2 id="6-6-修改文件-不提交和上一个版本比较差异-diff"><a href="#6-6-修改文件-不提交和上一个版本比较差异-diff" class="headerlink" title="6.6 修改文件,不提交和上一个版本比较差异(diff)"></a>6.6 修改文件,不提交和上一个版本比较差异(diff)</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">修改文件,此时不要提交</span><br><span class="line">选中文件--&gt;右键--&gt; TortoiseGit--&gt; 比较差异</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572167540734.png" alt="1572167540734"></p>
<h2 id="6-7-查看提交历史记录"><a href="#6-7-查看提交历史记录" class="headerlink" title="6.7 查看提交历史记录"></a>6.7 查看提交历史记录</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">选中文件</span><br><span class="line">右键--&gt; TortoiseGit--&gt; 显示日志</span><br><span class="line">此时我们可以看到所有的历史提交记录</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572167842481.png" alt="1572167842481"></p>
<p>##6.8 回退至历史版本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">右键--&gt; TortoiseGit--&gt; 显示日志</span><br><span class="line">选中某个版本--&gt; 进行如下操作</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572171034508.png" alt="1572171034508"></p>
<h2 id="6-9-文件删除"><a href="#6-9-文件删除" class="headerlink" title="6.9 文件删除"></a>6.9 文件删除</h2><h3 id="6-9-1本地删除与恢复"><a href="#6-9-1本地删除与恢复" class="headerlink" title="6.9.1本地删除与恢复"></a>6.9.1本地删除与恢复</h3>  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1) 直接选中文件删除的话,其实只是删除了本地工作区的文件,并没有删除 仓库中的文件</span><br><span class="line">   此时时可以回退的, 比如我们进行如下操作</span><br><span class="line">   1)文件删除</span><br><span class="line">   2)右键--&gt; TortoiseGit--&gt; 还原</span><br><span class="line">   此时我们发现文件又被恢复了</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572169387835.png" alt="1572169387835"></p>
<h3 id="6-9-2从版本库删除"><a href="#6-9-2从版本库删除" class="headerlink" title="6.9.2从版本库删除"></a>6.9.2从版本库删除</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2) 我们如果真的想要将某个文件从服务器删除,需要进行如下操作</span><br><span class="line">   1) 删除文件,和上面的操作一样</span><br><span class="line">   2) 提交,此时服务文件已经删除了(历史版本还在,还是可以恢复)</span><br></pre></td></tr></table></figure>

<h3 id="6-9-3从版本库删除-但是不删除本地"><a href="#6-9-3从版本库删除-但是不删除本地" class="headerlink" title="6.9.3从版本库删除,但是不删除本地"></a>6.9.3从版本库删除,但是不删除本地</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">我们可以如下操作,只删除服务器上的文件,但是本地文件并不删除</span><br><span class="line">备注: 删除之后需要提交,才会真正的从服务器删除</span><br></pre></td></tr></table></figure>



<p><img src="/assets/1572171572963.png" alt="1572171572963"></p>
<h2 id="6-10-忽略提交"><a href="#6-10-忽略提交" class="headerlink" title="6.10 忽略提交"></a>6.10 忽略提交</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">有时候我们一些文件是不需要提交的比如说idea/eclipse 开发的代码自动生成的配置文件</span><br><span class="line">如何配置不提交呢</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572172137494.png" alt="1572172137494"></p>
<p>此时我们的根目录下会生成一个.gitignore 文件</p>
<p>忽略文件如何阅读,常见格式</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 所有以.a 结尾的文件讲被忽略(递归)</span></span><br><span class="line"><span class="string">*.a</span></span><br><span class="line"><span class="comment"># 不管其他规则怎样,强制不忽略  lib.a</span></span><br><span class="line"><span class="type">!lib.a</span></span><br><span class="line"><span class="comment"># 只忽略 文件 TODO (注意这里是文件)</span></span><br><span class="line"><span class="string">/TODO</span></span><br><span class="line"><span class="comment"># 忽略 build文件夹下所有内容(递归) 这里是文件夹</span></span><br><span class="line"><span class="string">build/</span></span><br><span class="line"><span class="comment"># 忽略 doc 目录下以 *.txt 结尾的文件 (不递归)</span></span><br><span class="line"><span class="string">doc/*.txt</span></span><br><span class="line"><span class="comment"># 忽略 doc 目录下以 *.pdf 结尾的文件 (递归)</span></span><br><span class="line"><span class="string">doc/**/*.pdf</span></span><br></pre></td></tr></table></figure>

<p>当然理解了上述规则,我们也可以手动编辑该文件,而不用通过窗口化操作(如果不嫌麻烦)</p>
<h1 id="7-Git-的基本使用02-TortoiseGit-操作本地仓库-分支"><a href="#7-Git-的基本使用02-TortoiseGit-操作本地仓库-分支" class="headerlink" title="7. Git 的基本使用02-TortoiseGit 操作本地仓库(分支)"></a>7. Git 的基本使用02-TortoiseGit 操作本地仓库(分支)</h1><h2 id="7-1-分支的概念"><a href="#7-1-分支的概念" class="headerlink" title="7.1 分支的概念"></a>7.1 分支的概念</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">几乎所有的版本控制系统都以某种形式支持分支。 使用分支意味着你可以把你的工作从开发主线上分离开来，避免影响开发主线。多线程开发,可以同时开启多个任务的开发,多个任务之间互不影响.</span><br></pre></td></tr></table></figure>

<h2 id="7-2-为何要使用分支"><a href="#7-2-为何要使用分支" class="headerlink" title="7.2 为何要使用分支"></a>7.2 为何要使用分支</h2><p>先看单线程开发</p>
<p><img src="/assets/1572173978185.png" alt="1572173978185"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">思考如下现象</span><br><span class="line">    10.1 日 业务部门提出需求 : 明年 元旦3天做2个促销活动</span><br><span class="line">	1) 12.31 号上线活动1, </span><br><span class="line">	2) 1.4 号上线活动2 ,同时 要求撤销 活动1</span><br><span class="line">    你所在 部门领导 为了保证能顺利完成,要求 11.15 号完成 上述连个功能的开发工作</span><br><span class="line">此时作为开发人员:我要面临两个文件, 活动1 的代码,即要存在(12.31 要用)又要不存在(1.4 号要求删除) ,我们怎么做?</span><br><span class="line">显然比较棘手,如果使用分支(可以理解为将代码复制一份)将很好解决</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572174740013.png" alt="1572174740013"></p>
<h2 id="7-3-创建分支"><a href="#7-3-创建分支" class="headerlink" title="7.3 创建分支"></a>7.3 创建分支</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">到现在为止,我们一直使用的时主分支(master)</span><br><span class="line">在主分支上操作创建分支</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572178189980.png" alt="1572178189980"></p>
<h2 id="7-4-分支的查看切换"><a href="#7-4-分支的查看切换" class="headerlink" title="7.4 分支的查看切换"></a>7.4 分支的查看切换</h2><h3 id="7-4-1查看分支"><a href="#7-4-1查看分支" class="headerlink" title="7.4.1查看分支"></a>7.4.1查看分支</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">查看版本分支图,此时我们看到有两个分支</span><br><span class="line">当然,我们可以创建多个分支</span><br><span class="line">可以看到多个分支的图形</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572179420706.png" alt="1572179420706"></p>
<h3 id="7-4-2切换分支"><a href="#7-4-2切换分支" class="headerlink" title="7.4.2切换分支"></a>7.4.2切换分支</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">右键--&gt; 检出</span><br></pre></td></tr></table></figure>



<p><img src="/assets/1572180057481.png" alt="1572180057481"></p>
<p>##7.5 分支的合并与删除</p>
<h3 id="7-5-1合并"><a href="#7-5-1合并" class="headerlink" title="7.5.1合并"></a>7.5.1合并</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">我们将代码切换到分支1,然后写属于需求1 的代码并提交</span><br><span class="line">当我们把需求1 开发完毕如何把需求1 的代码合并到主分支呢?</span><br><span class="line">--&gt;1 切换到 主版本</span><br><span class="line">--&gt;2 右键 合并即可将需求1 写的代码合并至主分支</span><br><span class="line">-----此时我们看到代码自动合并到了master分支</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572180623156.png" alt="1572180623156"></p>
<h3 id="7-5-2删除分支"><a href="#7-5-2删除分支" class="headerlink" title="7.5.2删除分支"></a>7.5.2删除分支</h3><p><img src="/assets/1572183670465.png" alt="1572183670465"></p>
<p>5,冲突的处理<br>​	5.1)冲突的概念</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">现象演示	</span><br><span class="line">	开发人员A 开发需求1,开发了一个工具类 MathUtil,里面第一行写了一个方法 add(int [] args)</span><br><span class="line"> 同时开发人员B 开发需求2,开发了一个工具类 MathUtil,里面第一行写了一个方法 add(int a int b)</span><br><span class="line">他们在互相不知道对方需求的情况下同时提交了代码到自己的分支</span><br><span class="line">   思考此时如果我们把需求1 和需求2 同时都合并到主分支上, 主分支的 工具类 MathUtil 的第一行应该使用谁的代码? </span><br><span class="line">   此时主分支是不能智能判断第一行使用谁的代码,合并时会报错,我们叫做冲突.</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572181403239.png" alt="1572181403239"></p>
<p>​	5.2) 如何处理冲突</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">分析一下冲突的原因:</span><br><span class="line">	开发人员之间彼此没有沟通导致的同一个时间节点修改了同一个地方的代码,合并是冲突</span><br><span class="line">思考:</span><br><span class="line">	我们能直接把某个开发人员开发的代码删除吗?</span><br><span class="line">显然不能</span><br><span class="line">	所以在处理冲突时,第一步应该时找开发另一个需求的人员沟通,之后才是处理冲突</span><br><span class="line">-----</span><br><span class="line">--&gt; 选中冲突的文件(带黄色感叹号的文件都是冲突的文件,如果有多个需要逐一处理)</span><br><span class="line">--&gt; 右键--&gt; 编辑冲突,</span><br><span class="line">--&gt;处理完毕后.标记已解决</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572182565594.png" alt="1572182565594"></p>
<p><img src="/assets/1572182727568.png" alt="1572182727568"></p>
<h1 id="8-tag-标签"><a href="#8-tag-标签" class="headerlink" title="8.tag  标签"></a>8.tag  标签</h1><h2 id="8-1-标签的概念"><a href="#8-1-标签的概念" class="headerlink" title="8.1 标签的概念"></a>8.1 标签的概念</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">如果你的项目达到一个重要的阶段，并希望永远记住那个特别的提交快照，你可以给它打上标签(tag)</span><br><span class="line">比如说，我们想为我们的项目发布一个&quot;1.0&quot;版本。 我们给最新一次提交打上（HEAD）&quot;v1.0&quot;的标签。</span><br><span class="line">标签可以理解为项目里程碑的一个标记,一旦打上了这个标记则,表示当前的代码将不允许提交</span><br></pre></td></tr></table></figure>

<h2 id="8-2-标签的创建-tag"><a href="#8-2-标签的创建-tag" class="headerlink" title="8.2  标签的创建(tag)"></a>8.2  标签的创建(tag)</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">标签的创建和分支的创建操作几乎一样</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572183849972.png" alt="1572183849972"></p>
<h2 id="8-3-标签的切换与删除"><a href="#8-3-标签的切换与删除" class="headerlink" title="8.3 标签的切换与删除"></a>8.3 标签的切换与删除</h2><p><img src="/assets/1572184030612.png" alt="1572184030612"></p>
<p><img src="/assets/1572184109746.png" alt="1572184109746"></p>
<h1 id="9-远程仓库"><a href="#9-远程仓库" class="headerlink" title="9. 远程仓库"></a>9. 远程仓库</h1><p>我们的代码不能总是放在本地,因为总是放在本地,一旦电脑出现故障,数据将丢失,怎么共享呢,这里我们需要一个服务器, 我们可以把代码放到服务器上,然后让别人下载,这样我峨嵋你既可以备份代码,也可以进行团队协作开发</p>
<h2 id="9-0-局域网仓库"><a href="#9-0-局域网仓库" class="headerlink" title="9.0 局域网仓库"></a>9.0 局域网仓库</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">实际上我们可以搭建一个单间的局域网服务器共享我们的代码</span><br></pre></td></tr></table></figure>

<h3 id="9-0-1本地相对路径-多个文件夹之间共享代码"><a href="#9-0-1本地相对路径-多个文件夹之间共享代码" class="headerlink" title="9.0.1本地相对路径,多个文件夹之间共享代码"></a>9.0.1本地相对路径,多个文件夹之间共享代码</h3><p><img src="/assets/1572535798760.png" alt="1572535798760"></p>
<h3 id="9-0-2开启局域网共享代码"><a href="#9-0-2开启局域网共享代码" class="headerlink" title="9.0.2开启局域网共享代码"></a>9.0.2开启局域网共享代码</h3><p><img src="/assets/1572535921581.png" alt="1572535921581"></p>
<p><img src="/assets/1572536029899.png" alt="1572536029899"></p>
<p>局域网这种共享是没有安全控制的,都可以访问,如果想要搭建一个可以控制权限的服务器需要借助第三方软件</p>
<p>gitblit,可以自行搜索搭建</p>
<h2 id="9-1-常用远程仓库托管服务"><a href="#9-1-常用远程仓库托管服务" class="headerlink" title="9.1 常用远程仓库托管服务"></a>9.1 常用远程仓库托管服务</h2><p>除了自己搭建服务器,其实我们可以使用一些免费的远程仓库,远程仓库有很多,常见的免费互联网远程仓库托管服务如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">www.github.com</span><br><span class="line">www.gitee.com</span><br><span class="line">www.gitlab.com</span><br><span class="line"></span><br><span class="line">github  是一个基于git实现在线代码托管的仓库，向互联网开放，企业版要收钱。</span><br><span class="line">gitee    即码云，是 oschina 免费给企业用的，不用自己搭建环境。</span><br><span class="line">gitlab   类似 github，一般用于在企业内搭建git私服，要自己搭环境。</span><br><span class="line"></span><br><span class="line">GitHub(gitee)、GitLab 不同点：</span><br><span class="line">1、GitHub如果使用私有仓库是需要付费的，(2019年开始私有仓库也是免费的但是只能3个人协同开发,想要更多需要收费)，GitLab可以在上面搭建私人的免费仓库。</span><br><span class="line">2、GitLab让开发团队对他们的代码仓库拥有更多的控制，相对于GitHub，它有不少的特色：</span><br><span class="line">    (1)允许免费设置仓库权限</span><br><span class="line">    (2)允许用户选择分享一个project的部分代码</span><br><span class="line">    (3)允许用户设置project的获取权限，进一步提升安全性</span><br><span class="line">    (4)可以设置获取到团队整体的改进进度</span><br><span class="line">    (5)通过innersourcing让不在权限范围内的人访问不到该资源</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>鉴于国内用户可能网络不好,这里我们使用gitee(码云) 来讲解我们的课程,其他可自行找资料学习非常类似</p>
<h2 id="9-2-码云账号注册"><a href="#9-2-码云账号注册" class="headerlink" title="9.2  码云账号注册"></a>9.2  码云账号注册</h2><p><img src="/assets/1572267659865.png" alt="1572267659865"></p>
<p>填写邮箱发送验证码,然后可以注册账号,主页如下</p>
<p><img src="/assets/1572268091948.png" alt="1572268091948"></p>
<h2 id="9-3-创建远程仓库"><a href="#9-3-创建远程仓库" class="headerlink" title="9.3 创建远程仓库"></a>9.3 创建远程仓库</h2><p><img src="/assets/1572274044100.png" alt="1572274044100"></p>
<p><img src="/assets/224637.png"></p>
<p>各个类型仓库之间的区别</p>
<p><img src="/assets/1572274100802.png" alt="1572274100802"></p>
<p><img src="/assets/1572274406371.png" alt="1572274406371"></p>
<h2 id="9-4-把本地代码推送到远端"><a href="#9-4-把本地代码推送到远端" class="headerlink" title="9.4  把本地代码推送到远端"></a>9.4  把本地代码推送到远端</h2><p><img src="/assets/1572275202869.png" alt="1572275202869"></p>
<p><img src="/assets/1572275492670.png" alt="1572275492670"></p>
<p><img src="/assets/1572275546739.png" alt="1572275546739"></p>
<p>此时我们刷新仓库发现代码已经存在了</p>
<p>我们填写的用户信息,会被保存在本地,下次提交无需填写用户名和密码</p>
<p><img src="/assets/1572277483698.png" alt="1572277483698"></p>
<h2 id="9-5-从远程仓库克隆代码"><a href="#9-5-从远程仓库克隆代码" class="headerlink" title="9.5  从远程仓库克隆代码"></a>9.5  从远程仓库克隆代码</h2><p>我们同样可以从库下载代码,</p>
<p>新建一个文件夹 repo2 ,进入然后进行如下操作</p>
<p><img src="/assets/1572275958701.png" alt="1572275958701"></p>
<p>此时我们发现我们的代码已经被下载下来了</p>
<h2 id="9-6-代码的修改与提交-查看历史"><a href="#9-6-代码的修改与提交-查看历史" class="headerlink" title="9.6  代码的修改与提交,查看历史"></a>9.6  代码的修改与提交,查看历史</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1)此时我们修改代码就不能仅仅是提交到本地了,提交完毕应该推送到远端服务器</span><br><span class="line">2)此时如果别人从远端仓库下载最新的代码其实是可以看到我们的代码修改记录的</span><br><span class="line">   git --&gt;显示日志</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572277139243.png" alt="1572277139243"></p>
<h2 id="9-7-ssh-连接概述"><a href="#9-7-ssh-连接概述" class="headerlink" title="9.7 ssh 连接概述"></a>9.7 ssh 连接概述</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">实际上git 不仅仅支持用户名密码方式的配置,可以有另外一种相对更加安全的配置即ssh 方式配置</span><br></pre></td></tr></table></figure>

<p> ssh 方式的底层原理</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ssh连接地城是RAS加密算法,又称非对称加密,是一种现在公认的最安全的加密方式</span><br><span class="line">数学基础好的同学可以研究一下</span><br><span class="line">https://www.cnblogs.com/cjm123/p/8243424.html</span><br><span class="line"></span><br><span class="line">公钥私钥加密可以看作古代 的&quot;虎符&quot; , 我们本地电脑有一份,远程服务器有一份, 只要 &quot;虎符&quot; 核对通过 表示身份无误,可以执行提交等操作,无需输入用户名密码</span><br></pre></td></tr></table></figure>

<h2 id="9-8-ssh-密钥的生成"><a href="#9-8-ssh-密钥的生成" class="headerlink" title="9.8 ssh 密钥的生成"></a>9.8 ssh 密钥的生成</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#生成公钥私钥</span><br><span class="line"> ssh-keygen -t rsa</span><br><span class="line"> 一直回车即可</span><br><span class="line"> 会默认用户目录 .ssh 目录生成一个默认的id_rsa文件 和id_rsa.pub</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572520139221.png" alt="1572520139221"></p>
<p><img src="/assets/1572520293700.png" alt="1572520293700"></p>
<h2 id="9-9-ssh-密钥配置"><a href="#9-9-ssh-密钥配置" class="headerlink" title="9.9 ssh 密钥配置"></a>9.9 ssh 密钥配置</h2><p><img src="/assets/1572520396806.png" alt="1572520396806"></p>
<h2 id="9-10-ssh-方式克隆-x2F-提交代码"><a href="#9-10-ssh-方式克隆-x2F-提交代码" class="headerlink" title="9.10 ssh 方式克隆&#x2F;提交代码:"></a>9.10 ssh 方式克隆&#x2F;提交代码:</h2><p>  配置完成之后我们克隆我们之前的项目</p>
<p><img src="/assets/1572522118610.png" alt="1572522118610"></p>
<p>修改后直接提交推送即可成功,,git 会自动去.ssh 目录找我们的私钥进行匹配</p>
<h2 id="9-11-远程仓库的其他操作"><a href="#9-11-远程仓库的其他操作" class="headerlink" title="9.11. 远程仓库的其他操作"></a>9.11. 远程仓库的其他操作</h2><p>概念</p>
<p><img src="/assets/1572522822946.png" alt="1572522822946"></p>
<p>当我们从 gitee 上查看别人的项目的时候我们可能会看到上图中的按钮</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">指数:</span><br><span class="line">	是gitee 网站根据当前项目的各项指标计算出来的一个值</span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572523045267.png" alt="1572523045267"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Star:</span><br><span class="line">	点赞, 注意这里的并不像朋友圈那样容易获得点赞,圈内人还是很克制的</span><br><span class="line">Watch:</span><br><span class="line">   如果你watch 了某个开源项目,那么这个项目后续所有的改动你将收到通知</span><br><span class="line">Fork :</span><br><span class="line">	将别人的代码克隆到你自己的仓库</span><br><span class="line">	作用一: 如果担心某个优秀的项目别人突然有一天不开源了,你可以fork到自己的仓库</span><br><span class="line">    作用二: 修改别人的代码</span><br><span class="line">	  以linux 为例,你其实不是linux 社区的开发人员,但是你 又想为linux 开发做贡献(维护代码)</span><br><span class="line">	   你并没有权限,怎们办?</span><br><span class="line">	   你可以先把linux 开源的代码 fork 到你自己的仓库,此时你就可以操作自己的仓库进行修改代码了</span><br><span class="line">	   如何让别人合并你修改好的代码呢? </span><br><span class="line">	    我们注意项目的上方有一个 &quot; Pull Request&quot; 这个按钮的意思是 &quot;请求求别人合并你修改的代码&quot;</span><br><span class="line">	    当我们发起一个 Pull Request 时 , 项目的拥有者将收到 Pull Request请求,然后将根据你提交代码的质量决定是否合并</span><br></pre></td></tr></table></figure>

<p>项目操作</p>
<p>1)我们可以删除修改我们自己仓库的基本信息</p>
<ol start="2">
<li>我们可以邀请其他人成为项目的开发人员或者管理人员</li>
</ol>
<p><img src="/assets/1572523819666.png" alt="1572523819666"></p>
<p>我们可以删除修改我们自己仓库的基本信息</p>
<p><img src="/assets/1572523928774.png" alt="1572523928774"></p>
<p><img src="/assets/1572523968992.png" alt="1572523968992"></p>
<h2 id="9-12-利用-gitee-搭建个人主页"><a href="#9-12-利用-gitee-搭建个人主页" class="headerlink" title="9.12 利用 gitee 搭建个人主页"></a>9.12 利用 gitee 搭建个人主页</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1)将静态资源上传至仓库</span><br><span class="line">2) 选择服务 pages 即可部署</span><br><span class="line">注意 1)必须有个index.html 文件</span><br><span class="line">注意 2) 只能搭建静态网站,动态网站请租赁服务器搭建提供服务</span><br><span class="line">注意 3) gitee 要求必须绑定手机号</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><img src="/assets/1572525575923.png" alt="1572525575923"></p>
<p>点击开启后gitee 会自动生成一个域名</p>
<p><img src="/assets/1572526097257.png" alt="1572526097257"></p>
<p>直接访问即可</p>
<p>此时我们已经在git 上部署了一个静态的网站</p>
<p><img src="/assets/1572526136074.png" alt="1572526136074"></p>
<h1 id="10-命令行–-git基本操作"><a href="#10-命令行–-git基本操作" class="headerlink" title="10.命令行– git基本操作"></a>10.命令行– git基本操作</h1><h2 id="10-1-介绍"><a href="#10-1-介绍" class="headerlink" title="10.1  介绍"></a>10.1  介绍</h2><p>​	上述我们的操作 使用的 是客户端TortoiseGit 操作的git ,实际上底层依旧是使用的命令行帮我们执行, 在早期 git 并没有窗口化工具,开发人员只能使用命令行模式</p>
<p>  实际上,如果你掌握并熟练使用了命令行模式操作git 的话,你会发现某些操作命令行比窗口化操作要简单</p>
<p>所有你在工作中会发现高深的技术人员可能会喜欢命令行模式提交git</p>
<h3 id="10-2-环境配置"><a href="#10-2-环境配置" class="headerlink" title="10.2 环境配置"></a>10.2 环境配置</h3><p>当安装Git后首先要做的事情是设置用户名称和email地址。这是非常重要的，因为每次Git提交都会使用该用户信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">设置用户信息</span> </span><br><span class="line">   git config --global user.name “itcast”</span><br><span class="line">   git config --global user.email “itcast@itcast.cn”</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查看配置信息</span></span><br><span class="line">   git config --list</span><br><span class="line">   git config user.name</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">通过上面的命令设置的信息会保存在~/.gitconfig文件中</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="10-3-初始化本地仓库-init"><a href="#10-3-初始化本地仓库-init" class="headerlink" title="10.3  初始化本地仓库 init"></a>10.3  初始化本地仓库 init</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">初始化仓库带工作区</span></span><br><span class="line">git init</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">初始化仓库不带工作区</span></span><br><span class="line">git init --bare  </span><br></pre></td></tr></table></figure>

<h3 id="10-4-克隆-clone"><a href="#10-4-克隆-clone" class="headerlink" title="10.4 克隆 clone"></a>10.4 克隆 clone</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从远程仓库克隆</span></span><br><span class="line">git clone 远程Git仓库地址 </span><br><span class="line">例如: git clone https://gitee.com/itcast/gittest.git</span><br></pre></td></tr></table></figure>

<h3 id="10-5-查看状态-status"><a href="#10-5-查看状态-status" class="headerlink" title="10.5  查看状态 status"></a>10.5  查看状态 status</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看状态</span></span><br><span class="line">git status </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查看状态 使输出信息更加简洁</span></span><br><span class="line">git status –s </span><br></pre></td></tr></table></figure>

<h3 id="10-6-add"><a href="#10-6-add" class="headerlink" title="10.6 add"></a>10.6 add</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将未跟踪的文件加入暂存区</span></span><br><span class="line">git add  &lt;文件名&gt;  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将暂存区的文件取消暂存 (取消 add )</span></span><br><span class="line">git reset  &lt;文件名&gt;  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="10-7-commit"><a href="#10-7-commit" class="headerlink" title="10.7 commit"></a>10.7 commit</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">git commit 将暂存区的文件修改提交到本地仓库</span></span><br><span class="line">git commit -m &quot;日志信息&quot;  &lt;文件名&gt;  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="10-8-删除-rm"><a href="#10-8-删除-rm" class="headerlink" title="10.8 删除 rm"></a>10.8 删除 rm</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从本地工作区 删除文件</span></span><br><span class="line">git rm &lt;文件名&gt;  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果本工作区库误删, 想要回退</span></span><br><span class="line">git checkout head &lt;文件名&gt;  </span><br></pre></td></tr></table></figure>

<h1 id="11-命令行–git-远程仓库操作"><a href="#11-命令行–git-远程仓库操作" class="headerlink" title="11. 命令行–git 远程仓库操作"></a>11. 命令行–git 远程仓库操作</h1><h2 id="11-1-查看远程"><a href="#11-1-查看远程" class="headerlink" title="11.1    查看远程"></a>11.1    查看远程</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看远程  列出指定的每一个远程服务器的简写</span></span><br><span class="line">git remote </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看远程 , 列出 简称和地址</span></span><br><span class="line">git remote  -v  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看远程仓库详细地址</span></span><br><span class="line">git remote show  &lt;仓库简称&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="11-2-添加-x2F-移除远测仓库"><a href="#11-2-添加-x2F-移除远测仓库" class="headerlink" title="11.2 添加&#x2F;移除远测仓库"></a>11.2 添加&#x2F;移除远测仓库</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加远程仓库</span></span><br><span class="line">git remote add &lt;shortname&gt; &lt;url&gt;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">移除远程仓库和本地仓库的关系(只是从本地移除远程仓库的关联关系，并不会真正影响到远程仓库)</span></span><br><span class="line">git remote rm &lt;shortname&gt; </span><br></pre></td></tr></table></figure>

<h2 id="11-3-从远程仓库获取代码"><a href="#11-3-从远程仓库获取代码" class="headerlink" title="11.3 从远程仓库获取代码"></a>11.3 从远程仓库获取代码</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从远程仓库克隆</span></span><br><span class="line">git clone &lt;url&gt; </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从远程仓库拉取 (拉取到.git 目录,不会合并到工作区,工作区发生变化)</span></span><br><span class="line">git fetch  &lt;shortname&gt;  &lt;分支名称&gt;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">手动合并  把某个版本的某个分支合并到当前工作区</span></span><br><span class="line">git merge &lt;shortname&gt;/&lt;分支名称&gt;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">从远程仓库拉取 (拉取到.git 目录,合并到工作区,工作区不发生变化) = fetch+merge</span></span><br><span class="line">git pull  &lt;shortname&gt;  &lt;分支名称&gt;</span><br><span class="line">git pull  &lt;shortname&gt;  &lt;分支名称&gt;  --allow-unrelated-histories  #  强制拉取合并</span><br></pre></td></tr></table></figure>

<p>注意：如果当前本地仓库不是从远程仓库克隆，而是本地创建的仓库，并且仓库中存在文件，此时再从远程仓库拉取文件的时候会报错（fatal: refusing to merge unrelated histories ），解决此问题可以在git pull命令后加入参数–allow-unrelated-histories (如上 命令)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将本地仓库推送至远程仓库的某个分支</span></span><br><span class="line">git push [remote-name] [branch-name]</span><br></pre></td></tr></table></figure>

<h1 id="12-命令行–-分支"><a href="#12-命令行–-分支" class="headerlink" title="12.  命令行– 分支"></a>12.  命令行– 分支</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">默认 分支名称为 master</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">列出所有本地分支</span></span><br><span class="line">git branch</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">列出所有远程分支</span></span><br><span class="line">git branch -r</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">列出所有本地分支和远程分支</span></span><br><span class="line">git branch -a</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建分支</span></span><br><span class="line">git branch &lt;分支名&gt;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">切换分支</span> </span><br><span class="line">git checkout &lt;分支名&gt;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除分支(如果分支已经修改过,则不允许删除)</span></span><br><span class="line">git branch -d  &lt;分支名&gt;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">强制删除分支</span></span><br><span class="line">git branch -D  &lt;分支名&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">提交分支至远程仓库</span></span><br><span class="line">git push &lt;仓库简称&gt; &lt;分支名称&gt;	</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">合并分支 将其他分支合并至当前工作区</span></span><br><span class="line">git merge &lt;分支名称&gt;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除远程仓库分支</span></span><br><span class="line">git push origin –d branchName</span><br></pre></td></tr></table></figure>

<h1 id="13-命令行-–tag"><a href="#13-命令行-–tag" class="headerlink" title="13 . 命令行 –tag"></a>13 . 命令行 –tag</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">列出所有tag</span></span><br><span class="line">git tag</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看tag详细信息</span> </span><br><span class="line">git show [tagName]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新建一个tag</span></span><br><span class="line">git tag [tagName]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">提交指定tag</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git push [仓库简称] [tagName]</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">新建一个分支，指向某个tag</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git checkout -b [branch] [tag]</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除本地tag</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git tag -d [tag]</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">删除远程tag (注意 空格)</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">git push origin :refs/tags/[tag]</span></span><br></pre></td></tr></table></figure>

<h1 id="14-案例"><a href="#14-案例" class="headerlink" title="14. 案例"></a>14. 案例</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">企业中我们是如何开发的</span><br><span class="line">1) 入职第一天,管理人员分配/git账号密码 </span><br><span class="line">2) 开发人员下载代码即文档/ 根据文档将环境搭建成功</span><br><span class="line">3) 团队一般会给你讲讲项目相关的支持</span><br><span class="line">----</span><br><span class="line">4) 你接到第一个需求(或者某个功能,一般要经过沟通,分析,设计...等过程)</span><br><span class="line">5) 创建feature分支(一般一个需求对应一个feature,命名格式上标注该需求的id)</span><br><span class="line">6) 开发需求,本地测试,提交代码到当前需求对应的feature分支,</span><br><span class="line">	一般来讲为了避免将测试代码提交,需要提交前,检查如下步骤</span><br><span class="line">	6.1) 是否多提交了某个文件,比如测试文件</span><br><span class="line">	6.2) 是否漏提交文件</span><br><span class="line">	6.3) 打开每一个应该提交的文件,判断是否多提交了一行代码,是否少提交了一行代码,是否删除了本应该存在的代码 </span><br><span class="line">	检查完毕提交代码</span><br><span class="line">7) 合并分支至test分支-- 测试人员会在test分支中测试</span><br><span class="line">8) 测试人员测试bug ,开发者在feature分支上继续修改,提交</span><br><span class="line">9) 测试人员测试通过 ,test分支会被测试人员合并到develop开发分支,再次测试</span><br><span class="line">10)develop分支最终会被合并到master主分支</span><br><span class="line"></span><br></pre></td></tr></table></figure>











<p>&#96;</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/19/Git%E6%95%99%E7%A8%8B/" data-id="clj25kfya0002n0ur9xu45c0d" data-title="Git" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-flume" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/19/flume/" class="article-date">
  <time class="dt-published" datetime="2023-06-19T00:49:49.442Z" itemprop="datePublished">2023-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/19/flume/">Flume</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Flume概述"><a href="#Flume概述" class="headerlink" title="Flume概述"></a>Flume概述</h1><p>Flume是一个进行海量流式（streaming）事件数据采集的分布式系统。Flume多用于对软件系统的日志文件数据的采集，目前成为Apache软件基金会的顶级项目之一。</p>
<p>Flume是的一个<code>分布式、高可用、高可靠</code>的海量<code>日志采集</code>、<code>聚合</code>和<code>传输</code>的系统，支持在日志系统中定制各类数据发送方，用于收集数据，同时提供了对数据进行<code>简单处理</code>并<code>写到</code>各种<code>数据接收方</code>的能力。</p>
<p>Flume的设计原理是基于<code>数据流</code>的，能够将不同数据源的海量日志数据进行高效<code>收集、聚合、移动</code>，最后存储到一个<code>中心化数据存储系统</code>中。 Flume能够做到近似实时的推送，并且可以满足数据量是持续且量级很大的情况。比如它可以收集社交网站日志，并将这些数量庞大的日志数据从网站服务器上汇集起来，存储到HDFS或 HBase分布式数据库中。</p>
<blockquote>
<p>•Flume可以将应用产生的数据存储到任何集中存储器中，比如HDFS,HBase</p>
<p>•数据缓存。当收集数据的速度超过将写入数据的时候，保证其能够在两者之间提供平衡。</p>
<p>•Flume的管道是基于事务，保证了数据在传送和接收时的一致性.</p>
<p>•Flume是可靠的，容错性高的，可升级的，易管理的,并且可定制的</p>
</blockquote>
<p>Flume的应用场景:比如一个<code>电商网站</code>，想从网站访问者中访问一些<code>特定的节点区域</code>来<code>分析消费者</code>的<code>购物意图和行为</code>。为了实现这一点，需要收集到消费者访问的页面以及点击的产品等日志信息，并移交到大数据 Hadoop平台上去分析，可以利用 Flume做到这一点。现在流行的内容推送，比如广告定点投放以及新闻私人定制也是基于这个道理。</p>
<h1 id="Flume架构"><a href="#Flume架构" class="headerlink" title="Flume架构"></a>Flume架构</h1><p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200327112206283.png" alt="image-20200327112206283"></p>
<img src="./md图/flume.assets/image-20230303092506796.png" alt="image-20230303092506796" style="zoom:50%;" />

<p> Flume采集系统以Agent为单位，一个Agent就是一个工作进程。一个完整的Agent由三个组件构成，它们分别是：</p>
<p>(1)Source：用于从<code>数据源接收数据</code>（即采集数据），需要<code>设置数据的来源类型</code>；</p>
<p>(2)Sink：用于<code>传递数据给目的地</code>（即保存数据），需要<code>设置数据的目的地类型</code>；</p>
<p>(3)Channel：用于<code>Source和Sink的连接</code>，并缓存传输的数据，需要<code>设置缓存类型</code>。</p>
<h2 id="1-Event"><a href="#1-Event" class="headerlink" title="1.Event"></a>1.Event</h2><p>事件是Flume内部数据传输的最基本单元，将传输的数据进行<code>封装</code>。事件本身是由一个<code>载有数据的字节数组</code>和<code>可选的headers头部信息</code>构成，如下图所示。Flume以事件的形式将数据从源头传输到最终的目的地。</p>
<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200323103129436.png" alt="image-20200323103129436"></p>
<h2 id="2-Agent"><a href="#2-Agent" class="headerlink" title="2.Agent"></a>2.Agent</h2><p>Flume Agent 是一个<code>JVM进程</code>，通过三个组件（<code>source</code>、<code>channel</code>、<code>sink</code>）将事件流从一个外部数据源收集并发送给下一个目的地。</p>
<h3 id="（1）Source"><a href="#（1）Source" class="headerlink" title="（1）Source"></a>（1）Source</h3><p>从<code>数据发生器接收数据</code>，并将数据以Flume的Event格式传递给一个或多个通道（Channel）</p>
<p>支持Source:</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#avro-source">Avro Source</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#thrift-source">Thrift Source</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#exec-source">Exec Source</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#jms-source">JMS Source</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#spooling-directory-source">Spooling Directory Source</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#taildir-source">Taildir Source</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#twitter-1-firehose-source-experimental">Twitter 1% firehose Source (experimental)</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#kafka-source">Kafka Source</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#netcat-tcp-source">NetCat TCP Source</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#netcat-udp-source">NetCat UDP Source</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#sequence-generator-source">Sequence Generator Source</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#syslog-sources">Syslog Sources</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#http-source">HTTP Source</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#stress-source">Stress Source</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#legacy-sources">Legacy Sources</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#custom-source">Custom Source</a></li>
</ul>
<p>常用source类型：</p>
<table>
<thead>
<tr>
<th><strong>序号</strong></th>
<th><strong>类型名称</strong></th>
<th><strong>功   能</strong></th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td><strong>spooldir</strong></td>
<td>数据来源于磁盘文件目录</td>
</tr>
<tr>
<td>2</td>
<td><strong>exec</strong></td>
<td>表明数据来源于Linux命令</td>
</tr>
<tr>
<td>3</td>
<td><strong>taildir</strong></td>
<td>表明数据来源于文件的实时新增内容</td>
</tr>
<tr>
<td>4</td>
<td>avro</td>
<td>表明数据来源于指定IP和端口发送的AVRO消息</td>
</tr>
<tr>
<td>5</td>
<td>thrift</td>
<td>表明数据来源于指定IP和端口发送的THRIFT消息</td>
</tr>
<tr>
<td>6</td>
<td>netcat</td>
<td>表明数据来源于指定IP和端口发送的netcat  TCP消息</td>
</tr>
<tr>
<td>7</td>
<td>netcatudp</td>
<td>表明数据来源于指定IP和端口发送的netcat UDP消息</td>
</tr>
</tbody></table>
<h3 id="（2）Channel"><a href="#（2）Channel" class="headerlink" title="（2）Channel"></a>（2）Channel</h3><p>**一种<code>短暂的存储容器</code>**，位于<code> Source和Sink之间</code>，起着桥梁的作用。 Channel将从Source处接收到的 Event格式的数据<code>缓存</code>起来，当Sink<code>成功</code>地将<code> Events发送</code>到下一跳的Channel或最终<code>目的地</code>后， Events从 Channel<code>移除</code>。Channel是一个完整的事务，这一点保证了数据在收发的时候的一致性。可以把 Channel看成一个<code>FIFO（先进先出）队列</code>，<code>当数据的获取速率超过流出速率时，将Event保存到队列中，再从队中一个个出来</code>。</p>
<p>有以下几种Channel：</p>
<ul>
<li><strong>Memory Channel</strong> 事件存储在可配置容量的内存队列中，队列容量即为可存储最大事件数量，适用于高吞吐量场景，在agent出现错误时有可能会丢失部分数据</li>
<li><strong>File Channel</strong> 基于文件系统的持久化存储</li>
<li>Spillable Memory Channel 内存和文件混合Channel，当内存队列满了之后，新的事件会存储在文件系统，目前处于实验阶段，不建议在生产环境中使用</li>
<li>JDBC Channe 事件存储在持久化的数据库中，目前只支持Derby</li>
<li>Kafka Channel 事件存储在Kafka集群中</li>
<li>Pseudo Transaction Channel 伪事务Channel，仅用于测试，不能在生产环境使用</li>
<li>Custom Channel 自定义Channel</li>
</ul>
<p>常用channel类型</p>
<table>
<thead>
<tr>
<th><strong>序号</strong></th>
<th><strong>类型名称</strong></th>
<th><strong>功   能</strong></th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td><strong>memory</strong></td>
<td>以内存作为数据的缓存，读写速度快，但数据可能会丢失</td>
</tr>
<tr>
<td>2</td>
<td><strong>file</strong></td>
<td>以磁盘文件作为数据的缓存，读写速度相对慢，但数据不会丢失</td>
</tr>
</tbody></table>
<h3 id="（3）Sink"><a href="#（3）Sink" class="headerlink" title="（3）Sink"></a>（3）Sink</h3><p>获取Channel暂时保存的数据并进行处理。sink从channel中<code>移除事件</code>，并将其<code>发送到下一个agent</code>（简称下一跳）或者事件的<code>最终目的地</code>，比如HDFS。</p>
<p>Sink分类：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#hdfs-sink">HDFS Sink</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#hive-sink">Hive Sink</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#logger-sink">Logger Sink</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#avro-sink">Avro Sink</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#thrift-sink">Thrift Sink</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#irc-sink">IRC Sink</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#file-roll-sink">File Roll Sink</a> 将Events保存在本地文件系统</li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#null-sink">Null Sink</a> 抛弃从Channel接收的所有事件</li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#hbasesinks">HBaseSinks</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#morphlinesolrsink">MorphlineSolrSink</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#elasticsearchsink">ElasticSearchSink</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#kite-dataset-sink">Kite Dataset Sink</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#kafka-sink">Kafka Sink</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#http-sink">HTTP Sink</a></li>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#custom-sink">Custom Sink</a></li>
</ul>
<p>常用sink类型</p>
<table>
<thead>
<tr>
<th><strong>序号</strong></th>
<th><strong>类型名称</strong></th>
<th><strong>功能</strong></th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td><strong>hdfs</strong></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td><strong>file_roll</strong></td>
<td>表明数据的目的地是系统的本地目录</td>
</tr>
<tr>
<td>3</td>
<td><strong>KafkaSink</strong></td>
<td>表明数据的目的地是Kafka的生产者</td>
</tr>
<tr>
<td>4</td>
<td><strong>avro</strong></td>
<td>表明采用的是AVRO消息方式传递数据到数据目的地，需指定数据目的地的主机名和端口</td>
</tr>
<tr>
<td>5</td>
<td>hbase</td>
<td>表明数据的目的地是HBase</td>
</tr>
<tr>
<td>6</td>
<td>asynchbase</td>
<td>表明以异步方式写数据到HBase</td>
</tr>
<tr>
<td>7</td>
<td>hive</td>
<td>表明数据的目的地是Hive</td>
</tr>
<tr>
<td>8</td>
<td>elasticsearch</td>
<td>表明数据的目的地是elasticsearch</td>
</tr>
</tbody></table>
<h2 id="3-数据流模型"><a href="#3-数据流模型" class="headerlink" title="3.数据流模型"></a>3.数据流模型</h2><p><img src="/md%E5%9B%BE%5Cflume.assets/UserGuide_image00.png" alt="Agent component diagram"></p>
<p>过程简要说明如下:<br>（1）外部数据源（Web Server）将Flume可识别的 Event发送到 Source<br>（2） Source收到 Event事件后存储到一个或多个Channel通道中。<br>（3）Channel保留 Event直到Sink将其处理完毕。<br>（4）Sink从 Channel中取出数据，并将其传输至外部存储（HDFS）。</p>
<h3 id="4-可靠性"><a href="#4-可靠性" class="headerlink" title="4.可靠性"></a>4.可靠性</h3><p>事件在每个agent的channel中短暂存储，然后事件被发送到下一个agent或者最终目的地。事件只有在存储在下一个channel或者最终存储后才从当前的channel中删除。</p>
<p>Flume使用事务的办法来保证Events的可靠传递。Source和Sink分别被封装在事务中，事务由保存Event的存储或者Channel提供。这就保证了Event在数据流的点对点传输中是可靠的。在多跳的数据流中，上一跳的sink和下一跳的source均运行事务来保证数据被安全地存储到下一跳的channel中。</p>
<h1 id="零、Flume安装"><a href="#零、Flume安装" class="headerlink" title="零、Flume安装"></a>零、Flume安装</h1><p>Flume下载页面：<a target="_blank" rel="noopener" href="http://flume.apache.org/download.html">http://flume.apache.org/download.html</a></p>
<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200323142300139.png" alt="image-20200323142300139"></p>
<p>将<a target="_blank" rel="noopener" href="http://www.apache.org/dyn/closer.lua/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz"> apache-flume-1.9.0-bin.tar.gz</a>下载到CentOS系统中，对其解压</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">解压命令</span></span><br><span class="line">tar xzf apache-flume-1.9.0-bin.tar.gz</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加软连接</span></span><br><span class="line">ln -s apache-flume-1.9.0-bin flume</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Flume使用需要依赖JDK1.8以上环境，确保已安装</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将Flume安装目录配置到PATH中，方便在任意目录使用</span></span><br><span class="line">vi /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加以下内容</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">FLUME_HOME</span></span><br><span class="line">export FLUME_HOME=/export/server/flume</span><br><span class="line">export PATH=$PATH:$FLUME_HOME/bin</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">保存成功后刷新</span></span><br><span class="line">source /etc/profile</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看是否设置成功</span></span><br><span class="line">echo $FLUME_HOME</span><br></pre></td></tr></table></figure>

<h1 id="一、入门使用示例"><a href="#一、入门使用示例" class="headerlink" title="一、入门使用示例"></a>一、入门使用示例</h1><h2 id="案例说明"><a href="#案例说明" class="headerlink" title="案例说明"></a>案例说明</h2><p>使用Flume监听某个端口，使用Netcat向这个端口发送数据，Flume将接收到的数据打印到控制台。</p>
<p><code>Netcat是一款TCP/UDP测试工具</code>，可以通过以下命令安装</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y nc</span><br></pre></td></tr></table></figure>

<h2 id="使用组件"><a href="#使用组件" class="headerlink" title="使用组件"></a>使用组件</h2><ul>
<li><p><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#netcat-tcp-source">NetCat TCP Source</a></p>
<p>必须属性</p>
<table>
<thead>
<tr>
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>channels</strong></td>
<td align="left">–</td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>type</strong></td>
<td align="left">–</td>
<td align="left"><code>netcat</code></td>
</tr>
<tr>
<td align="left"><strong>bind</strong></td>
<td align="left">–</td>
<td align="left">绑定的主机名或者IP地址</td>
</tr>
<tr>
<td align="left"><strong>port</strong></td>
<td align="left">–</td>
<td align="left">绑定端口</td>
</tr>
</tbody></table>
</li>
<li><p><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#memory-channel">Memory Channel</a></p>
<p>必须属性</p>
<table>
<thead>
<tr>
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>type</strong></td>
<td align="left">–</td>
<td align="left"><code>memory</code></td>
</tr>
</tbody></table>
</li>
<li><p><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#logger-sink">Logger Sink</a></p>
<p>必须属性</p>
<table>
<thead>
<tr>
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>channel</strong></td>
<td align="left">–</td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>type</strong></td>
<td align="left">–</td>
<td align="left"><code>logger</code></td>
</tr>
</tbody></table>
</li>
</ul>
<h2 id="添加配置文件"><a href="#添加配置文件" class="headerlink" title="添加配置文件"></a>添加配置文件</h2><p>在flume&#x2F;myconf目录下添加配置文件netcat-logger.conf</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example1-netcat-logger.conf: 单节点Flume配置</span></span><br><span class="line"><span class="comment"># 定义agent名称为a1</span></span><br><span class="line"><span class="comment"># 设置3个组件的名称</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置source类型为NetCat,监听地址为本机，端口为44444</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">netcat</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#source和channel关联</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置sink类型为Logger</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="comment"># 将sink绑定到channel上</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>

<h2 id="启动flume"><a href="#启动flume" class="headerlink" title="启动flume"></a>启动flume</h2><p>查看Flume使用命令</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng help</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">Usage: /opt/soft/apache-flume-1.9.0-bin/bin/flume-ng &lt;command&gt; [options]...</span><br><span class="line"></span><br><span class="line">commands:</span><br><span class="line">  help                      display this help text</span><br><span class="line">  agent                     run a Flume agent</span><br><span class="line">  avro-client               run an avro Flume client</span><br><span class="line">  version                   show Flume version info</span><br><span class="line"></span><br><span class="line">global options:</span><br><span class="line">  --conf,-c &lt;conf&gt;          use configs in &lt;conf&gt; directory</span><br><span class="line">  --classpath,-C &lt;cp&gt;       append to the classpath</span><br><span class="line">  --dryrun,-d               do not actually start Flume, just print the command</span><br><span class="line">  --plugins-path &lt;dirs&gt;     colon-separated list of plugins.d directories. See the</span><br><span class="line">                            plugins.d section in the user guide for more details.</span><br><span class="line">                            Default: $FLUME_HOME/plugins.d</span><br><span class="line">  -Dproperty=value          sets a Java system property value</span><br><span class="line">  -Xproperty=value          sets a Java -X option</span><br><span class="line"></span><br><span class="line">agent options:</span><br><span class="line">  --name,-n &lt;name&gt;          the name of this agent (required)</span><br><span class="line">  --conf-file,-f &lt;file&gt;     specify a config file (required if -z missing)</span><br><span class="line">  --zkConnString,-z &lt;str&gt;   specify the ZooKeeper connection to use (required if -f missing)</span><br><span class="line">  --zkBasePath,-p &lt;path&gt;    specify the base path in ZooKeeper for agent configs</span><br><span class="line">  --no-reload-conf          do not reload config file if changed</span><br><span class="line">  --help,-h                 display help text</span><br><span class="line"></span><br><span class="line">avro-client options:</span><br><span class="line">  --rpcProps,-P &lt;file&gt;   RPC client properties file with server connection params</span><br><span class="line">  --host,-H &lt;host&gt;       hostname to which events will be sent</span><br><span class="line">  --port,-p &lt;port&gt;       port of the avro source</span><br><span class="line">  --dirname &lt;dir&gt;        directory to stream to avro source</span><br><span class="line">  --filename,-F &lt;file&gt;   text file to stream to avro source (default: std input)</span><br><span class="line">  --headerFile,-R &lt;file&gt; File containing event headers as key/value pairs on each new line</span><br><span class="line">  --help,-h              display help text</span><br><span class="line"></span><br><span class="line">  Either --rpcProps or both --host and --port must be specified.</span><br><span class="line"></span><br><span class="line">Note that if &lt;conf&gt; directory is specified, then it is always included first</span><br><span class="line">in the classpath.</span><br></pre></td></tr></table></figure>

<p>启动agent</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">`flume-ng agent -n a1 -c conf -f example.conf -Dflume.root.logger=INFO,console`</span><br><span class="line"></span><br><span class="line">[root@node1 flume]</span><br><span class="line">bin/flume-ng agent -n a1 -c conf -f myconf/example1-netcat-logger.conf -Dflume.root.logger=INFO,console</span><br><span class="line"></span><br><span class="line">在工作环境中的启动命令一般为：</span><br><span class="line">nohup bin/flume-ng agent -n a1 -c conf -f myconf/example1-netcat-logger.conf 1&gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">用 /dev/null 2&gt;&amp;1 这条命令的意思是将标准输出和错误输出全部重定向到/dev/null中,也就是将产生的所有信息丢弃</span><br></pre></td></tr></table></figure>



<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200110174336210.png" alt="image-20200110174336210"></p>
<h2 id="（1）使用Netcat测试"><a href="#（1）使用Netcat测试" class="headerlink" title="（1）使用Netcat测试"></a>（1）使用Netcat测试</h2><p>从另一个终端启动Netcat连接到44444端口，发送一些字符串</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc node1 44444</span><br></pre></td></tr></table></figure>

<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200323153910638.png" alt="image-20200323153910638"></p>
<p>观察agent控制台</p>
<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200323154036797.png" alt="image-20200323154036797"></p>
<p>从这里可以看到事件由头部和字节数组组成。</p>
<p><code>如果开启了防火墙，需要添加防火墙规则</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --zone=public --add-port=44444/tcp --permanent</span><br><span class="line">firewall-cmd --reload</span><br></pre></td></tr></table></figure>



<h2 id="（2）使用telnet测试"><a href="#（2）使用telnet测试" class="headerlink" title="（2）使用telnet测试"></a>（2）使用telnet测试</h2><p>如果没有安装telnet，检查源</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum list | grep telnet</span><br></pre></td></tr></table></figure>

<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200110173530605.png" alt="image-20200110173530605"></p>
<p>安装telnet</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y telnet.x86_64</span><br><span class="line">yum install -y telnet-server.x86_64</span><br></pre></td></tr></table></figure>

<p>启动telnet</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">telnet node1 44444</span><br></pre></td></tr></table></figure>

<p>在控制台输入内容</p>
<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200110174126132.png" alt="image-20200110174126132"></p>
<p>可以在flume窗口查看到消息</p>
<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200110174514218.png" alt="image-20200110174514218"></p>
<h1 id="二、exec-source测试"><a href="#二、exec-source测试" class="headerlink" title="二、exec_source测试"></a>二、exec_source测试</h1><p>企业中应用程序部署后会将日志写入到文件中，可以使用Flume从各个日志文件将日志收集到日志中心以便于查找和分析。</p>
<h2 id="工作机制："><a href="#工作机制：" class="headerlink" title="工作机制："></a><strong>工作机制</strong>：</h2><p><code>启动一个用户所指定的linux shell命令</code>；<br>采集这个linux shell命令的<code>标准输出</code>，作为<code>收集到的数据</code>，<code>转为event写入channel</code>；</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230114090311228.png" alt="image-20230114090311228"></p>
<h2 id="参数详解："><a href="#参数详解：" class="headerlink" title="参数详解："></a><strong>参数详解：</strong></h2><table>
<thead>
<tr>
<th><strong>channels</strong></th>
<th>–</th>
<th>本source要发往的channel</th>
</tr>
</thead>
<tbody><tr>
<td><strong>type</strong></td>
<td>–</td>
<td>本source的类别名称：exec</td>
</tr>
<tr>
<td><strong>command</strong></td>
<td>–</td>
<td>本source所要运行的linux命令,比如： tail  -F &#x2F;path&#x2F;file</td>
</tr>
<tr>
<td>shell</td>
<td>–</td>
<td>指定运行上述命令所用shell</td>
</tr>
<tr>
<td>restartThrottle</td>
<td>10000</td>
<td>命令die了以后，重启的时间间隔</td>
</tr>
<tr>
<td>restart</td>
<td>false</td>
<td>命令die了以后，是否要重启</td>
</tr>
<tr>
<td>logStdErr</td>
<td>false</td>
<td>是否收集命令的错误输出stderr</td>
</tr>
<tr>
<td>batchSize</td>
<td>20</td>
<td>提交的event批次大小</td>
</tr>
<tr>
<td>batchTimeout</td>
<td>3000</td>
<td>发往下游没完成前，等待的时间</td>
</tr>
<tr>
<td>selector.type</td>
<td>replicating</td>
<td>指定channel选择器：replicating  or multiplexing</td>
</tr>
<tr>
<td>selector.*</td>
<td></td>
<td>选择器的具体参数</td>
</tr>
<tr>
<td>interceptors</td>
<td>–</td>
<td>指定拦截器</td>
</tr>
<tr>
<td>interceptors.*</td>
<td></td>
<td>指定的拦截器的具体参数</td>
</tr>
</tbody></table>
<h2 id="配置文件："><a href="#配置文件：" class="headerlink" title="配置文件："></a><strong>配置文件：</strong></h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example2-exec-source-logger.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.command</span> = <span class="string">tail -F /export/data/flume-example-data/shell/access.log </span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>

<h2 id="启动测试："><a href="#启动测试：" class="headerlink" title="启动测试："></a><strong>启动测试：</strong></h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1.准备一个日志文件</span><br><span class="line">2.写一个脚本模拟往日志文件中持续写入数据</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..10000&#125;; </span><br><span class="line"><span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$&#123;i&#125;</span> “bigdata <span class="built_in">log</span>”  &gt;&gt;  access.log ; </span><br><span class="line"><span class="built_in">sleep</span> 0.5; </span><br><span class="line"><span class="keyword">done</span></span><br><span class="line">3.创建一个flume自定义配置文件</span><br><span class="line">4.启动flume采集</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c conf/ -f myconf/example2-exec-source-logger.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<h1 id="三、spooldir-source测试"><a href="#三、spooldir-source测试" class="headerlink" title="三、spooldir_source测试"></a>三、spooldir_source测试</h1><h2 id="工作机制：-1"><a href="#工作机制：-1" class="headerlink" title="工作机制："></a><strong>工作机制：</strong></h2><p><code>监视一个指定的文件夹</code>，如果文件夹下<code>有没采集过的新文件</code>，则将这些<code>新文件中的数据采集</code>，并<code>转成event写入channel</code>；<br>注意：spooling目录中的文件<code>必须是不可变的</code>（静态的），而且是<code>不能重名</code>的！否则，source会loudly fail！（<code>抛异常</code>）</p>
<h2 id="参数详解：-1"><a href="#参数详解：-1" class="headerlink" title="参数详解："></a><strong>参数详解：</strong></h2><table>
<thead>
<tr>
<th><strong>Property Name</strong></th>
<th><strong>Default</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>channels</strong></td>
<td>–</td>
<td></td>
</tr>
<tr>
<td><strong>type</strong></td>
<td>–</td>
<td>The component type name, needs to be spooldir.</td>
</tr>
<tr>
<td><strong>spoolDir</strong></td>
<td>–</td>
<td>The directory from which to read files from.</td>
</tr>
<tr>
<td>fileSuffix</td>
<td>.COMPLETED</td>
<td>采集完成的文件，添加什么后缀名</td>
</tr>
<tr>
<td>deletePolicy</td>
<td>never</td>
<td>是否删除采完的文件: never or immediate</td>
</tr>
<tr>
<td>fileHeader</td>
<td>false</td>
<td>是否将所采集文件的绝对路径添加到header中</td>
</tr>
<tr>
<td>fileHeaderKey</td>
<td>file</td>
<td>上述header的key名称</td>
</tr>
<tr>
<td>basenameHeader</td>
<td>false</td>
<td>是否将文件名添加到header</td>
</tr>
<tr>
<td>basenameHeaderKey</td>
<td>basename</td>
<td>上述header的key名称</td>
</tr>
<tr>
<td>includePattern</td>
<td>^.*$</td>
<td>指定需要采集的文件名的正则表达式</td>
</tr>
<tr>
<td>ignorePattern</td>
<td>^$</td>
<td>指定要排除的文件名的正则表达式  如果一个文件名即符合includePattern又匹配ignorePattern，则该文件不采</td>
</tr>
<tr>
<td>trackerDir</td>
<td>.flumespool</td>
<td>记录元数据的目录所在路径，可以用绝对路径也可以用相对路径（相对于采集目录）</td>
</tr>
<tr>
<td>trackingPolicy</td>
<td>rename</td>
<td>采集进度跟踪策略，有两种：  “rename”和  “tracker_dir”. 本参数只在deletePolicy&#x3D;never时才生效   “rename”- 采完的文件根据filesuffix重命名   “tracker_dir” - 采完的文件会在trackerDir目录中生成一个同名的空文件</td>
</tr>
<tr>
<td>consumeOrder</td>
<td>oldest</td>
<td>采集顺序： oldest, youngest and random.   oldest和youngest情况下，可能会带来一定效率的损失；（需要对文件夹中所有文件进行一次扫描以寻找最old或最young的）</td>
</tr>
<tr>
<td>pollDelay</td>
<td>500</td>
<td>Delay (in milliseconds) used when polling for new  files.</td>
</tr>
<tr>
<td>recursiveDirectorySearch</td>
<td>false</td>
<td>Whether to monitor sub directories for new files to read.</td>
</tr>
<tr>
<td>maxBackoff</td>
<td>4000</td>
<td>The maximum time (in millis) to wait between  consecutive attempts to write to the channel(s) if the channel is full. The  source will start at a low backoff and increase it exponentially each time  the channel throws a ChannelException, upto the value specified by this  parameter.</td>
</tr>
<tr>
<td>batchSize</td>
<td>100</td>
<td>一次传输到channel的event条数（一批）</td>
</tr>
<tr>
<td>inputCharset</td>
<td>UTF-8</td>
<td>Character set used by deserializers that treat the  input file as text.</td>
</tr>
<tr>
<td>decodeErrorPolicy</td>
<td>FAIL</td>
<td>What to do when we see a non-decodable character in the  input file. FAIL: Throw an exception and fail to parse the  file. REPLACE: Replace the unparseable character with the  “replacement character” char, typically Unicode U+FFFD. IGNORE: Drop the unparseable character sequence.</td>
</tr>
<tr>
<td>deserializer</td>
<td>LINE</td>
<td>Specify the deserializer used to parse the file into  events. Defaults to parsing each line as an event. The class specified must  implementEventDeserializer.Builder.</td>
</tr>
<tr>
<td>deserializer.*</td>
<td></td>
<td>Varies per event deserializer.</td>
</tr>
<tr>
<td>bufferMaxLines</td>
<td>–</td>
<td>(Obselete) This option is now ignored.</td>
</tr>
<tr>
<td>bufferMaxLineLength</td>
<td>5000</td>
<td>(Deprecated) Maximum length of a line in the commit  buffer. Use deserializer.maxLineLength instead.</td>
</tr>
<tr>
<td>selector.type</td>
<td>replicating</td>
<td>replicating or multiplexing</td>
</tr>
<tr>
<td>selector.*</td>
<td></td>
<td>Depends on the selector.type value</td>
</tr>
<tr>
<td>interceptors</td>
<td>–</td>
<td>Space-separated list of interceptors</td>
</tr>
<tr>
<td>interceptors.*</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2 id="配置文件：-1"><a href="#配置文件：-1" class="headerlink" title="配置文件："></a><strong>配置文件：</strong></h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example3-spooldir-source.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">spooldir</span></span><br><span class="line"><span class="attr">a1.sources.r1.spoolDir</span> = <span class="string">/export/data/flume-example-data/weblog </span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>

<h2 id="启动测试：-1"><a href="#启动测试：-1" class="headerlink" title="启动测试："></a><strong>启动测试：</strong></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c conf -f myconf/example3-spooldir-source.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>注意：spooldir source 与exec source不同，spooldir source本身是可靠的，会记录崩溃之前的采集位置。</p>
<h1 id="四、taildir-source测试"><a href="#四、taildir-source测试" class="headerlink" title="四、taildir_source测试"></a>四、taildir_source测试</h1><h2 id="工作机制：-2"><a href="#工作机制：-2" class="headerlink" title="工作机制："></a><strong>工作机制：</strong></h2><p><code>监视指定目录下的一批文件</code>，只要某个文件中<code>有新写入的行</code>，则会被<code>tail</code>到<br>它会<code>记录每一个文件所tail到的位置</code>，记录到一个指定的<code>positionfile</code>保存目录中，<code>格式为json</code>（如果需要的时候，可以人为修改，就可以让source从任意指定的位置开始读取数据）<br>所以，这个source真的像官网所吹的，是可靠的reliable！<br>它对采集完成的文件，不会做任何修改（比如重命名，删除…..）</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230114093012740.png" alt="image-20230114093012740"></p>
<p>taildir source会把读到的数据<code>成功写入channel后</code>，<code>再更新记录偏移量</code>，这种机制，能保证数据不会漏采（丢失），但是有<code>可能会产生数据重复</code>。</p>
<h2 id="参数详解：-2"><a href="#参数详解：-2" class="headerlink" title="参数详解："></a><strong>参数详解：</strong></h2><table>
<thead>
<tr>
<th><strong>Property Name</strong></th>
<th><strong>Default</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>channels</strong></td>
<td>–</td>
<td>所要写往的channel</td>
</tr>
<tr>
<td><strong>type</strong></td>
<td>–</td>
<td>本source的别名： TAILDIR.</td>
</tr>
<tr>
<td><strong>filegroups</strong></td>
<td>–</td>
<td>空格分割的组名，每一组代表着一批文件  g1 g2</td>
</tr>
<tr>
<td><strong>filegroups.<filegroupName></strong></td>
<td>–</td>
<td>每个文件组的绝路路径，文件名可用正则表达式</td>
</tr>
<tr>
<td>positionFile</td>
<td>~&#x2F;.flume&#x2F;taildir_position.json</td>
<td>记录偏移量位置的文件所在路径</td>
</tr>
<tr>
<td>headers.<filegroupName>.<headerKey></td>
<td>–</td>
<td>Header value which is  the set with header key. Multiple headers can be specified for one file  group.</td>
</tr>
<tr>
<td>byteOffsetHeader</td>
<td>false</td>
<td>Whether to add the byte  offset of a tailed line to a header called ‘byteoffset’.</td>
</tr>
<tr>
<td>skipToEnd</td>
<td>false</td>
<td>Whether to skip the  position to EOF in the case of files not written on the position file.</td>
</tr>
<tr>
<td>idleTimeout</td>
<td>120000</td>
<td>关闭非活动文件的时延。如果被关闭的这个文件又在某个时间有了新增行,会被此source检测到，并重新打开</td>
</tr>
<tr>
<td>writePosInterval</td>
<td>3000</td>
<td>3s 记录一次偏移量到positionfile</td>
</tr>
<tr>
<td>batchSize</td>
<td>100</td>
<td>提交event到channel的批次最大条数</td>
</tr>
<tr>
<td>maxBatchCount</td>
<td>Long.MAX_VALUE</td>
<td>控制在一个文件上连续读取的最大批次个数（如果某个文件正在被高速写入，那就应该让这个参数调为最大值，以让source可以集中精力专采这个文件）</td>
</tr>
<tr>
<td>backoffSleepIncrement</td>
<td>1000</td>
<td>The increment for time  delay before reattempting to poll for new data, when the last attempt did not  find any new data.</td>
</tr>
<tr>
<td>maxBackoffSleep</td>
<td>5000</td>
<td>The max time delay  between each reattempt to poll for new data, when the last attempt did not  find any new data.</td>
</tr>
<tr>
<td>cachePatternMatching</td>
<td>true</td>
<td>Listing directories and  applying the filename regex pattern may be time consuming for directories  containing thousands of files. Caching the list of matching files can improve  performance. The order in which files are consumed will also be cached.  Requires that the file system keeps track of modification times with at least  a 1-second granularity.</td>
</tr>
<tr>
<td>fileHeader</td>
<td>false</td>
<td>Whether to add a header  storing the absolute path filename.</td>
</tr>
<tr>
<td>fileHeaderKey</td>
<td>file</td>
<td>Header key to use when  appending absolute path filename to event header.</td>
</tr>
</tbody></table>
<h2 id="配置文件：-2"><a href="#配置文件：-2" class="headerlink" title="配置文件："></a><strong>配置文件：</strong></h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example4-taildir-source.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">TAILDIR</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.positionFile</span> = <span class="string">/export/data/flume-example-data/flumedata/taildir_position.json</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups</span> = <span class="string">g1 g2</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.g1</span> = <span class="string">/export/data/flume-example-data/weblog/web.*</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.g2</span> = <span class="string">/export/data/flume-example-data/wxlog/wx.*</span></span><br><span class="line"><span class="attr">a1.sources.r1.fileHeader</span> = <span class="string">true</span></span><br><span class="line"><span class="comment">#动态的header-keys eg：filepath=/../../../</span></span><br><span class="line"><span class="attr">a1.sources.r1.fileHeaderKey</span> = <span class="string">filepath</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#写死的header-keys（静态的） eg:a1 = aa1</span></span><br><span class="line"><span class="attr">a1.sources.r1.headers.g1.a1</span> = <span class="string">aa1</span></span><br><span class="line"><span class="attr">a1.sources.r1.headers.g1.b1</span> = <span class="string">bb1</span></span><br><span class="line"><span class="attr">a1.sources.r1.headers.g2.a2</span> = <span class="string">aa2</span></span><br><span class="line"><span class="attr">a1.sources.r1.headers.g2.b2</span> = <span class="string">bb2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.maxBatchCount</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">10000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="启动测试：-2"><a href="#启动测试：-2" class="headerlink" title="启动测试："></a><strong>启动测试：</strong></h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c conf/ -f  myconf/example4-taildir-source.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<p>经过人为破坏测试，发现， this source还是真正挺reliable的，不会丢失数据，但在极端情况下可能会产生重复数据。</p>
<h1 id="五、avro-source"><a href="#五、avro-source" class="headerlink" title="五、avro source"></a>五、avro source</h1><h2 id="基本介绍："><a href="#基本介绍：" class="headerlink" title="基本介绍："></a><strong>基本介绍：</strong></h2><p>Avro source 是通过<code>监听一个网络端口来接收数据</code>，而且接受的数据必须是使用<code>avro序列化框架</code>序列化后的数据（必须是：avro序列化流）；Avro是一种序列化框架，<code>跨语言的</code>；</p>
<blockquote>
<p>扩展：什么是序列化，什么是序列化框架？</p>
<p>序列化： <code>是将一个有复杂结构的数据块（对象）变成扁平的（线性的）二进制序列</code></p>
<p>序列化框架： 一套现成的软件，可以按照既定策略，将对象转成二进制序列</p>
<p>比如： jdk就有： ObjectOutputStream</p>
<p>​       	 hadoop就有： Writable</p>
<p>​    		跨平台的序列化框架： avro</p>
</blockquote>
<h2 id="工作机制：-3"><a href="#工作机制：-3" class="headerlink" title="工作机制："></a><strong>工作机制：</strong></h2><p><code>启动一个网络服务，监听一个端口，收集端口上收到的avro序列化数据流</code></p>
<p>该source中拥有avro的反序列化器，能够将收到的二进制流进行正确反序列化，并装入一个event写入channel！</p>
<h2 id="参数详解：-3"><a href="#参数详解：-3" class="headerlink" title="参数详解："></a><strong>参数详解：</strong></h2><table>
<thead>
<tr>
<th><strong>Property Name</strong></th>
<th><strong>Default</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>channels</strong></td>
<td>–</td>
<td></td>
</tr>
<tr>
<td><strong>type</strong></td>
<td>–</td>
<td>本source的别名： avro</td>
</tr>
<tr>
<td><strong>bind</strong></td>
<td>–</td>
<td>要绑定的地址</td>
</tr>
<tr>
<td><strong>port</strong></td>
<td>–</td>
<td>要绑定的端口号</td>
</tr>
<tr>
<td><strong>threads</strong></td>
<td>–</td>
<td><strong>服务的最大线程数（最好大于<strong><strong>source</strong></strong>所对接的上游发送者数量）</strong></td>
</tr>
<tr>
<td>selector.type</td>
<td></td>
<td></td>
</tr>
<tr>
<td>selector.*</td>
<td></td>
<td></td>
</tr>
<tr>
<td>interceptors</td>
<td>–</td>
<td>Space-separated list of interceptors</td>
</tr>
<tr>
<td>interceptors.*</td>
<td></td>
<td></td>
</tr>
<tr>
<td>compression-type</td>
<td>none</td>
<td>压缩类型：跟发过来的数据是否压缩要匹配：none | deflate</td>
</tr>
<tr>
<td>ssl</td>
<td>false</td>
<td>Set this to true to enable SSL encryption. If SSL is enabled,  you must also specify a “keystore” and a “keystore-password”, either through  component level parameters (see below) or as global SSL parameters (see <a href="file:///D:/install-pkgs/apache-flume-1.9.0-bin/docs/FlumeUserGuide.html#ssl-tls-support">SSL&#x2F;TLS support</a> section).</td>
</tr>
<tr>
<td>keystore</td>
<td>–</td>
<td>This is the path to a Java keystore file. If not specified here,  then the global keystore will be used (if defined, otherwise configuration  error).</td>
</tr>
<tr>
<td>keystore-password</td>
<td>–</td>
<td>The password for the Java keystore. If not specified here, then  the global keystore password will be used (if defined, otherwise  configuration error).</td>
</tr>
<tr>
<td>keystore-type</td>
<td>JKS</td>
<td>The type of the Java keystore. This can be “JKS” or “PKCS12”. If  not specified here, then the global keystore type will be used (if defined,  otherwise the default is JKS).</td>
</tr>
<tr>
<td>exclude-protocols</td>
<td>SSLv3</td>
<td>Space-separated list of SSL&#x2F;TLS protocols to exclude. SSLv3 will  always be excluded in addition to the protocols specified.</td>
</tr>
<tr>
<td>include-protocols</td>
<td>–</td>
<td>Space-separated list of SSL&#x2F;TLS protocols to include. The  enabled protocols will be the included protocols without the excluded  protocols. If included-protocols is empty, it includes every supported  protocols.</td>
</tr>
<tr>
<td>exclude-cipher-suites</td>
<td>–</td>
<td>Space-separated list of cipher suites to exclude.</td>
</tr>
<tr>
<td>include-cipher-suites</td>
<td>–</td>
<td>Space-separated list of cipher suites to include. The enabled  cipher suites will be the included cipher suites without the excluded cipher  suites. If included-cipher-suites is empty, it includes every supported  cipher suites.</td>
</tr>
<tr>
<td>ipFilter</td>
<td>false</td>
<td>Set this to true to enable ipFiltering for netty</td>
</tr>
<tr>
<td>ipFilterRules</td>
<td>–</td>
<td>Define N netty ipFilter pattern rules with this config.</td>
</tr>
</tbody></table>
<h2 id="配置文件：-3"><a href="#配置文件：-3" class="headerlink" title="配置文件："></a><strong>配置文件：</strong></h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example5-avro-source.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="string">4141</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">200</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>

<h2 id="启动测试：-3"><a href="#启动测试：-3" class="headerlink" title="启动测试："></a><strong>启动测试：</strong></h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">启动agent：</span><br><span class="line"></span><br><span class="line">bin/flume-ng agent -c conf -f  myconf/example5-avro-source.conf -n a1 -Dflume.root.logger=INFO,console  </span><br><span class="line"></span><br><span class="line">新建avro-log.txt，用一个客户端去给启动好的source发送avro序列化数据：</span><br><span class="line"></span><br><span class="line">bin/flume-ng avro-client --host node1  --port 4141  -F /export/data/flume-example-data/avro-log.txt</span><br></pre></td></tr></table></figure>



<h1 id="六、使用File-Channel实现数据持久化"><a href="#六、使用File-Channel实现数据持久化" class="headerlink" title="六、使用File Channel实现数据持久化"></a>六、使用File Channel实现数据持久化</h1><p>使用组件</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#file-channel">File Channel</a></li>
</ul>
<p>属性设置</p>
<table>
<thead>
<tr>
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>type</strong></td>
<td align="left">–</td>
<td align="left"><code>file</code></td>
</tr>
<tr>
<td align="left">checkpointDir</td>
<td align="left">~&#x2F;.flume&#x2F;file-channel&#x2F;checkpoint</td>
<td align="left">检查点文件存放路径</td>
</tr>
<tr>
<td align="left">dataDirs</td>
<td align="left">~&#x2F;.flume&#x2F;file-channel&#x2F;data</td>
<td align="left">日志存储路径，多个路径使用逗号分隔. 使用不同的磁盘上的多个路径能提高file channel的性能</td>
</tr>
</tbody></table>
<p>添加配置文件file-channel.conf，添加一个FileChannel</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example6-file-channel.conf</span></span><br><span class="line"><span class="comment"># 定义agent名称为a1</span></span><br><span class="line"><span class="comment"># 设置3个组件的名称</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="comment"># 多个channel使用空格分隔</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1 c2</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置source类型为NetCat,监听地址为本机，端口为44444</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">netcat</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置sink类型为Logger</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置FileChannel,checkpointDir为检查点文件存储目录，dataDirs为日志数据存储目录，</span></span><br><span class="line"><span class="attr">a1.channels.c2.type</span> = <span class="string">file</span></span><br><span class="line"><span class="attr">a1.channels.c2.checkpointDir</span> = <span class="string">/export/data/flume-example-data/flumedata/checkpoint_filechannel</span></span><br><span class="line"><span class="attr">a1.channels.c2.dataDirs</span> = <span class="string">/export/data/flume-example-data/flumedata/data_filechannel</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 将source和sink绑定到channel上</span></span><br><span class="line"><span class="comment"># source同时绑定到c1和c2上</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1 c2</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>

<p>为了方便日志打印，可以将<code>-Dflume.root.logger=INFO,console</code>添加在conf的环境配置中，从模板复制一份配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp flume-env.sh.template flume-env.sh</span><br><span class="line">vi flume-env.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加JAVA_OPTS</span></span><br><span class="line">export JAVA_OPTS=&quot;-Dflume.root.logger=INFO,console&quot;</span><br></pre></td></tr></table></figure>

<p>启动Flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c conf -f myconf/example6-file-channel.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>



<p>通过Netcat发送数据，此时发送到c2的数据没有被消费，关闭Flume，修改配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将sink绑定到c2上</span></span><br><span class="line">a1.sinks.k1.channel = c2</span><br></pre></td></tr></table></figure>

<p>重启Flume，可以看到会重新消费c2的数据</p>
<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200323164045854.png" alt="image-20200323164045854"></p>
<h1 id="七、利用avro-source和avro-sink实现agent级联"><a href="#七、利用avro-source和avro-sink实现agent级联" class="headerlink" title="七、利用avro source和avro sink实现agent级联"></a>七、利用avro source和avro sink实现agent级联</h1><h2 id="基本介绍：-1"><a href="#基本介绍：-1" class="headerlink" title="基本介绍："></a><strong>基本介绍：</strong></h2><p>级联的场景属于跨网络中转传输</p>
<h2 id="多个agent模型"><a href="#多个agent模型" class="headerlink" title="多个agent模型"></a>多个agent模型</h2><p>可以将多个Flume agent 程序连接在一起，其中一个agent的sink将数据发送到另一个agent的source。Avro文件格式是使用Flume通过网络发送数据的标准方法。</p>
<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200324142535996.png" alt="image-20200324142535996"></p>
<p>从多个Web服务器收集日志，发送到一个或多个集中处理的agent，之后再发往日志存储中心：</p>
<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200324151344310.png" alt="image-20200324151344310"></p>
<p>同样的日志发送到不同的目的地：</p>
<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200324151408108.png" alt="image-20200324151408108"></p>
<h2 id="需求说明："><a href="#需求说明：" class="headerlink" title="需求说明："></a><strong>需求说明：</strong></h2><h3 id="（1）机房跨网段flume中转传输网络示意图："><a href="#（1）机房跨网段flume中转传输网络示意图：" class="headerlink" title="（1）机房跨网段flume中转传输网络示意图："></a>（1）机房跨网段flume中转传输网络示意图：</h3><p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5C%E6%9C%BA%E6%88%BF%E8%B7%A8%E7%BD%91%E6%AE%B5flume%E4%B8%AD%E8%BD%AC%E4%BC%A0%E8%BE%93%E7%BD%91%E7%BB%9C%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="机房跨网段flume中转传输网络示意图"></p>
<h3 id="（2）流量汇聚传输网络示意图："><a href="#（2）流量汇聚传输网络示意图：" class="headerlink" title="（2）流量汇聚传输网络示意图："></a>（2）流量汇聚传输网络示意图：</h3><p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5C%E6%B5%81%E9%87%8F%E6%B1%87%E8%81%9A%E4%BC%A0%E8%BE%93%E7%BD%91%E7%BB%9C%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="流量汇聚传输网络示意图"></p>
<h3 id="（3）app上报日志及flume采集全流程示意图-2023-x2F-1-x2F-17-15-30-19"><a href="#（3）app上报日志及flume采集全流程示意图-2023-x2F-1-x2F-17-15-30-19" class="headerlink" title="（3）app上报日志及flume采集全流程示意图 2023&#x2F;1&#x2F;17 15:30:19"></a>（3）app上报日志及flume采集全流程示意图 2023&#x2F;1&#x2F;17 15:30:19</h3><p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Capp%E4%B8%8A%E6%8A%A5%E6%97%A5%E5%BF%97%E5%8F%8Aflume%E9%87%87%E9%9B%86%E5%85%A8%E6%B5%81%E7%A8%8B%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="app上报日志及flume采集全流程示意图"></p>
<h2 id="配置文件：-4"><a href="#配置文件：-4" class="headerlink" title="配置文件："></a><strong>配置文件：</strong></h2><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#上游服务器配置 example7-1-taildir-f-avro.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">TAILDIR</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.positionFile</span> = <span class="string">/export/data/flume-example-data/flumedata/taildir_position.json</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups</span> = <span class="string">g1 g2</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.g1</span> = <span class="string">/export/data/flume-example-data/weblog/web.*</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.g2</span> = <span class="string">/export/data/flume-example-data/wxlog/wx.*</span></span><br><span class="line"><span class="comment">#提高吞吐量</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="comment">#动态的header-keys eg：filepath=/../../../</span></span><br><span class="line"><span class="attr">a1.sources.r1.fileHeaderKey</span> = <span class="string">filepath</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#拦截器配置，添加header=timestamp</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors</span> = <span class="string">i1</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i1.type</span> = <span class="string">timestamp</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i1.headerName</span> = <span class="string">timestamp</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">file</span></span><br><span class="line"><span class="comment">#本机数据汇集检查点、event存储目录</span></span><br><span class="line"><span class="attr">a1.channels.c1.checkpointDir</span> = <span class="string">/export/data/flume-example-data/flumedata/checkpoint</span></span><br><span class="line"><span class="attr">a1.channels.c1.dataDirs</span> = <span class="string">/export/data/flume-example-data/flumedata/data</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">2000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sinks.k1.batch-size</span> = <span class="string">1000</span></span><br><span class="line"><span class="comment">#下游目标主机、端口</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = <span class="string">node3</span></span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="string">44444</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下游服务器配置 example7-2-avro-f-hdfs.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#下游数据汇集avro source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="attr">a1.sources.r1.threads</span> = <span class="string">10</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">file</span></span><br><span class="line"><span class="attr">a1.channels.c1.checkpointDir</span> = <span class="string">/export/data/flume-example-data/flumedata/checkpoint</span></span><br><span class="line"><span class="attr">a1.channels.c1.dataDirs</span> = <span class="string">/export/data/flume-example-data/flumedata/data</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">2000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#hdfs sink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.path</span> = <span class="string">hdfs://node1:8020/logdata/%Y-%m-%d/%H/</span></span><br><span class="line"><span class="comment">#eg：文件名 logdata_34438hxfd.log，在滚动时，logdata_34438hxfd.log.tmp</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.filePrefix</span> = <span class="string">logdata_</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileSuffix</span> = <span class="string">.log</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#三个条件没有优先级，谁先达到就进行滚动</span></span><br><span class="line"><span class="comment">#按时间间隔滚动</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollInterval</span> = <span class="string">0</span></span><br><span class="line"><span class="comment">#按文件大小滚动 256MB</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollSize</span> = <span class="string">268435456</span></span><br><span class="line"><span class="comment">#按event条数滚动</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollCount</span> = <span class="string">100000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.codeC</span> = <span class="string">gzip</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileType</span> = <span class="string">CompressedStream</span></span><br></pre></td></tr></table></figure>

<h2 id="级联案例操作手册"><a href="#级联案例操作手册" class="headerlink" title="级联案例操作手册"></a>级联案例操作手册</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">级联案例操作手册</span><br><span class="line"></span><br><span class="line">-- 1. 启动hdfs，并检查工作状态</span><br><span class="line">-- 2. 把案例的上游配置文件，保存到node1,node2上</span><br><span class="line">-- 3. 把案例的下游配置文件，保存到node3上</span><br><span class="line"></span><br><span class="line">-- 4. 启动下游node3上的flume agent</span><br><span class="line">bin/flume-ng agent -n a1 -c conf/ -f myconf/example7-2-avro-f-hdfs.conf -Dflume.root.logger=DEBUG,console</span><br><span class="line"></span><br><span class="line">-- 5. （在node1和node2上）准备两个日志目录来生成模拟日志数据</span><br><span class="line">mkdir /export/data/flume-example-data/weblog/</span><br><span class="line">mkdir /export/data/flume-example-data/wxlog/</span><br><span class="line"></span><br><span class="line">-- 6.（在node1和node2上）利用shell脚本生成日志数据  </span><br><span class="line">vim avro-hdfs.sh</span><br><span class="line"></span><br><span class="line">while true</span><br><span class="line">do</span><br><span class="line">echo webwebwebwebweb &gt;&gt; /export/data/flume-example-data/weblog/web-access.log</span><br><span class="line">echo wxwxwxwxwxwxwx  &gt;&gt; /export/data/flume-example-data/wxlog/wx-access.log </span><br><span class="line">sleep 0.01</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">-- 7.启动node1和node2上的flume agent</span><br><span class="line">nohup bin/flume-ng agent -n a1 -c conf/ -f myconf/example7-1-taildir-f-avro.conf 1&gt;/dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>



<h1 id="八、拦截器"><a href="#八、拦截器" class="headerlink" title="八、拦截器"></a>八、拦截器</h1><p>拦截器可以修改或者丢弃事件，Flume支持链式调用拦截器，拦截器定义在source中</p>
<h2 id="（1）Host-Interceptor"><a href="#（1）Host-Interceptor" class="headerlink" title="（1）Host Interceptor"></a>（1）Host Interceptor</h2><p>这个拦截器将运行agent的hostname 或者 IP地址写入到事件的headers中</p>
<table>
<thead>
<tr>
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>type</strong></td>
<td align="left">–</td>
<td align="left"><code>host</code></td>
</tr>
<tr>
<td align="left">preserveExisting</td>
<td align="left">false</td>
<td align="left">如果header已经存在host, 是否要保留 - true保留原始的，false写入当前机器</td>
</tr>
<tr>
<td align="left">useIP</td>
<td align="left">true</td>
<td align="left">true为IP地址, false为 hostname.</td>
</tr>
<tr>
<td align="left">hostHeader</td>
<td align="left">host</td>
<td align="left">header中key的名称</td>
</tr>
</tbody></table>
<p>在myconf中添加example8-interceptor.conf</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example8-interceptor.conf</span></span><br><span class="line"><span class="comment"># 定义agent名称为a1</span></span><br><span class="line"><span class="comment"># 设置3个组件的名称</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置source类型为NetCat,监听地址为本机，端口为44444</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">netcat</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="comment"># 配置拦截器为host</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors</span> = <span class="string">i1 </span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i1.type</span> = <span class="string">host</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置sink类型为Logger</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 将source和sink绑定到channel上</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>

<p>启动flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c conf -f myconf/example8-interceptor.conf -Dflume.root.logger=INFO,console</span><br></pre></td></tr></table></figure>

<h2 id="（2）Timestamp-Interceptor"><a href="#（2）Timestamp-Interceptor" class="headerlink" title="（2）Timestamp Interceptor"></a>（2）Timestamp Interceptor</h2><p>这个拦截器将当前时间写入到事件的headers中</p>
<table>
<thead>
<tr>
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>type</strong></td>
<td align="left">–</td>
<td align="left"><code>timestamp</code></td>
</tr>
<tr>
<td align="left">headerName</td>
<td align="left">timestamp</td>
<td align="left">header中key的名称</td>
</tr>
<tr>
<td align="left">preserveExisting</td>
<td align="left">false</td>
<td align="left">If the timestamp already exists, should it be preserved - true or false</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.r1.interceptors = i1 i2</span><br><span class="line">a1.sources.r1.interceptors.i1.type = host</span><br><span class="line">a1.sources.r1.interceptors.i2.type = timestamp</span><br></pre></td></tr></table></figure>

<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200113212610717.png" alt="image-20200113212610717"></p>
<h2 id="（3）Static-Interceptor"><a href="#（3）Static-Interceptor" class="headerlink" title="（3）Static Interceptor"></a>（3）Static Interceptor</h2><p>运行用户对所有的事件添加固定的header</p>
<table>
<thead>
<tr>
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>type</strong></td>
<td align="left">–</td>
<td align="left"><code>static</code></td>
</tr>
<tr>
<td align="left">preserveExisting</td>
<td align="left">true</td>
<td align="left">If configured header already exists, should it be preserved - true or false</td>
</tr>
<tr>
<td align="left">key</td>
<td align="left">key</td>
<td align="left">header 中key名称</td>
</tr>
<tr>
<td align="left">value</td>
<td align="left">value</td>
<td align="left">header 中value值</td>
</tr>
</tbody></table>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.r1.interceptors = i1 i2 i3</span><br><span class="line">a1.sources.r1.interceptors.i1.type = host</span><br><span class="line">a1.sources.r1.interceptors.i2.type = timestamp</span><br><span class="line">a1.sources.r1.interceptors.i3.type = static</span><br><span class="line">a1.sources.r1.interceptors.i3.key = datacenter</span><br><span class="line">a1.sources.r1.interceptors.i3.value = NEW_YORK</span><br></pre></td></tr></table></figure>

<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200113212850218.png" alt="image-20200113212850218"></p>
<h2 id="（4）UUID-Interceptor"><a href="#（4）UUID-Interceptor" class="headerlink" title="（4）UUID Interceptor"></a>（4）UUID Interceptor</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.r1.interceptors = i1 i2 i3 i4</span><br><span class="line">a1.sources.r1.interceptors.i1.type = host</span><br><span class="line">a1.sources.r1.interceptors.i2.type = timestamp</span><br><span class="line">a1.sources.r1.interceptors.i3.type = static</span><br><span class="line">a1.sources.r1.interceptors.i3.key = datacenter</span><br><span class="line">a1.sources.r1.interceptors.i3.value = NEW_YORK</span><br><span class="line">a1.sources.r1.interceptors.i4.type = org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder</span><br></pre></td></tr></table></figure>

<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200113213451217.png" alt="image-20200113213451217"></p>
<h2 id="（5）Search-and-Replace-Interceptor"><a href="#（5）Search-and-Replace-Interceptor" class="headerlink" title="（5）Search and Replace Interceptor"></a>（5）Search and Replace Interceptor</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a1.sources.r1.interceptors = i1 i2 i3 i4 i5</span><br><span class="line">a1.sources.r1.interceptors.i1.type = host</span><br><span class="line">a1.sources.r1.interceptors.i2.type = timestamp</span><br><span class="line">a1.sources.r1.interceptors.i3.type = static</span><br><span class="line">a1.sources.r1.interceptors.i3.key = datacenter</span><br><span class="line">a1.sources.r1.interceptors.i3.value = NEW_YORK</span><br><span class="line">a1.sources.r1.interceptors.i4.type = org.apache.flume.sink.solr.morphline.UUIDInterceptor$Builder</span><br><span class="line">a1.sources.r1.interceptors.i5.type = search_replace</span><br><span class="line">a1.sources.r1.interceptors.i5.searchPattern = \\d&#123;6&#125;</span><br><span class="line">a1.sources.r1.interceptors.i5.replaceString = ******1234</span><br></pre></td></tr></table></figure>

<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200113220257571.png" alt="image-20200113220257571"></p>
<h2 id="（6）自定义拦截器"><a href="#（6）自定义拦截器" class="headerlink" title="（6）自定义拦截器"></a>（6）自定义拦截器</h2><p>新建工程，添加pom引用</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flume<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flume-ng-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.9.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>添加自定义拦截器</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.google.gson.Gson;</span><br><span class="line"><span class="keyword">import</span> com.google.gson.reflect.TypeToken;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.codec.digest.DigestUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.interceptor.Interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">EventTimestampInterceptor</span> <span class="keyword">implements</span> <span class="title class_">Interceptor</span> &#123;</span><br><span class="line"></span><br><span class="line">    String timeStampFiledName;</span><br><span class="line">    String toEncryFieldName;</span><br><span class="line">    String keyName;</span><br><span class="line">    Gson gson;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">EventTimestampInterceptor</span><span class="params">(String timeStampFiledName,String toEncryFieldName,String keyName)</span>&#123;</span><br><span class="line">        <span class="built_in">this</span>.timeStampFiledName = timeStampFiledName;</span><br><span class="line">        <span class="built_in">this</span>.toEncryFieldName = toEncryFieldName;</span><br><span class="line">        <span class="built_in">this</span>.keyName = keyName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 初始化工作所在的方法</span></span><br><span class="line"><span class="comment">     * 在拦截操作之前，会被调用一次</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">initialize</span><span class="params">()</span> &#123;</span><br><span class="line">        gson = <span class="keyword">new</span> <span class="title class_">Gson</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 拦截操作的具体逻辑所在方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> event</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> Event <span class="title function_">intercept</span><span class="params">(Event event)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">byte</span>[] body = event.getBody();</span><br><span class="line">        <span class="type">String</span> <span class="variable">lineJson</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(body);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取时间戳，放入header</span></span><br><span class="line">        Map&lt;String,Object&gt; map = gson.fromJson(lineJson, <span class="keyword">new</span> <span class="title class_">TypeToken</span>&lt;HashMap&lt;String,Object&gt;&gt;()&#123;&#125;.getType());</span><br><span class="line">        <span class="type">Double</span> <span class="variable">ts</span> <span class="operator">=</span> (Double)map.get(timeStampFiledName);</span><br><span class="line">        event.getHeaders().put(keyName,ts.longValue()+<span class="string">&quot;&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将账号字段进行加密</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">s</span> <span class="operator">=</span> (String) map.get(toEncryFieldName);</span><br><span class="line">        <span class="type">String</span> <span class="variable">s1</span> <span class="operator">=</span> DigestUtils.md5Hex(s);</span><br><span class="line">        map.put(toEncryFieldName,s1);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 将加密处理后的日志内容（map中），重新恢复成json，并set到event的body中</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">newJson</span> <span class="operator">=</span> gson.toJson(map);</span><br><span class="line">        event.setBody(newJson.getBytes());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> event;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> List&lt;Event&gt; <span class="title function_">intercept</span><span class="params">(List&lt;Event&gt; list)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (Event event : list) &#123;</span><br><span class="line">            intercept(event);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> list;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * agent关闭时，会调用该方法来做一些清理工作</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">EventTimestampInterceptorBuilder</span> <span class="keyword">implements</span> <span class="title class_">Builder</span>&#123;</span><br><span class="line"></span><br><span class="line">        String timeStampFiledName;</span><br><span class="line">        String toEncryFieldName;</span><br><span class="line">        String keyName;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 构造拦截器实例对象的方法</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">public</span> Interceptor <span class="title function_">build</span><span class="params">()</span> &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">EventTimestampInterceptor</span>(timeStampFiledName,toEncryFieldName,keyName);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * 从配置文件中获取配置参数的方法</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line">            timeStampFiledName = context.getString(<span class="string">&quot;tsFiledName&quot;</span>);</span><br><span class="line">            toEncryFieldName = context.getString(<span class="string">&quot;toEncryFieldName&quot;</span>);</span><br><span class="line">            keyName = context.getString(<span class="string">&quot;keyName&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>将项目打成jar包后复制到Flume安装目录的lib目录中</p>
<p>修改上游服务器配置 taildir-f-avro.conf为taildir-f-avro-interceptor.conf</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#上游服务器配置example9-1-taildir-f-avro-interceptor.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">TAILDIR</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.positionFile</span> = <span class="string">/export/data/flume-example-data/flumedata/taildir_position.json</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups</span> = <span class="string">g1</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.g1</span> = <span class="string">/export/data/flume-example-data/app/event.*</span></span><br><span class="line"><span class="comment">#提高吞吐量</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="comment">#动态的header-keys eg：filepath=/../../../</span></span><br><span class="line"><span class="attr">a1.sources.r1.fileHeaderKey</span> = <span class="string">filepath</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#拦截器配置,添加自定义拦截器</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors</span> = <span class="string">i1</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i1.type</span> = <span class="string">ccjz.rgzn.flume.EventTimestampInterceptor$EventTimestampInterceptorBuilder</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i1.tsFiledName</span> = <span class="string">timeStamp</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i1.keyName</span> = <span class="string">timestamp</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i1.toEncryFieldName</span> = <span class="string">account</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">file</span></span><br><span class="line"><span class="comment">#本机数据汇集检查点、event存储目录</span></span><br><span class="line"><span class="attr">a1.channels.c1.checkpointDir</span> = <span class="string">/export/data/flume-example-data/flumedata/checkpoint</span></span><br><span class="line"><span class="attr">a1.channels.c1.dataDirs</span> = <span class="string">/export/data/flume-example-data/flumedata/data</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">2000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sinks.k1.batch-size</span> = <span class="string">1000</span></span><br><span class="line"><span class="comment">#下游目标主机、端口</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = <span class="string">node3</span></span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="string">44444</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#下游服务器配置 example9-2-avro-f-hdfs-interceptor.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#下游数据汇集avro source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="attr">a1.sources.r1.threads</span> = <span class="string">10</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">file</span></span><br><span class="line"><span class="attr">a1.channels.c1.checkpointDir</span> = <span class="string">/export/data/flume-example-data/flumedata/checkpoint</span></span><br><span class="line"><span class="attr">a1.channels.c1.dataDirs</span> = <span class="string">/export/data/flume-example-data/flumedata/data</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">2000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#hdfs sink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.path</span> = <span class="string">hdfs://node1:8020/logdata-interceptor/%Y-%m-%d/%H/</span></span><br><span class="line"><span class="comment">#eg：文件名 logdata_34438hxfd.log，在滚动时，logdata_34438hxfd.log.tmp</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.filePrefix</span> = <span class="string">logdata_</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileSuffix</span> = <span class="string">.log</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment">#三个条件没有优先级，谁先达到就进行滚动</span></span><br><span class="line"><span class="comment">#按时间间隔滚动</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollInterval</span> = <span class="string">0</span></span><br><span class="line"><span class="comment">#按文件大小滚动 256MB</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollSize</span> = <span class="string">268435456</span></span><br><span class="line"><span class="comment">#按event条数滚动</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollCount</span> = <span class="string">100000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.codeC</span> = <span class="string">gzip</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileType</span> = <span class="string">CompressedStream</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">自定义拦截器-级联案例操作手册</span><br><span class="line"></span><br><span class="line">-- 1. 启动hdfs，并检查工作状态</span><br><span class="line">-- 2. 把案例的上游配置文件，保存到node1,node2上,并将自定义的拦截器jar包保存到node1和node2flume的lib中</span><br><span class="line">-- 3. 把案例的下游配置文件，保存到node3上</span><br><span class="line"></span><br><span class="line">-- 4. 启动下游node3上的flume agent</span><br><span class="line">bin/flume-ng agent -n a1 -c conf/ -f myconf/example9-2-avro-f-hdfs-interceptor.conf -Dflume.root.logger=DEBUG,console</span><br><span class="line"></span><br><span class="line">-- 5. （在node1和node2上）上传两个日志目录来生成模拟日志数据</span><br><span class="line">mkdir /export/data/flume-example-data/app/</span><br><span class="line">mkdir /export/data/flume-example-data/app/</span><br><span class="line"></span><br><span class="line">-- 6.（在node1和node2上）修改配置文件，加大flume启动内存(选做)</span><br><span class="line">mv flume-env.sh.template flume-env.sh</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">生产环境建议4G，测试环境以本机内存为参考</span></span><br><span class="line">export JAVA_OPTS=&quot;-Xms100m -Xmx2000m -Dcom.sun.management.jmxremote&quot;</span><br><span class="line"></span><br><span class="line">-- 7.启动node1和node2上的flume agent</span><br><span class="line">nohup bin/flume-ng agent -n a1 -c conf/ -f myconf/example9-1-taildir-f-avro-interceptor.conf 1&gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">--8.去hdfs上查看是否采集到数据</span><br><span class="line">hadoop fs -text /logdata-interceptor/../../.. | tail</span><br></pre></td></tr></table></figure>

<h1 id="九、Flume常用组件详解：channel"><a href="#九、Flume常用组件详解：channel" class="headerlink" title="九、Flume常用组件详解：channel"></a>九、Flume常用组件详解：channel</h1><p>channel是agent中用来缓存event的repository（池，仓库）</p>
<p>source往channel中添加event</p>
<p>sink从channel中取并移除event</p>
<p>channel跟事务控制有极大关系；</p>
<p>channel 有容量大小、可靠性级别、事务容量等特性；</p>
<h2 id="（1）memory-channel"><a href="#（1）memory-channel" class="headerlink" title="（1）memory channel"></a>（1）memory channel</h2><h3 id="特性："><a href="#特性：" class="headerlink" title="特性："></a>特性：</h3><p>事件被存储在实现配置好容量的内存（队列）中。速度快，但可靠性较低，有可能会丢失数据</p>
<h3 id="参数："><a href="#参数：" class="headerlink" title="参数："></a>参数：</h3><table>
<thead>
<tr>
<th><strong>Property Name</strong></th>
<th><strong>Default</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>type</strong></td>
<td>–</td>
<td>别名： memory</td>
</tr>
<tr>
<td>capacity</td>
<td>100</td>
<td>能存储的最大事件event数</td>
</tr>
<tr>
<td>transactionCapacity</td>
<td>100</td>
<td>最大事务控制容量</td>
</tr>
<tr>
<td>keep-alive</td>
<td>3</td>
<td>添加或移除event的超时时间</td>
</tr>
<tr>
<td>byteCapacityBufferPercentage</td>
<td>20</td>
<td>除了body以外的字节所能占用的容量百分比</td>
</tr>
<tr>
<td>byteCapacity</td>
<td>see description</td>
<td>channel中最大的总byte数（只计算body）</td>
</tr>
</tbody></table>
<p>配置示例</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">10000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">10000</span></span><br><span class="line"><span class="attr">a1.channels.c1.byteCapacityBufferPercentage</span> = <span class="string">20</span></span><br><span class="line"><span class="attr">a1.channels.c1.byteCapacity</span> = <span class="string">800000</span></span><br></pre></td></tr></table></figure>

<p>MemoryChannel的逻辑相对简单，主要是通过MemoryTransaction中的putList、takeList与MemoryChannel中的queue打交道，这里的queue相当于持久化层，只不过放到了内存中，如果是FileChannel的话，会把这个queue放到本地文件中。下面表示了Event在一个使用了MemoryChannel的agent中数据流向：</p>
<p><strong>source —&gt; putList —&gt; queue —&gt; takeList —&gt; sink</strong></p>
<p>还需要注意的一点是，这里的事务可以嵌套使用，如下图：</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230121151610824.png" alt="image-20230121151610824"></p>
<p>当有两个agent级连时，sink的事务中包含了一个source的事务，这也应证了前面所说的：</p>
<p><strong>在任何时刻，Event至少在一个Channel中是完整有效的</strong></p>
<h2 id="（2）file-channel"><a href="#（2）file-channel" class="headerlink" title="（2）file channel"></a>（2）file channel</h2><h3 id="特性：-1"><a href="#特性：-1" class="headerlink" title="特性："></a>特性：</h3><p>event被缓存在本地磁盘文件中</p>
<p>可靠性高，不会丢失</p>
<p>但在极端情况下可能会重复数据</p>
<h3 id="参数：-1"><a href="#参数：-1" class="headerlink" title="参数："></a>参数：</h3><table>
<thead>
<tr>
<th><strong>Property Name   Default</strong></th>
<th><strong>Description</strong></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><strong>type</strong></td>
<td>–</td>
<td>别名： file.</td>
</tr>
<tr>
<td>checkpointDir</td>
<td>~&#x2F;.flume&#x2F;file-channel&#x2F;checkpoint</td>
<td>Checkpoint信息保存目录</td>
</tr>
<tr>
<td>useDualCheckpoints</td>
<td>false</td>
<td>Checkpoint是否双重checkpoint机制</td>
</tr>
<tr>
<td>backupCheckpointDir</td>
<td>–</td>
<td>备份checkpoint的保存目录</td>
</tr>
<tr>
<td>dataDirs</td>
<td>~&#x2F;.flume&#x2F;file-channel&#x2F;data</td>
<td>Event数据缓存目录</td>
</tr>
<tr>
<td>transactionCapacity</td>
<td>10000</td>
<td>事务管理容量</td>
</tr>
<tr>
<td>checkpointInterval</td>
<td>30000</td>
<td>记录checkpoint信息的时间间隔</td>
</tr>
<tr>
<td>maxFileSize</td>
<td>2146435071</td>
<td>控制一个数据文件的大小规格</td>
</tr>
<tr>
<td>minimumRequiredSpace</td>
<td>524288000</td>
<td>所需的最低磁盘空间，低于则停止接收新数据</td>
</tr>
<tr>
<td>capacity</td>
<td>1000000</td>
<td>最大event缓存数</td>
</tr>
<tr>
<td>keep-alive</td>
<td>3</td>
<td>等待添加数据的最大时间</td>
</tr>
</tbody></table>
<h3 id="配置示例"><a href="#配置示例" class="headerlink" title="配置示例"></a>配置示例</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">TAILDIR</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.positionFile</span> = <span class="string">/root/taildir_chkp/taildir_position.json</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups</span> = <span class="string">f1</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.f1</span> = <span class="string">/root/weblog/access.log</span></span><br><span class="line"><span class="attr">a1.sources.r1.fileHeader</span> = <span class="string">true</span></span><br><span class="line"><span class="attr">a1.sources.ri.maxBatchCount</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">file</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"><span class="attr">a1.channels.c1.checkpointDir</span> = <span class="string">/root/flume_chkp</span></span><br><span class="line"><span class="attr">a1.channels.c1.dataDirs</span> = <span class="string">/root/flume_data</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c</span></span><br></pre></td></tr></table></figure>

<h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>在使用taildir source 和  file channel的情况下，经过反复各种人为破坏，发现，没有数据丢失的现象发生；</p>
<p>但是，如果时间点掐的比较好(sink 取了一批数据写出，但还没来得及向channel提交事务)，会产生数据重复的现象！</p>
<h1 id="十、Flume常用组件详解：sink"><a href="#十、Flume常用组件详解：sink" class="headerlink" title="十、Flume常用组件详解：sink"></a>十、Flume常用组件详解：sink</h1><p>sink是从channel中获取、移除数据，并输出到下游（可能是下一级agent，也可能是最终目标存储系统）</p>
<h2 id="（1）hdfs-sink"><a href="#（1）hdfs-sink" class="headerlink" title="（1）hdfs sink"></a>（1）hdfs sink</h2><h3 id="特性：-2"><a href="#特性：-2" class="headerlink" title="特性："></a>特性：</h3><p>数据被最终发往hdfs</p>
<p>可以生成text文件或 sequence 文件，而且支持压缩；</p>
<p>支持生成文件的周期性roll机制：基于文件size，或者时间间隔，或者event数量；</p>
<p>目标路径，可以使用动态通配符替换，比如用%D代表当前日期；</p>
<p>当然，它也能从event的header中，取到一些标记来作为通配符替换；</p>
<p>header:{type&#x3D;acb}</p>
<p>&#x2F;weblog&#x2F;%{type}&#x2F;%D&#x2F; 就会被替换成： &#x2F;weblog&#x2F;abc&#x2F;19-06-09&#x2F;</p>
<h3 id="参数：-2"><a href="#参数：-2" class="headerlink" title="参数："></a>参数：</h3><table>
<thead>
<tr>
<th><strong>Name</strong></th>
<th><strong>Default</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>channel</strong></td>
<td>–</td>
<td>从哪个channel取数据</td>
</tr>
<tr>
<td><strong>type</strong></td>
<td>–</td>
<td>别名： hdfs</td>
</tr>
<tr>
<td><strong>hdfs.path</strong></td>
<td>–</td>
<td>目标hdfs存储路径（URI）</td>
</tr>
<tr>
<td>hdfs.filePrefix</td>
<td>FlumeData</td>
<td>指定生成的文件名前缀</td>
</tr>
<tr>
<td>hdfs.fileSuffix</td>
<td>–</td>
<td>后缀</td>
</tr>
<tr>
<td>hdfs.inUsePrefix</td>
<td>–</td>
<td>正在写入的文件的前缀标识</td>
</tr>
<tr>
<td>hdfs.inUseSuffix</td>
<td>.tmp</td>
<td>正在写入的文件的后缀标识</td>
</tr>
<tr>
<td>hdfs.rollInterval</td>
<td>30</td>
<td>切换文件的条件：间隔时间；为0则不生效（秒）</td>
</tr>
<tr>
<td>hdfs.rollSize</td>
<td>134217728</td>
<td>切换文件的条件：文件大小；为0则不生效</td>
</tr>
<tr>
<td>hdfs.rollCount</td>
<td>10</td>
<td>切换文件的条件：event条数；为0则不生效</td>
</tr>
<tr>
<td>hdfs.idleTimeout</td>
<td>0</td>
<td>不活跃文件的关闭超时时长；0则不自动关闭</td>
</tr>
<tr>
<td>hdfs.batchSize</td>
<td>100</td>
<td>从channel中取一批数据的最大大小；</td>
</tr>
<tr>
<td>hdfs.codeC</td>
<td>–</td>
<td>压缩编码: gzip, bzip2, lzo,  lzop, snappy</td>
</tr>
<tr>
<td>hdfs.fileType</td>
<td>SequenceFile</td>
<td>目标文件格式: SequenceFile, DataStream or CompressedStream   注意：DataStream 不能支持压缩  CompressedStream 必须设置压缩编码  SequenceFile 可压缩可不压缩</td>
</tr>
<tr>
<td>hdfs.maxOpenFiles</td>
<td>5000</td>
<td>允许同时最多打开的文件数；如果超出，则会关闭最早打开的</td>
</tr>
<tr>
<td>hdfs.minBlockReplicas</td>
<td>–</td>
<td>目标文件的block副本数</td>
</tr>
<tr>
<td>hdfs.writeFormat</td>
<td>Writable</td>
<td>指定sequence file中的对象类型；支持Text和Writable  同时请使用Text，否则后续数据处理平台可能无法解析</td>
</tr>
<tr>
<td>hdfs.threadsPoolSize</td>
<td>10</td>
<td>操作HDFS时的线程池大小</td>
</tr>
<tr>
<td>hdfs.rollTimerPoolSize</td>
<td>1</td>
<td>检查文件是否需要被roll的线程数</td>
</tr>
<tr>
<td>hdfs.kerberosPrincipal</td>
<td>–</td>
<td>Kerberos user principal for accessing secure HDFS</td>
</tr>
<tr>
<td>hdfs.kerberosKeytab</td>
<td>–</td>
<td>Kerberos keytab for accessing secure HDFS</td>
</tr>
<tr>
<td>hdfs.proxyUser</td>
<td></td>
<td></td>
</tr>
<tr>
<td>hdfs.round</td>
<td>false</td>
<td>目录通配符切换是是否需要切掉尾数</td>
</tr>
<tr>
<td>hdfs.roundValue</td>
<td>10</td>
<td>时间尾数切掉多少</td>
</tr>
<tr>
<td>hdfs.roundUnit</td>
<td>minute</td>
<td>时间尾数切掉大小的单位- second, minute or hour.</td>
</tr>
<tr>
<td>hdfs.timeZone</td>
<td>Local Time</td>
<td>时间通配符所使用的时区</td>
</tr>
<tr>
<td>hdfs.useLocalTimeStamp</td>
<td>false</td>
<td>所用的时间是否要从agent sink本地获取</td>
</tr>
<tr>
<td>hdfs.closeTries</td>
<td>0</td>
<td>重命名已完成文件的重试次数；0则一直尝试重命名</td>
</tr>
<tr>
<td>hdfs.retryInterval</td>
<td>180</td>
<td>关闭一个文件的重试时间间隔</td>
</tr>
<tr>
<td>serializer</td>
<td>TEXT</td>
<td>将channel中的event body解析成什么格式：Text| avro_event ； 也可以使用自定义的序列化器</td>
</tr>
<tr>
<td>serializer.*</td>
<td></td>
<td></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>小  提  示：什么叫做URI</th>
<th><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cclip_image002.jpg" alt="img"></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="配置示例："><a href="#配置示例：" class="headerlink" title="配置示例："></a>配置示例：</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 定义</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">## source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="attr">a1.sources.r1.command</span> = <span class="string">tail -F /root/logs/a.log</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors</span> = <span class="string">i1</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i1.type</span> = <span class="string">timestamp</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">## channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">100000000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">## sink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.path</span> = <span class="string">hdfs://node1:8020/logdata/%&#123;b&#125;/%Y-%m-%d/%H-%M</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.round</span> = <span class="string">true</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.roundValue</span> = <span class="string">10</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.roundUnit</span> = <span class="string">minute</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.filePrefix</span> = <span class="string">doit_</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileSuffix</span> = <span class="string">.log.gz</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollInterval</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollSize</span> = <span class="string">102400</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollCount</span> = <span class="string">0</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileType</span> = <span class="string">CompressedStream</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.codeC</span> = <span class="string">gzip</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.writeFormat</span> = <span class="string">Text</span></span><br></pre></td></tr></table></figure>



<h3 id="测试："><a href="#测试：" class="headerlink" title="测试："></a>测试：</h3><ol>
<li>启动hdfs</li>
<li>清除以前的taildirsource产生的偏移量记录文件、filechannel缓存的数据目录和checkpoint目录</li>
<li>启动agent</li>
<li>用for循环脚本往日志文件中不断写入新的数据</li>
<li>到hdfs中观察结果</li>
</ol>
<h2 id="2-avro-sink"><a href="#2-avro-sink" class="headerlink" title="(2)avro sink"></a>(2)avro sink</h2><h3 id="特性：-3"><a href="#特性：-3" class="headerlink" title="特性："></a>特性：</h3><p>avro sink用来向avro source发送avro序列化数据，这样就可以实现agent之间的级联</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230128000603210.png" alt="image-20230128000603210"></p>
<h3 id="参数：-3"><a href="#参数：-3" class="headerlink" title="参数："></a>参数：</h3><table>
<thead>
<tr>
<th><strong>Property Name</strong></th>
<th><strong>Default</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>channel</strong></td>
<td>–</td>
<td></td>
</tr>
<tr>
<td><strong>type</strong></td>
<td>–</td>
<td>The component type name,  needs to be avro.</td>
</tr>
<tr>
<td><strong>hostname</strong></td>
<td>–</td>
<td>目标avro source的主机</td>
</tr>
<tr>
<td><strong>port</strong></td>
<td>–</td>
<td>目标avro source的绑定端口</td>
</tr>
<tr>
<td>batch-size</td>
<td>100</td>
<td>number of event to batch  together for send.</td>
</tr>
<tr>
<td>connect-timeout</td>
<td>20000</td>
<td>连接超时时间</td>
</tr>
<tr>
<td>request-timeout</td>
<td>20000</td>
<td>请求超时时间</td>
</tr>
<tr>
<td>reset-connection-interval</td>
<td>none</td>
<td>Amount of time (s)  before the connection to the next hop is reset. This will force the Avro Sink  to reconnect to the next hop. This will allow the sink to connect to hosts  behind a hardware load-balancer when news hosts are added without having to  restart the agent.</td>
</tr>
<tr>
<td>compression-type</td>
<td>none</td>
<td>This can be “none” or “deflate”. The  compression-type must match the compression-type of matching AvroSource</td>
</tr>
<tr>
<td>compression-level</td>
<td>6</td>
<td>The level of compression  to compress event. 0 &#x3D; no compression and 1-9 is compression. The higher the  number the more compression</td>
</tr>
<tr>
<td>ssl</td>
<td>false</td>
<td>Set to true to enable  SSL for this AvroSink. When configuring SSL, you can optionally set a “truststore”, “truststore-password”, “truststore-type”, and specify whether to “trust-all-certs”.</td>
</tr>
<tr>
<td>trust-all-certs</td>
<td>false</td>
<td>If this is set to true,  SSL server certificates for remote servers (Avro Sources) will not be  checked. This should NOT be used in production because it makes it easier for  an attacker to execute a man-in-the-middle attack and “listen in” on  the encrypted connection.</td>
</tr>
<tr>
<td>truststore</td>
<td>–</td>
<td>The path to a custom  Java truststore file. Flume uses the certificate authority information in  this file to determine whether the remote Avro Source’s SSL authentication credentials should be trusted. If not  specified, then the global keystore will be used. If the global keystore not  specified either, then the default Java JSSE certificate authority files  (typically “jssecacerts” or “cacerts” in the Oracle JRE) will be used.</td>
</tr>
<tr>
<td>truststore-password</td>
<td>–</td>
<td>The password for the  truststore. If not specified, then the global keystore password will be used  (if defined).</td>
</tr>
<tr>
<td>truststore-type</td>
<td>JKS</td>
<td>The type of the Java  truststore. This can be “JKS” or other supported Java truststore type. If not specified, then  the global keystore type will be used (if defined, otherwise the defautl is  JKS).</td>
</tr>
<tr>
<td>exclude-protocols</td>
<td>SSLv3</td>
<td>Space-separated list of  SSL&#x2F;TLS protocols to exclude. SSLv3 will always be excluded in addition to  the protocols specified.</td>
</tr>
<tr>
<td>maxIoWorkers</td>
<td>2 * the number of  available processors in the machine</td>
<td>The maximum number of  I&#x2F;O worker threads. This is configured on the NettyAvroRpcClient  NioClientSocketChannelFactory.</td>
</tr>
</tbody></table>
<h3 id="配置示例：-1"><a href="#配置示例：-1" class="headerlink" title="配置示例："></a>配置示例：</h3><p>级联配置，需要至少两个flume agent来演示</p>
<p>在C703上，配置avro sink 发送者</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## c703 ##</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">s1</span></span><br><span class="line"><span class="attr">a1.sources.s1.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="attr">a1.sources.s1.command</span> = <span class="string">tail -F /root/weblog/access.log</span></span><br><span class="line"><span class="attr">a1.sources.s1.channels</span> = <span class="string">c1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = <span class="string">c701</span></span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="string">4545</span></span><br></pre></td></tr></table></figure>



<p>在C701上，配置avro source 接收者</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## c701 ##</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">s1</span></span><br><span class="line"><span class="attr">a1.sources.s1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sources.s1.hostname</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="attr">a1.sources.s1.port</span> = <span class="string">4545</span></span><br><span class="line"><span class="attr">a1.sources.s1.channel</span> = <span class="string">c1</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>



<h3 id="启动测试：-4"><a href="#启动测试：-4" class="headerlink" title="启动测试："></a>启动测试：</h3><p>先在C701上启动接受者avro source（服务）</p>
<p><em>bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;avro-mem-logger.conf -Dflume.root.logger&#x3D;INFO,console</em></p>
<p>再在C703上启动发送者avro sink（客户端）</p>
<p><em>bin&#x2F;flume-ng agent -n a1 -c conf&#x2F; -f myconf&#x2F;tail-mem-avro.conf -Dflume.root.logger&#x3D;INFO,console</em></p>
<h1 id="十一、Channel选择器（channel-selector）"><a href="#十一、Channel选择器（channel-selector）" class="headerlink" title="十一、Channel选择器（channel selector）"></a>十一、Channel选择器（channel selector）</h1><p>一个source可以对接多个channel</p>
<p>那么，source的数据如何在多个channel之间传递，就由selector来控制</p>
<p>配置应该挂载到source组件上</p>
<h2 id="1-Replicating-Channel-Selector（复制选择器）"><a href="#1-Replicating-Channel-Selector（复制选择器）" class="headerlink" title="(1)Replicating Channel Selector（复制选择器）"></a>(1)Replicating Channel Selector（复制选择器）</h2><p>replicating selector就是默认的选择器</p>
<p>官网配置参考</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230128000918130.png" alt="image-20230128000918130"></p>
<h3 id="目标场景："><a href="#目标场景：" class="headerlink" title="目标场景："></a>目标场景：</h3><p>selector将event复制，分发给所有下游节点</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230128001034328.png" alt="image-20230128001034328"></p>
<h3 id="可选属性如下"><a href="#可选属性如下" class="headerlink" title="可选属性如下"></a>可选属性如下</h3><table>
<thead>
<tr>
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">selector.type</td>
<td align="left">replicating</td>
<td align="left"><code>replicating</code></td>
</tr>
<tr>
<td align="left">selector.optional</td>
<td align="left">–</td>
<td align="left"><code>optional</code></td>
</tr>
</tbody></table>
<h3 id="使用案例："><a href="#使用案例：" class="headerlink" title="使用案例："></a>使用案例：</h3><p>下面的配置中，c2是一个可选的channel，写入c2失败的话会被忽略，c1没有标记为可选，如果写入c1失败会导致事务的失败</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example10-channel-replicating.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1 k2</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1 c2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1 c2</span></span><br><span class="line"><span class="attr">a1.sources.r1.command</span> = <span class="string">tail -F /export/data/flume-example-data/logdata/access.log</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sources.r1.selector.type</span> = <span class="string">replicating</span></span><br><span class="line"><span class="attr">a1.sources.r1.selector.optional</span> = <span class="string">c2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c2.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c2.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c2.transactionCapacity</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.path</span> = <span class="string">hdfs://node1:8020/logdata_c1/%Y-%m-%d/%H/</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.filePrefix</span> = <span class="string">logdata_</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileSuffix</span> = <span class="string">.log</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollInterval</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollSize</span> = <span class="string">268435456</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollCount</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileType</span> = <span class="string">DataStream</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.useLocalTimeStamp</span> = <span class="string">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k2.channel</span> = <span class="string">c2</span></span><br><span class="line"><span class="attr">a1.sinks.k2.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.path</span> = <span class="string">hdfs://node1:8020/logdata_c2/%Y-%m-%d/%H/</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.filePrefix</span> = <span class="string">logdata_</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.fileSuffix</span> = <span class="string">.log</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.rollInterval</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.rollSize</span> = <span class="string">268435456</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.rollCount</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.fileType</span> = <span class="string">DataStream</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.useLocalTimeStamp</span> = <span class="string">true</span></span><br></pre></td></tr></table></figure>

<h2 id="（2）Multiplexing-Channel-Selector（多路选择器）"><a href="#（2）Multiplexing-Channel-Selector（多路选择器）" class="headerlink" title="（2）Multiplexing Channel Selector（多路选择器）"></a>（2）Multiplexing Channel Selector（多路选择器）</h2><p>multiplexing selector可以根据event中的一个指定key的value来决定这条消息会写入哪个channel，具体在选择时，需要配置一个映射关系，比如</p>
<p>a1.sources.r1.selector.mapping.CZ&#x3D;c1 ; 就意味着header中的value为CZ的话，这条消息就会被写入c1这个channel</p>
<p>multiplexing selector官方配置参考</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230128003257255.png" alt="image-20230128003257255"></p>
<h3 id="目标场景"><a href="#目标场景" class="headerlink" title="目标场景"></a>目标场景</h3><p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230128003317149.png" alt="image-20230128003317149"></p>
<h3 id="多路channel选择器，可选属性如下"><a href="#多路channel选择器，可选属性如下" class="headerlink" title="多路channel选择器，可选属性如下"></a>多路channel选择器，可选属性如下</h3><table>
<thead>
<tr>
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left">selector.type</td>
<td align="left">replicating</td>
<td align="left"><code>multiplexing</code></td>
</tr>
<tr>
<td align="left">selector.header</td>
<td align="left">flume.selector.header</td>
<td align="left">键值Key</td>
</tr>
<tr>
<td align="left">selector.default</td>
<td align="left">–</td>
<td align="left"></td>
</tr>
<tr>
<td align="left">selector.mapping.*</td>
<td align="left">–</td>
<td align="left">路由</td>
</tr>
</tbody></table>
<h3 id="使用案例：-1"><a href="#使用案例：-1" class="headerlink" title="使用案例："></a>使用案例：</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example11-channel-Multiplexing.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1 c2</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1 k2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">TAILDIR</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1 c2</span></span><br><span class="line"><span class="attr">a1.sources.r1.positionFile</span> = <span class="string">/export/data/flume-example-data/flumedata/taildir_position.json</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups</span> = <span class="string">g1 g2</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.g1</span> = <span class="string">/export/data/flume-example-data/weblog/web.*</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.g2</span> = <span class="string">/export/data/flume-example-data/wxlog/wx.*</span></span><br><span class="line"><span class="attr">a1.sources.r1.headers.g1.logtype</span> = <span class="string">web</span></span><br><span class="line"><span class="attr">a1.sources.r1.headers.g2.logtype</span> = <span class="string">wx</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.selector.type</span> = <span class="string">multiplexing</span></span><br><span class="line"><span class="attr">a1.sources.r1.selector.header</span> = <span class="string">logtype</span></span><br><span class="line"><span class="attr">a1.sources.r1.selector.mapping.web</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.selector.mapping.wx</span> = <span class="string">c2</span></span><br><span class="line"><span class="attr">a1.sources.r1.selector.default</span> = <span class="string">c2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c2.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c2.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c2.transactionCapacity</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.path</span> = <span class="string">hdfs://node1:8020/%&#123;logtype&#125;/%Y-%m-%d/%H/</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.filePrefix</span> = <span class="string">logdata_</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileSuffix</span> = <span class="string">.log</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollInterval</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollSize</span> = <span class="string">268435456</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollCount</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileType</span> = <span class="string">DataStream</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.useLocalTimeStamp</span> = <span class="string">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k2.channel</span> = <span class="string">c2</span></span><br><span class="line"><span class="attr">a1.sinks.k2.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.path</span> = <span class="string">hdfs://node1:8020/%&#123;logtype&#125;/%Y-%m-%d/%H/</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.filePrefix</span> = <span class="string">logdata_</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.fileSuffix</span> = <span class="string">.log</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.rollInterval</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.rollSize</span> = <span class="string">268435456</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.rollCount</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.fileType</span> = <span class="string">DataStream</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hdfs.useLocalTimeStamp</span> = <span class="string">true</span></span><br></pre></td></tr></table></figure>

<p><strong>这里通过事件的header值来判断将事件发送到哪个channel，还可以配合拦截器一起使用。</strong></p>
<h1 id="十二、Sink处理器（sink-processor）"><a href="#十二、Sink处理器（sink-processor）" class="headerlink" title="十二、Sink处理器（sink processor）"></a>十二、Sink处理器（sink processor）</h1><p>一个agent中，多个sink可以被组装到一个sink组，而数据在组内多个sink之间发送，由sink processor来决定</p>
<p>Sink processor在flume中有3种：</p>
<p>第一种： 默认的，不需要专门去配置的；相当于负载均衡，但是sink不需要创建group；</p>
<p>第二种： failover sink processor 自动失败切换，需要将多个sink创建成group</p>
<p>第三种： load_balance sink processor 负载均衡，需要将多个sink创建成group</p>
<p>可以将多个sink放入到一个组中，Sink处理器能够对一个组中所有的sink进行负载均衡，在一个sink出现临时错误时进行故障转移。</p>
<p>必须设置属性：</p>
<table>
<thead>
<tr>
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>sinks</strong></td>
<td align="left">–</td>
<td align="left">组中多个sink使用空格分隔</td>
</tr>
<tr>
<td align="left"><strong>processor.type</strong></td>
<td align="left"><code>default</code></td>
<td align="left"><code>default</code>, <code>failover</code> 或<code>load_balance</code></td>
</tr>
</tbody></table>
<p>举例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br></pre></td></tr></table></figure>

<h2 id="（1）Default-Sink-Processor"><a href="#（1）Default-Sink-Processor" class="headerlink" title="（1）Default Sink Processor"></a>（1）Default Sink Processor</h2><p>默认的Sink处理器只支持单个Sink</p>
<h2 id="（2）Failover-Sink-Processor"><a href="#（2）Failover-Sink-Processor" class="headerlink" title="（2）Failover Sink Processor"></a>（2）Failover Sink Processor</h2><p>一组中只有优先级高的那个sink在工作，另一个是等待中</p>
<p>如果高优先级的sink发送数据失败，则专用低优先级的sink去工作，并且，<strong>在配置时间penalty之后，还会尝试用高优先级的去发送数据。</strong></p>
<p>故障转移处理器维护了一个带有优先级的sink列表，故障转移机制将失败的sink放入到一个冷却池中，如果sink成功发送了事件，将其放入到活跃池中，sink可以设置优先级，数字越高，优先级越高，如果一个sink发送事件失败，下一个有更高优先级的sink将被用来发送事件，比如，<strong>优先级100的比优先级80的先被使用，如果没有设置优先级，按配置文件中配置的顺序决定。</strong>设置属性如下：</p>
<table>
<thead>
<tr>
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>sinks</strong></td>
<td align="left">–</td>
<td align="left">组内多个sinks空格分隔</td>
</tr>
<tr>
<td align="left"><strong>processor.type</strong></td>
<td align="left"><code>default</code></td>
<td align="left"><code>failover</code></td>
</tr>
<tr>
<td align="left"><strong>processor.priority.</strong></td>
<td align="left">–</td>
<td align="left">优先级</td>
</tr>
<tr>
<td align="left">processor.maxpenalty</td>
<td align="left">30000</td>
<td align="left">失败sink的最大冷却时间</td>
</tr>
</tbody></table>
<p>示例如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = failover</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 对两个sink分配不同的优先级</span></span></span><br><span class="line">a1.sinkgroups.g1.processor.priority.k1 = 200</span><br><span class="line">a1.sinkgroups.g1.processor.priority.k2 = 100</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 主sink失败后，停用惩罚时间</span></span></span><br><span class="line">a1.sinkgroups.g1.processor.maxpenalty = 5000</span><br></pre></td></tr></table></figure>

<p>优先级高的sink如果失败，则processor会激活优先级第二的sink；</p>
<p>然后经过指定惩罚时间后，processor会再次尝试激活优先级高的sink；</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230130103523537.png" alt="image-20230130103523537"></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230130103553408.png" alt="image-20230130103553408"></p>
<h3 id="实例："><a href="#实例：" class="headerlink" title="实例："></a>实例：</h3><h4 id="（1）上游flume配置（node1）"><a href="#（1）上游flume配置（node1）" class="headerlink" title="（1）上游flume配置（node1）"></a>（1）上游flume配置（node1）</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example12-1-sink-failover.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1 k2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.command</span> = <span class="string">tail -F /export/data/flume-example-data/logdata/access.log</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = <span class="string">node2</span></span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="attr">a1.sinks.k1.batch-size</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k2.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k2.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hostname</span> = <span class="string">node3</span></span><br><span class="line"><span class="attr">a1.sinks.k2.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="attr">a1.sinks.k2.batch-size</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinkgroups</span> = <span class="string">g1</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.sinks</span> = <span class="string">k1 k2</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.type</span> = <span class="string">failover</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.priority.k1</span> = <span class="string">200</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.priority.k2</span> = <span class="string">100</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.maxpenalty</span> = <span class="string">5000</span></span><br></pre></td></tr></table></figure>

<h4 id="（2）下游flume配置（node2-node3）"><a href="#（2）下游flume配置（node2-node3）" class="headerlink" title="（2）下游flume配置（node2 node3）"></a>（2）下游flume配置（node2 node3）</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example12-2-sink-failover.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="attr">a1.sources.r1.threads</span> = <span class="string">10</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>



<h2 id="（3）Load-balancing-Sink-Processor"><a href="#（3）Load-balancing-Sink-Processor" class="headerlink" title="（3）Load balancing Sink Processor"></a>（3）Load balancing Sink Processor</h2><p>允许channel中的数据在一组sink中的多个sink之间进行交替，交替策略有：</p>
<p><strong>round_robin</strong>（轮询算法）</p>
<p><strong>random</strong>（随机）</p>
<p>负载均衡处理器，可以通过轮询或者随机的方式进行负载均衡，也可以通过继承AbstractSinkSelector 自定义负载均衡，设置属性如下：</p>
<table>
<thead>
<tr>
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>processor.sinks</strong></td>
<td align="left">–</td>
<td align="left">组内多个sinks空格分隔</td>
</tr>
<tr>
<td align="left"><strong>processor.type</strong></td>
<td align="left"><code>default</code></td>
<td align="left"><code>load_balance</code></td>
</tr>
<tr>
<td align="left">processor.backoff</td>
<td align="left">false</td>
<td align="left">是否将失败的sink加入黑名单</td>
</tr>
<tr>
<td align="left">processor.selector</td>
<td align="left"><code>round_robin</code></td>
<td align="left">轮询机制:<code>round_robin</code>, <code>random</code> 或者自定义</td>
</tr>
<tr>
<td align="left">processor.selector.maxTimeOut</td>
<td align="left">30000</td>
<td align="left">黑名单有效时间（单位毫秒）</td>
</tr>
</tbody></table>
<p>示例如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a1.sinkgroups = g1</span><br><span class="line">a1.sinkgroups.g1.sinks = k1 k2</span><br><span class="line">a1.sinkgroups.g1.processor.type = load_balance</span><br><span class="line">a1.sinkgroups.g1.processor.backoff = true</span><br><span class="line">a1.sinkgroups.g1.processor.selector = round_robin</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230130111311611.png" alt="image-20230130111311611"></p>
<h3 id="实例：-1"><a href="#实例：-1" class="headerlink" title="实例："></a>实例：</h3><h4 id="（1）上游flume配置（node1）-1"><a href="#（1）上游flume配置（node1）-1" class="headerlink" title="（1）上游flume配置（node1）"></a>（1）上游flume配置（node1）</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example13-1-sink-loadbalance.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1 k2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.command</span> = <span class="string">tail -F /export/data/flume-example-data/logdata/access.log</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = <span class="string">node2</span></span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="attr">a1.sinks.k1.batch-size</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k2.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k2.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hostname</span> = <span class="string">node3</span></span><br><span class="line"><span class="attr">a1.sinks.k2.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="attr">a1.sinks.k2.batch-size</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinkgroups</span> = <span class="string">g1</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.sinks</span> = <span class="string">k1 k2</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.type</span> = <span class="string">load_balance</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.backoff</span> = <span class="string">true </span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.selector</span> = <span class="string">round_robin</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（2）下游flume配置（node2-node3）-1"><a href="#（2）下游flume配置（node2-node3）-1" class="headerlink" title="（2）下游flume配置（node2 node3）"></a>（2）下游flume配置（node2 node3）</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example13-2-sink-loadbalance.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="attr">a1.sources.r1.threads</span> = <span class="string">10</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">logger</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>



<h1 id="十三、flume进阶"><a href="#十三、flume进阶" class="headerlink" title="十三、flume进阶"></a>十三、flume进阶</h1><h2 id="（1）flume-事务机制"><a href="#（1）flume-事务机制" class="headerlink" title="（1）flume 事务机制"></a>（1）flume 事务机制</h2><p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230130164206781.png" alt="image-20230130164206781"></p>
<p>二、Delivery 保证</p>
<p>认识 Flume 对事件投递的可靠性保证是非常重要的，它往往是我们是否使用 Flume 来解决问题的决定因素之一。</p>
<p>消息投递的可靠保证有三种：</p>
<ul>
<li>At-least-once</li>
<li>At-most-once</li>
<li>Exactly-once</li>
</ul>
<p>基本上所有工具的使用用户都希望工具框架能保证消息 Exactly-once ，这样就不必在设计实现上考虑消息的丢失或者重复的处理场景。但是事实上很少有工具和框架能做到这一点，真正能做到这一点所付出的成本往往很大，或者带来的额外影响反而让你觉得不值得。假设 Flume 真的做到了 Exactly-once ，那势必降低了稳定性和吞吐量，所以 Flume 选择的策略是 At-least-once 。</p>
<p>当然这里的 At-least-once 需要加上引号，并不是说用上 Flume 的随便哪个组件组成一个实例，运行过程中就能保存消息不会丢失。事实上 At-least-once 原则只是说的是 Source 、 Channel 和 Sink 三者之间上下投递消息的保证。而当你选择 MemoryChannel 时，实例如果异常挂了再重启，在 channel 中的未被 sink 所消费的残留数据也就丢失了，从而没办法保证整条链路的 At-least-once。</p>
<p>Flume 的 At-least-once 保证的实现基础是建立了自身的 Transaction 机制。</p>
<p>Flume 的 Transaction 有4个生命周期函数，分别是 start、 commit、rollback 和 close。</p>
<p>当 Source 往 Channel 批量投递事件时首先调用 start 开启事务,批量</p>
<p>put 完事件后通过 commit 来提交事务，如果 commit 异常则 rollback ，然后 close 事务，最后 Source 将刚才提交的一批消息事件向源服务 ack（比如 kafka 提交新的 offset ）。Sink 消费 Channel 也是相同的模式，唯一的区别就是 Sink 需要在向目标源完成写入之后才对事务进行 commit。</p>
<p>两个组件的相同做法都是只有向下游成功投递了消息才会向上游 ack，从而保证了数据能 At-least-once 向下投递。</p>
<h2 id="（2）flume-agent-内部机制"><a href="#（2）flume-agent-内部机制" class="headerlink" title="（2）flume agent 内部机制"></a>（2）flume agent 内部机制</h2><p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230130164421179.png" alt="image-20230130164421179"></p>
<p>组件：</p>
<p>1、ChannelSelector</p>
<p>ChannelSelector 的作用就是选出 Event 将要被发往哪个 Channel。其共有两种类型，分别是 Replicating（复制）和 Multiplexing（多路复用）。 ReplicatingSelector 会将同一个 Event 发往所有的 Channel，Multiplexing 会根据相应的原则，将不同的 Event 发往不同的 Channel。</p>
<p>2、SinkProcessor</p>
<p>(1) SinkProcessor 共 有 三 种 类 型 ， 分 别 是 DefaultSinkProcessor 、    LoadBalancingSinkProcessor 和 FailoverSinkProcessor。</p>
<p>(2) DefaultSinkProcessor 对应的是单个的 Sink，<br>LoadBalancingSinkProcessor 和 FailoverSinkProcessor 对应的是 Sink Group。</p>
<p>(3) LoadBalancingSinkProcessor 可以实现负载均衡的功能，FailoverSinkProcessor 可以实现故障转移的功能。</p>
<h2 id="（3）flume监控及ganglia监控"><a href="#（3）flume监控及ganglia监控" class="headerlink" title="（3）flume监控及ganglia监控"></a>（3）flume监控及ganglia监控</h2><h3 id="1）flume监控"><a href="#1）flume监控" class="headerlink" title="1）flume监控"></a>1）flume监控</h3><p>FLUME在运行时，状态是否正常，吞吐量是否正常，需要监控</p>
<p>Flume自身具有向外提交状态数据的功能；<strong>但是它本身没有一个完善的监控平台；</strong></p>
<p>开启内置监控功能，<strong>启动时加入参数：</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Dflume.monitoring.type=http -Dflume.monitoring.port=34545</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230131134112881.png" alt="image-20230131134112881"></p>
<h3 id="2）ganglia监控"><a href="#2）ganglia监控" class="headerlink" title="2）ganglia监控"></a>2）ganglia监控</h3><p>将监控数据发往<strong>ganglia</strong>进行展现</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-Dflume.monitoring.type=ganglia -Dflume.monitoring.port=34890</span><br></pre></td></tr></table></figure>

<p>Ganglia是一个通用的集群运维监控系统；</p>
<p>它在各台需要监控状态信息的机器上安装“探针”，然后这些“探针”会收集所在机器上的各种状态信息<strong>（cpu负载，内存负载，磁盘IO负载，网络IO负载，以及各类应用软件的状态信息）</strong>，然后汇聚到它的中心汇聚点，并提供<strong>web页面进行图形可视化查看</strong></p>
<p><em><strong>各种应用软件的态，是不会被ganglia的“探针”获取到的；而是由应用软件自身开发相应功能，将自身需要被监控的状态数据，提交给ganglia；</strong></em></p>
<p>Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。Ganglia的核心包含<strong>gmond（监控守护进程）</strong>、<strong>gmetad（元数据守护进程）</strong>以及一个<strong>Web前端</strong>。主要是用来监控系统性能，如：cpu 、mem、硬盘利用率， I&#x2F;O负载、网络流量情况等，通过曲线很容易见到每个节点的工作状态，对合理调整、分配系统资源，提高系统整体性能起到重要作用。</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230130171701412.png" alt="image-20230130171701412"></p>
<h3 id="3）Ganglia安装"><a href="#3）Ganglia安装" class="headerlink" title="3）Ganglia安装"></a>3）Ganglia安装</h3><p><strong>中心节点的安装</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- epel包的安装：yum install -y epel-release(解决不能yum安装某些安装包的问题)</span><br><span class="line">- gmetad的安装：yum install -y ganglia-gmetad</span><br><span class="line">- gmond的安装：yum install -y ganglia-gmond</span><br><span class="line">- rrdtool的安装：yum install -y rrdtool</span><br><span class="line">- httpd服务器的安装：yum install -y httpd</span><br><span class="line">- ganglia-web及php安装：yum install -y ganglia-web php</span><br></pre></td></tr></table></figure>

<p><strong>被监测节点的安装</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- epel包的安装：yum install -y epel-release(解决不能yum安装某些安装包的问题)</span><br><span class="line">- gmond的安装：yum install -y gmond(提示找不到，感觉应该换成上面那个yum install -y ganglia-gmond)</span><br></pre></td></tr></table></figure>

<h3 id="4）Ganglia配置"><a href="#4）Ganglia配置" class="headerlink" title="4）Ganglia配置"></a>4）Ganglia配置</h3><p><strong>中心节点的配置</strong><br>安装目录说明</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- ganglia配置文件目录：/etc/ganglia</span><br><span class="line">- rrd数据库存放目录：/var/lib/ganglia/rrds</span><br><span class="line">- ganglia-web安装目录：/usr/share/ganglia</span><br><span class="line">- ganglia-web配置目录：/etc/httpd/conf.d/ganglia.conf</span><br></pre></td></tr></table></figure>

<p>相关配置文件修改<br>将ganglia-web的站点目录连接到httpd主站点目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"> <span class="built_in">ln</span> -s /usr/share/ganglia /var/www/html</span></span><br></pre></td></tr></table></figure>

<p>修改httpd主站点目录下ganglia站点目录的访问权限<br>将ganglia站点目录访问权限改为apache:apache，否则会报错</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"> <span class="built_in">chown</span> -R apache:apache /var/www/html/ganglia</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"> <span class="built_in">chmod</span> -R 755 /var/www/html/ganglia</span></span><br></pre></td></tr></table></figure>

<p>修改rrd数据库存放目录访问权限<br>将rrd数据库存放目录访问权限改为nobody:nobody,否则会报错</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"> <span class="built_in">chown</span> -R nobody:nobody /var/lib/ganglia/rrds</span></span><br></pre></td></tr></table></figure>

<p>修改ganglia-web的访问权限：<br>修改&#x2F;etc&#x2F;httpd&#x2F;conf.d&#x2F;ganglia.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Alias /ganglia /usr/share/ganglia</span><br><span class="line">&lt;Location /ganglia&gt; </span><br><span class="line"> Require all granted</span><br><span class="line"><span class="meta prompt_"> #</span><span class="language-bash">Require ip 10.1.2.3</span></span><br><span class="line"><span class="meta prompt_"> #</span><span class="language-bash">Require host example.org</span></span><br><span class="line">&lt;/Location&gt;</span><br></pre></td></tr></table></figure>

<p>修改dwoo下面的权限</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 777 /var/lib/ganglia/dwoo/compiled</span><br><span class="line">chmod 777 /var/lib/ganglia/dwoo/cache</span><br></pre></td></tr></table></figure>

<p>配置&#x2F;etc&#x2F;ganglia&#x2F;gmetad.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_source  &quot;my cluster&quot; 192.168.88.161:8649(注意是所有节点都加上，如master:8649 slave0x:8649)</span><br><span class="line"> </span><br><span class="line">setuid_username nobody</span><br></pre></td></tr></table></figure>

<p>配置&#x2F;etc&#x2F;ganglia&#x2F;gmond.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cluster &#123; </span><br><span class="line">  name = &quot;node1&quot;</span><br><span class="line">  ... </span><br><span class="line">&#125; </span><br><span class="line">udp_send_channel &#123; </span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">the host <span class="built_in">who</span> gather this cluster<span class="string">&#x27;s monitoring data and send these data   to gmetad node</span></span></span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash"><span class="string">注释掉多播模式的,以下出现这个都要注释掉</span></span></span><br><span class="line"><span class="meta prompt_"> #</span><span class="language-bash"><span class="string">mcast_join = 239.2.11.71</span></span></span><br><span class="line"><span class="meta prompt_"> #</span><span class="language-bash"><span class="string">添加单播模式的</span></span></span><br><span class="line"> host = 192.168.88.161</span><br><span class="line"> port = 8649 </span><br><span class="line">&#125; </span><br><span class="line">udp_recv_channel &#123; </span><br><span class="line">  bind = 192.168.88.161</span><br><span class="line">  port = 8649 </span><br><span class="line">&#125; </span><br><span class="line">tcp_accept_channel &#123; </span><br><span class="line">  port = 8649 </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>被监测节点的配置</strong><br>配置&#x2F;etc&#x2F;ganglia&#x2F;gmond.conf</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">cluster &#123; </span><br><span class="line">  name = &quot;hadoop cluster&quot;</span><br><span class="line">  ... </span><br><span class="line">&#125; </span><br><span class="line">udp_send_channel &#123; </span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">the host <span class="built_in">who</span> gather this cluster<span class="string">&#x27;s monitoring data and send these data   to gmetad node</span></span></span><br><span class="line"> host = 192.168.88.161  </span><br><span class="line"> port = 8649 </span><br><span class="line">&#125; </span><br><span class="line">udp_recv_channel &#123; </span><br><span class="line">  port = 8649 </span><br><span class="line">&#125; </span><br><span class="line">tcp_accept_channel &#123; </span><br><span class="line">  port = 8649 </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="5）Ganglia启动"><a href="#5）Ganglia启动" class="headerlink" title="5）Ganglia启动"></a>5）Ganglia启动</h3><p><strong>中心节点的启动</strong><br>start httpd, gmetad, gmond</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl start httpd.service</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl start gmetad.service</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl start gmond.service</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl <span class="built_in">enable</span> httpd.service</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl <span class="built_in">enable</span> gmetad.service</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl <span class="built_in">enable</span> gmond.service</span></span><br></pre></td></tr></table></figure>

<p><strong>被监测节点的启动</strong><br>start gmond</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl start gmond.service</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl <span class="built_in">enable</span> gmond.service</span></span><br></pre></td></tr></table></figure>

<p><strong>关闭selinux</strong></p>
<p>vi &#x2F;etc&#x2F;selinux&#x2F;config，把SELINUX&#x3D;enforcing改成SELINUX&#x3D;disable；该方法需要重启机器。<br>可以使用命令setenforce 0来关闭selinux而不需要重启，刷新页面，即可访问；不过此法只是权宜之计，如果想永久修改selinux设置，还是要使用第一种方法</p>
<p><strong>开启防火墙</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --zone=public --add-port=80/tcp --permanent</span><br><span class="line">firewall-cmd --reload</span><br></pre></td></tr></table></figure>

<p><strong>访问网页</strong><br>浏览器访问 {namenode的ip}&#x2F;ganglia即可 </p>
<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200326103901392.png" alt="image-20200326103901392"></p>
<h3 id="6）添加Flume监控"><a href="#6）添加Flume监控" class="headerlink" title="6）添加Flume监控"></a>6）添加Flume监控</h3><p>修改flume-env.sh配置，添加虚拟机选项</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JAVA_OPTS=&quot;-Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=192.168.88.161:8649 -Xms100m -Xmx200m&quot;</span><br></pre></td></tr></table></figure>

<p>启动flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent -n a1 -c conf -f example.conf -Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=192.168.88.161:8649</span><br></pre></td></tr></table></figure>

<p>查看ganglia页面</p>
<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200326111609055.png" alt="image-20200326111609055"></p>
<table>
<thead>
<tr>
<th>字段（图表名称）</th>
<th>字段含义</th>
</tr>
</thead>
<tbody><tr>
<td>EventPutAttemptCount</td>
<td>source尝试写入channel的事件总数量</td>
</tr>
<tr>
<td>EventPutSuccessCount</td>
<td>成功写入channel且提交的事件总数量</td>
</tr>
<tr>
<td>EventTakeAttemptCount</td>
<td>sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据。</td>
</tr>
<tr>
<td>EventTakeSuccessCount</td>
<td>sink成功读取的事件的总数量</td>
</tr>
<tr>
<td>StartTime</td>
<td>channel启动的时间（毫秒）</td>
</tr>
<tr>
<td>StopTime</td>
<td>channel停止的时间（毫秒）</td>
</tr>
<tr>
<td>ChannelSize</td>
<td>目前channel中事件的总数量</td>
</tr>
<tr>
<td>ChannelFillPercentage</td>
<td>channel占用百分比</td>
</tr>
<tr>
<td>ChannelCapacity</td>
<td>channel的容量</td>
</tr>
</tbody></table>
<h2 id="（4）flume调优"><a href="#（4）flume调优" class="headerlink" title="（4）flume调优"></a>（4）flume调优</h2><p>flume-ng agent包括source、channel、sink三个部分，这三部分都运行在JVM上，而JVM运行在linux操作系统之上。因此，对于flume的性能调优，就是对这三部分及影响因素调优。</p>
<h3 id="1）source的配置"><a href="#1）source的配置" class="headerlink" title="1）source的配置"></a>1）source的配置</h3><p>项目中采用的是 taildir source，他的读取速度能够跟上命令行写入日志的速度，故并未做特殊的处理。</p>
<h3 id="2）channel的配置"><a href="#2）channel的配置" class="headerlink" title="2）channel的配置"></a>2）channel的配置</h3><p>可选的channel配置一般有两种，一是memory channel，二是file channel。</p>
<p>建议在内存足够的情况下，优先选择memory channel。</p>
<p>尝试过相同配置下使用file channel和memory channel，file channel明显速度较慢，并且会生成log的文件，应该是用作缓存，当source已经接收但是还未写入sink时的event都会存在这个文件中。这样的好处是保证数据不会丢失，所以当对数据的丢失情况非常敏感且对实时性没有太大要求的时候，还是使用file memory吧。</p>
<p>一开始的memory channel配置用的是默认的，然后控制台报出了如下警告：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">WARN：The channel is full or unexpected failure. The <span class="built_in">source</span> will try again after 1000 ms</span><br></pre></td></tr></table></figure>

<p>这个是因为当前被采集的文件过大，可以通过增大keep-alive的值解决。深层的原因是文件采集的速度和sink的速度没有匹配好。</p>
<p>所以memory channel有三个比较重要的参数需要配置：</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#channel中最多缓存多少</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">5000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#channel一次最多吐给sink多少</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">2000</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#event的活跃时间</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.keep-alive</span> = <span class="string">10</span></span><br></pre></td></tr></table></figure>

<h3 id="3）sink的配置"><a href="#3）sink的配置" class="headerlink" title="3）sink的配置"></a>3）sink的配置</h3><p>可以通过压缩来节省空间和网络流量，但是会增加cpu的消耗。</p>
<p>batch：size越大性能越好，但是太大会影响时效性，一般batch size和源数据端的大小相同。</p>
<h3 id="4）java内存的配置"><a href="#4）java内存的配置" class="headerlink" title="4）java内存的配置"></a>4）java内存的配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_OPTS=<span class="string">&quot;-Xms512m -Xmx2048m&quot;</span></span><br></pre></td></tr></table></figure>

<p>主要涉及Xms和Xmx两个参数，可以根据实际的服务器的内存大小进行设计。</p>
<h3 id="5）OS内核参数的配置"><a href="#5）OS内核参数的配置" class="headerlink" title="5）OS内核参数的配置"></a>5）OS内核参数的配置</h3><p>如果单台服务器启动的flume agent过多的话，默认的内核参数设置偏小，需要调整。（待补充，暂时还未涉及）。</p>
<h3 id="6）调优实战案例"><a href="#6）调优实战案例" class="headerlink" title="6）调优实战案例"></a>6）调优实战案例</h3><p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230130172232611.png" alt="image-20230130172232611"></p>
<h4 id="agent层的调试数据"><a href="#agent层的调试数据" class="headerlink" title="agent层的调试数据"></a><strong>agent</strong>层的调试数据</h4><table>
<thead>
<tr>
<th align="center">优化方法</th>
<th align="center">java环境</th>
<th align="center">channel类型</th>
<th align="center">sink类型与个数</th>
<th align="center">压缩</th>
<th align="center">source接收到条数&amp;速度</th>
<th align="center">channel已写入 &amp;channel占用率</th>
<th align="center">sink输出量</th>
<th align="center">sink输出速度</th>
<th align="center">cpu占用</th>
<th align="center">内存占用</th>
<th align="center">总结</th>
</tr>
</thead>
<tbody><tr>
<td align="center">无</td>
<td align="center">默认</td>
<td align="center">file 100w</td>
<td align="center">负载均衡 2个avro  sink</td>
<td align="center">否</td>
<td align="center">2472192 1373&#x2F;s</td>
<td align="center">2473728 0.02%</td>
<td align="center">2360100</td>
<td align="center">1311&#x2F;s</td>
<td align="center">约35%</td>
<td align="center">8.6G</td>
<td align="center">传输效率很低</td>
</tr>
<tr>
<td align="center">优化java配置,启用2G内存</td>
<td align="center">Xms2g Xmx2g Xss256k Xmn1g</td>
<td align="center">file 100w</td>
<td align="center">负载均衡 2个avro  sink</td>
<td align="center">否</td>
<td align="center">6226688 3459&#x2F;s</td>
<td align="center">6227712 99.90%</td>
<td align="center">5228000</td>
<td align="center">2904&#x2F;s</td>
<td align="center">约30%</td>
<td align="center">8.6G</td>
<td align="center">传输效率有较大提升</td>
</tr>
<tr>
<td align="center">avro压缩传输</td>
<td align="center">Xms2g Xmx2g Xss256k Xmn1g</td>
<td align="center">file 100w</td>
<td align="center">负载均衡 2个avro  sink</td>
<td align="center">是</td>
<td align="center">5076224 2820&#x2F;s</td>
<td align="center">5077504 99.90%</td>
<td align="center">4077800</td>
<td align="center">2265&#x2F;s</td>
<td align="center">约47%</td>
<td align="center">10.4G</td>
<td align="center">传输效率不升反降</td>
</tr>
<tr>
<td align="center">采用memory channel</td>
<td align="center">Xms2g Xmx2g Xss256k Xmn1g</td>
<td align="center">memory 100w</td>
<td align="center">负载均衡 2个avro  sink</td>
<td align="center">是</td>
<td align="center">6767104 3759&#x2F;s</td>
<td align="center">6771968 99.90%</td>
<td align="center">5772200</td>
<td align="center">3207&#x2F;s</td>
<td align="center">100%-200%</td>
<td align="center">10.4G</td>
<td align="center">传输效率有少量提升，但是cpu压力很大</td>
</tr>
<tr>
<td align="center">数据不传输到L2，直接sink到本地</td>
<td align="center">Xms2g Xmx2g Xss256k Xmn1g</td>
<td align="center">file 100w</td>
<td align="center">负载均衡 2个file  sink</td>
<td align="center">否</td>
<td align="center">5761280 3201&#x2F;s</td>
<td align="center">5762816 0.00%</td>
<td align="center">5761662</td>
<td align="center">3201&#x2F;s</td>
<td align="center">10%-50%</td>
<td align="center">10.4G</td>
<td align="center">channel无堆积，但是传输量无明显提升</td>
</tr>
<tr>
<td align="center">将数据传给8个Sink组成的L2</td>
<td align="center">Xms2g Xmx2g Xss256k Xmn1g</td>
<td align="center">file 100w</td>
<td align="center">负载均衡 8个avro  sink</td>
<td align="center">否</td>
<td align="center">1423616 791&#x2F;s</td>
<td align="center">1393664 99.90%</td>
<td align="center">393700</td>
<td align="center">218&#x2F;s</td>
<td align="center">5%</td>
<td align="center">10.4G</td>
<td align="center">传输效率极低</td>
</tr>
<tr>
<td align="center">优化avro-avro传输，avro sink配置maxIoWorkers128，L2的avro source threads设为1000</td>
<td align="center">Xms2g Xmx2g Xss256k Xmn1g</td>
<td align="center">file 100w</td>
<td align="center">负载均衡 2个file  sink</td>
<td align="center">否</td>
<td align="center">2971392 1651&#x2F;s</td>
<td align="center">2972416 99.90%</td>
<td align="center">1972500</td>
<td align="center">1095&#x2F;s</td>
<td align="center">5%-20%</td>
<td align="center">10.4G</td>
<td align="center">传输效率不升反降</td>
</tr>
<tr>
<td align="center">将L2数据sink到本地</td>
<td align="center">Xms2g Xmx2g Xss256k Xmn1g</td>
<td align="center">file 100w</td>
<td align="center">负载均衡 2个avro  sink</td>
<td align="center">否</td>
<td align="center">6279168 3488&#x2F;s</td>
<td align="center">6280448 99.90%</td>
<td align="center">5280500</td>
<td align="center">2934&#x2F;s</td>
<td align="center">约30%</td>
<td align="center">10.4G</td>
<td align="center">传输效率无明显提升</td>
</tr>
<tr>
<td align="center">增大java内存 增大avro sink batchsize 启用压缩 启用memorychannel</td>
<td align="center">Xms8g Xmx8g Xss256k Xmn3g</td>
<td align="center">memory 100w</td>
<td align="center">负载均衡 2个avro  sink</td>
<td align="center">是</td>
<td align="center">5640704 3134&#x2F;s</td>
<td align="center">5643776 99.90%</td>
<td align="center">4644000</td>
<td align="center">2580&#x2F;s</td>
<td align="center">30%</td>
<td align="center">16.5G</td>
<td align="center">传输效率无明显提升</td>
</tr>
<tr>
<td align="center">增加jvm内存到8G，filechannel设为1000w容量</td>
<td align="center">Xms8g Xmx8g Xss256k Xmn3g</td>
<td align="center">file 1000w</td>
<td align="center">负载均衡 2个file  sink</td>
<td align="center">否</td>
<td align="center">7330816 4073&#x2F;s</td>
<td align="center">7332096 48%</td>
<td align="center">2519000</td>
<td align="center">1399&#x2F;s</td>
<td align="center">2%-16%</td>
<td align="center">16.5G</td>
<td align="center">传输效率不升反降</td>
</tr>
<tr>
<td align="center">在上一种方法的基础上增加avro batchsize为2000</td>
<td align="center">Xms8g Xmx8g Xss256k Xmn3g</td>
<td align="center">file 2000w</td>
<td align="center">负载均衡 2个avro  sink</td>
<td align="center">否</td>
<td align="center">8168960 4538&#x2F;s</td>
<td align="center">8170496 69%</td>
<td align="center">1270000</td>
<td align="center">706&#x2F;s</td>
<td align="center">10%-20%</td>
<td align="center">16.5G</td>
<td align="center">传输效率极低</td>
</tr>
<tr>
<td align="center">4个sink无组策略</td>
<td align="center">Xms8g Xmx8g Xss256k Xmn3g</td>
<td align="center">file 2000w</td>
<td align="center">无组策略 4个avro  sink</td>
<td align="center">否</td>
<td align="center">7222272 4012&#x2F;s</td>
<td align="center">7223808 0.02%</td>
<td align="center">7222272</td>
<td align="center">4012&#x2F;s</td>
<td align="center">30%-50%</td>
<td align="center">16.6G</td>
<td align="center">取消负载均衡，效率有较大提升666</td>
</tr>
<tr>
<td align="center">8个sink无组策略</td>
<td align="center">Xms8g Xmx8g Xss256k Xmn3g</td>
<td align="center">file 2000w</td>
<td align="center">无组策略 8个avro  sink</td>
<td align="center">否</td>
<td align="center">7034624 3908&#x2F;s</td>
<td align="center">7035648 0.03614%</td>
<td align="center">7032034</td>
<td align="center">3906&#x2F;s</td>
<td align="center">30%-50%</td>
<td align="center">16.6G</td>
<td align="center">无组策略的情况下，8个sink和4个sink效率无差</td>
</tr>
<tr>
<td align="center">4个sink无组策略  memrorychannel</td>
<td align="center">Xms8g Xmx8g Xss256k Xmn3g</td>
<td align="center">memory 100w</td>
<td align="center">无组策略 4个avro  sink</td>
<td align="center">否</td>
<td align="center">17435648 9686&#x2F;s</td>
<td align="center">17441536 47.56%</td>
<td align="center">12678000</td>
<td align="center">7043&#x2F;s</td>
<td align="center">100%-500%</td>
<td align="center">16.6G</td>
<td align="center">cpu压力很大</td>
</tr>
<tr>
<td align="center">2个sink无组策略  memrorychannel L2的内存channel扩大到1亿</td>
<td align="center">Xms8g Xmx8g Xss256k Xmn3g</td>
<td align="center">memory 100w</td>
<td align="center">无组策略 4个avro  sink</td>
<td align="center">否</td>
<td align="center">18679438 10337&#x2F;s</td>
<td align="center">18684778 0.00256%</td>
<td align="center">18683444</td>
<td align="center">10448&#x2F;s</td>
<td align="center">10%-20%</td>
<td align="center">16.6G</td>
<td align="center">此为目前传输最快的方案</td>
</tr>
<tr>
<td align="center">8个sink无组策略，2G内存</td>
<td align="center">Xms2g Xmx2g Xss256k Xmn1g</td>
<td align="center">file 2000w</td>
<td align="center">无组策略 8个avro  sink</td>
<td align="center">否</td>
<td align="center">7067392 3926&#x2F;s</td>
<td align="center">7068416 0.00256%</td>
<td align="center">7067212</td>
<td align="center">3926&#x2F;s</td>
<td align="center">30%-40%</td>
<td align="center">10.8G</td>
<td align="center">jvm2G内存已经够用</td>
</tr>
<tr>
<td align="center">8个sink无组策略，2G内存,压缩</td>
<td align="center">Xms2g Xmx2g Xss256k Xmn1g</td>
<td align="center">file 2000w</td>
<td align="center">无组策略 8个avro  sink</td>
<td align="center">是</td>
<td align="center">7362048 4090&#x2F;s</td>
<td align="center">7363072 0.0055%</td>
<td align="center">7361995</td>
<td align="center">4090&#x2F;s</td>
<td align="center">50%-100%</td>
<td align="center">10.6G</td>
<td align="center">L1到L2压缩对传输速度无太大提升，增大了CPU负担</td>
</tr>
<tr>
<td align="center">4个sink无组策略，2G内存,压缩</td>
<td align="center">Xms2g Xmx2g Xss256k Xmn1g</td>
<td align="center">file 2000w</td>
<td align="center">无组策略 4个avro  sink</td>
<td align="center">是</td>
<td align="center">7501056 4167&#x2F;s</td>
<td align="center">7502592 0.01664</td>
<td align="center">8036352</td>
<td align="center">4464&#x2F;s</td>
<td align="center">40%-50%</td>
<td align="center">10.5G</td>
<td align="center">从channel获取的数量大于channel写入的数量，说明此时的瓶颈在channel写入</td>
</tr>
<tr>
<td align="center">4个sink无组策略，2G内存,不压缩</td>
<td align="center">Xms2g Xmx2g Xss256k Xmn1g</td>
<td align="center">file 2000w</td>
<td align="center">无组策略 4个avro  sink</td>
<td align="center">否</td>
<td align="center">7745792 4303&#x2F;s</td>
<td align="center">7747328 0.02688</td>
<td align="center">7741952</td>
<td align="center">4301&#x2F;s</td>
<td align="center">50%-100%</td>
<td align="center">10.5G</td>
<td align="center">L1到L2压缩对传输速度无太大提升，增大了CPU负担</td>
</tr>
<tr>
<td align="center">8个sink无组策略，发送到4个L2，每个L2有两个source，两个channel</td>
<td align="center">Xms2g Xmx2g Xss256k Xmn1g</td>
<td align="center">file 2000w</td>
<td align="center">无组策略 8个avro  sink</td>
<td align="center">否</td>
<td align="center">6734336 3741&#x2F;s</td>
<td align="center">6735360 0.00177</td>
<td align="center">6735006</td>
<td align="center">3742&#x2F;s</td>
<td align="center">30%-50%</td>
<td align="center">10.8G</td>
<td align="center">发给同一机器的两个avro source无性能提升</td>
</tr>
</tbody></table>
<h4 id="collector层的调试数据"><a href="#collector层的调试数据" class="headerlink" title="collector层的调试数据"></a>collector层的调试数据</h4><table>
<thead>
<tr>
<th align="center">优化方法</th>
<th align="center">java环境</th>
<th align="center">channel类型</th>
<th align="center">sink类型与个数</th>
<th>压缩</th>
<th align="center">source接收到条数&amp;速度</th>
<th align="center">channel已写入 &amp;channel占用率</th>
<th align="center">sink输出量</th>
<th align="center">sink输出速度</th>
<th align="center">cpu占用</th>
<th align="center">内存占用</th>
<th align="center">总结</th>
</tr>
</thead>
<tbody><tr>
<td align="center">正常传输</td>
<td align="center">Xms20g Xmx20g Xss256k Xmn8g</td>
<td align="center">file 100w</td>
<td align="center">无组策略 1个kafka  sink</td>
<td>否</td>
<td align="center">3099048 1722&#x2F;s</td>
<td align="center">3095238 99.90%</td>
<td align="center">2144761</td>
<td align="center">1192&#x2F;s</td>
<td align="center">2%-5%</td>
<td align="center">25.6G</td>
<td align="center">输出速度  不够</td>
</tr>
<tr>
<td align="center">增加到4个sink，filechannel容量扩大</td>
<td align="center">Xms20g Xmx20g Xss256k Xmn8g</td>
<td align="center">file 1亿</td>
<td align="center">无组策略 4个kafka  sink</td>
<td>否</td>
<td align="center">18850000 10472&#x2F;s</td>
<td align="center">18850000 9.86%</td>
<td align="center">8970971</td>
<td align="center">4984&#x2F;s</td>
<td align="center">50%-100%</td>
<td align="center">25.6G</td>
<td align="center">速度提升  约4倍</td>
</tr>
<tr>
<td align="center">使用memory channel</td>
<td align="center">Xms20g Xmx20g Xss256k Xmn8g</td>
<td align="center">memory 1000w</td>
<td align="center">无组策略 4个kafka  sink</td>
<td>否</td>
<td align="center">17794000 9886&#x2F;s</td>
<td align="center">17744000 99.95%</td>
<td align="center">7745000</td>
<td align="center">4303&#x2F;s</td>
<td align="center">100%-800%</td>
<td align="center">25.6G</td>
<td align="center">相比使用file channel无性能提升</td>
</tr>
<tr>
<td align="center">增加到8个sink</td>
<td align="center">Xms20g Xmx20g Xss256k Xmn8g</td>
<td align="center">file 1亿</td>
<td align="center">无组策略 8个kafka  sink</td>
<td>否</td>
<td align="center">19251655 10695&#x2F;s</td>
<td align="center">19249668 2%</td>
<td align="center">17148675</td>
<td align="center">9527&#x2F;s</td>
<td align="center">100%-150%</td>
<td align="center">26.8G</td>
<td align="center">8个sink传输比1个速度提升约8倍</td>
</tr>
</tbody></table>
<h5 id="agent到collector启用zlib压缩前后的网络带宽情况"><a href="#agent到collector启用zlib压缩前后的网络带宽情况" class="headerlink" title="agent到collector启用zlib压缩前后的网络带宽情况"></a>agent到collector启用zlib压缩前后的网络带宽情况</h5><p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230130173816550.png" alt="image-20230130173816550"></p>
<h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a><strong>总结：</strong></h4><p>1、默认jvm环境只使用了20m内存，需要调整，扩大到2G基本够用，不需要过大； </p>
<p>2、collector到kafka，启用的sink越多，传输越快； </p>
<p>3、如果collector到kafka通路受阻，使数据堆积在channel中，如果channel堆满，则会影响agent到collector的传输； </p>
<p>4、agent到collector如果配置了组策略（sink group），则一个组策略只启动一个传输线程，多个sink成一组则传输效率远不如sink各传各的，一个sink相当于一个传输线程； </p>
<p>5、通路不受阻的情况下，memoryChannel传输效率比fileChannel高很多，不过对cpu、内存的要求会很高； </p>
<p>6、avro压缩传输对传输效率没有太大提升，反而增大cpu负担,但是会大大降低网络带宽； </p>
<p>7、在网络带宽受限的情况下，增加sink、改用memory channel等方法都不能增加传输效率； </p>
<p>8、影响flume传输性能的<strong>主要因素有，jvm内存、网络带宽、channel类型、sink个数、是否压缩、机器硬件性能</strong>，各条件需要做一个综合的性能平衡，某一个环节出现瓶颈就会影响整个系统的传输性能。</p>
<h1 id="十四、使用Flume导入数据到HDFS"><a href="#十四、使用Flume导入数据到HDFS" class="headerlink" title="十四、使用Flume导入数据到HDFS"></a>十四、使用Flume导入数据到HDFS</h1><p>数据导出到HDFS需要使用HDFS Sink，需要配置属性如下：</p>
<table>
<thead>
<tr>
<th align="left">属性名</th>
<th align="left">默认值</th>
<th align="left">说明</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>channel</strong></td>
<td align="left">–</td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>type</strong></td>
<td align="left">–</td>
<td align="left"><code>hdfs</code></td>
</tr>
<tr>
<td align="left"><strong>hdfs.path</strong></td>
<td align="left">–</td>
<td align="left">HDFS 文件路径 (例如 hdfs:&#x2F;&#x2F;namenode&#x2F;flume&#x2F;webdata&#x2F;)</td>
</tr>
<tr>
<td align="left">hdfs.fileType</td>
<td align="left">SequenceFile</td>
<td align="left">文件格式:  <code>SequenceFile</code>, <code>DataStream</code> or <code>CompressedStream</code> (1)DataStream 不会压缩输出文件且不用设置 codeC (2)CompressedStream 需要设置 hdfs.codeC</td>
</tr>
<tr>
<td align="left">hdfs.codeC</td>
<td align="left"></td>
<td align="left">压缩格式 : gzip, bzip2, lzo, lzop, snappy</td>
</tr>
</tbody></table>
<p>注：使用HDFS Sink需要用到Hadoop的多个包，可以在装有Hadoop的主机上运行Flume，如果是单独部署的Flume，可以通过多个Agent的形式将单独部署的Flume Agent 日志数据发送到装有Hadoop的Flume Agent上。</p>
<p>创建hdfs.conf </p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义agent名称为a1</span></span><br><span class="line"><span class="comment"># 设置3个组件的名称</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置source类型为NetCat,监听地址为本机，端口为44444</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">netcat</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = <span class="string">localhost</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置sink类型为hdfs</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.path</span> = <span class="string">hdfs://node01:9000/user/flume/logs</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileType</span> = <span class="string">DataStream</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 配置channel类型为内存，内存队列最大容量为1000，一个事务中从source接收的Events数量或者发送给sink的Events数量最大为100</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># 将source和sink绑定到channel上</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>

<p>启动flume</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent --conf conf/ --conf-file conf/hdfs.conf -Dfile.root.logger=debug,info,console --name hdfs</span><br></pre></td></tr></table></figure>

<p>注：如果出现<code>com.google.common.base.Preconditions.checkArgument</code> 查看下<code>flume/lib</code>目录下</p>
<p>的<code>guava.jar</code>版本是否与<code>hadoop/share/hadoop/common/lib</code>中的版本是否一致，不一致拷贝新版</p>
<p>重新运行</p>
<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200112212041558.png" alt="image-20200112212041558"></p>
<p>在后台查看</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat /user/flume/messages/flume-.1578835130630</span><br></pre></td></tr></table></figure>



<p><img src="/md%E5%9B%BE%5Cflume.assets/image-20200112212150353.png" alt="image-20200112212150353"></p>
<h1 id="十五、Flume-SDK"><a href="#十五、Flume-SDK" class="headerlink" title="十五、Flume SDK"></a>十五、Flume SDK</h1><h2 id="（1）自定义Source"><a href="#（1）自定义Source" class="headerlink" title="（1）自定义Source"></a>（1）自定义Source</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MySource</span> <span class="keyword">extends</span> <span class="title class_">AbstractSource</span> <span class="keyword">implements</span> <span class="title class_">Configurable</span>, PollableSource &#123;</span><br><span class="line">  <span class="keyword">private</span> String myProp;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line">    <span class="type">String</span> <span class="variable">myProp</span> <span class="operator">=</span> context.getString(<span class="string">&quot;myProp&quot;</span>, <span class="string">&quot;defaultValue&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Process the myProp value (e.g. validation, convert to another type, ...)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Store myProp for later retrieval by process() method</span></span><br><span class="line">    <span class="built_in">this</span>.myProp = myProp;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">start</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// Initialize the connection to the external client</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">stop</span> <span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// Disconnect from external client and do any additional cleanup</span></span><br><span class="line">    <span class="comment">// (e.g. releasing resources or nulling-out field values) ..</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> Status <span class="title function_">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException &#123;</span><br><span class="line">    <span class="type">Status</span> <span class="variable">status</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// This try clause includes whatever Channel/Event operations you want to do</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// Receive new data</span></span><br><span class="line">      <span class="type">Event</span> <span class="variable">e</span> <span class="operator">=</span> getSomeData();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Store the Event into this Source&#x27;s associated Channel(s)</span></span><br><span class="line">      getChannelProcessor().processEvent(e);</span><br><span class="line"></span><br><span class="line">      status = Status.READY;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">      <span class="comment">// Log exception, handle individual exceptions as needed</span></span><br><span class="line"></span><br><span class="line">      status = Status.BACKOFF;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// re-throw all Errors</span></span><br><span class="line">      <span class="keyword">if</span> (t <span class="keyword">instanceof</span> Error) &#123;</span><br><span class="line">        <span class="keyword">throw</span> (Error)t;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      txn.close();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> status;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="添加MySource"><a href="#添加MySource" class="headerlink" title="添加MySource"></a>添加MySource</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.itheima.flume.source;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Context;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.Event;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.EventDeliveryException;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.PollableSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.event.SimpleEvent;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.source.AbstractSource;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MySource</span> <span class="keyword">extends</span> <span class="title class_">AbstractSource</span> <span class="keyword">implements</span> <span class="title class_">Configurable</span>, PollableSource &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 处理数据</span></span><br><span class="line">    <span class="keyword">public</span> Status <span class="title function_">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException &#123;</span><br><span class="line">        <span class="type">Status</span> <span class="variable">status</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 接收新数据</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">                <span class="type">Event</span> <span class="variable">e</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SimpleEvent</span>();</span><br><span class="line">                e.setBody((<span class="string">&quot;data:&quot;</span>+i).getBytes());</span><br><span class="line">                <span class="comment">// 将数据存储到与Source关联的Channel中</span></span><br><span class="line">                getChannelProcessor().processEvent(e);</span><br><span class="line">                status = Status.READY;</span><br><span class="line">            &#125;</span><br><span class="line">            Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">            <span class="comment">// 打印日志</span></span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line">            <span class="comment">// 抛出异常</span></span><br><span class="line">            <span class="keyword">if</span> (t <span class="keyword">instanceof</span> Error) &#123;</span><br><span class="line">                <span class="keyword">throw</span> (Error)t;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getBackOffSleepIncrement</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="type">long</span> <span class="title function_">getMaxBackOffSleepInterval</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="添加mySourceAgent-conf"><a href="#添加mySourceAgent-conf" class="headerlink" title="添加mySourceAgent.conf"></a>添加mySourceAgent.conf</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">定义agent名称为a1</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置3个组件的名称</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置<span class="built_in">source</span>类型为mysource</span></span><br><span class="line">a1.sources.r1.type = com.itheima.flume.source.MySource</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置sink类型为Logger</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置channel类型为内存，内存队列最大容量为1000，一个事务中从<span class="built_in">source</span>接收的Events数量或者发送给sink的Events数量最大为100</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将<span class="built_in">source</span>和sink绑定到channel上</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<h3 id="启动Flume"><a href="#启动Flume" class="headerlink" title="启动Flume"></a>启动Flume</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent -n a1 -c conf -f mySourceAgent.conf</span><br></pre></td></tr></table></figure>



<h2 id="（2）自定义Sink"><a href="#（2）自定义Sink" class="headerlink" title="（2）自定义Sink"></a>（2）自定义Sink</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MySink</span> <span class="keyword">extends</span> <span class="title class_">AbstractSink</span> <span class="keyword">implements</span> <span class="title class_">Configurable</span> &#123;</span><br><span class="line">  <span class="keyword">private</span> String myProp;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line">    <span class="type">String</span> <span class="variable">myProp</span> <span class="operator">=</span> context.getString(<span class="string">&quot;myProp&quot;</span>, <span class="string">&quot;defaultValue&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Process the myProp value (e.g. validation)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Store myProp for later retrieval by process() method</span></span><br><span class="line">    <span class="built_in">this</span>.myProp = myProp;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">start</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// Initialize the connection to the external repository (e.g. HDFS) that</span></span><br><span class="line">    <span class="comment">// this Sink will forward Events to ..</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">stop</span> <span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">// Disconnect from the external respository and do any</span></span><br><span class="line">    <span class="comment">// additional cleanup (e.g. releasing resources or nulling-out</span></span><br><span class="line">    <span class="comment">// field values) ..</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="keyword">public</span> Status <span class="title function_">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException &#123;</span><br><span class="line">    <span class="type">Status</span> <span class="variable">status</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Start transaction</span></span><br><span class="line">    <span class="type">Channel</span> <span class="variable">ch</span> <span class="operator">=</span> getChannel();</span><br><span class="line">    <span class="type">Transaction</span> <span class="variable">txn</span> <span class="operator">=</span> ch.getTransaction();</span><br><span class="line">    txn.begin();</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// This try clause includes whatever Channel operations you want to do</span></span><br><span class="line"></span><br><span class="line">      <span class="type">Event</span> <span class="variable">event</span> <span class="operator">=</span> ch.take();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Send the Event to the external repository.</span></span><br><span class="line">      <span class="comment">// storeSomeData(e);</span></span><br><span class="line"></span><br><span class="line">      txn.commit();</span><br><span class="line">      status = Status.READY;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">      txn.rollback();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Log exception, handle individual exceptions as needed</span></span><br><span class="line"></span><br><span class="line">      status = Status.BACKOFF;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// re-throw all Errors</span></span><br><span class="line">      <span class="keyword">if</span> (t <span class="keyword">instanceof</span> Error) &#123;</span><br><span class="line">        <span class="keyword">throw</span> (Error)t;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> status;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="添加MySink，可以参考LoggerSink"><a href="#添加MySink，可以参考LoggerSink" class="headerlink" title="添加MySink，可以参考LoggerSink"></a>添加MySink，可以参考LoggerSink</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.itheima.flume.sink;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flume.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.conf.Configurable;</span><br><span class="line"><span class="keyword">import</span> org.apache.flume.sink.AbstractSink;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MySink</span> <span class="keyword">extends</span> <span class="title class_">AbstractSink</span> <span class="keyword">implements</span> <span class="title class_">Configurable</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">Logger</span> <span class="variable">logger</span> <span class="operator">=</span> LoggerFactory</span><br><span class="line">            .getLogger(MySink.class);</span><br><span class="line">    <span class="keyword">public</span> Status <span class="title function_">process</span><span class="params">()</span> <span class="keyword">throws</span> EventDeliveryException &#123;</span><br><span class="line">        <span class="type">Status</span> <span class="variable">status</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="comment">// 开启事务</span></span><br><span class="line">        <span class="type">Channel</span> <span class="variable">ch</span> <span class="operator">=</span> getChannel();</span><br><span class="line">        <span class="type">Transaction</span> <span class="variable">txn</span> <span class="operator">=</span> ch.getTransaction();</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            txn.begin();</span><br><span class="line">            <span class="comment">// 从channel中获取数据</span></span><br><span class="line">            <span class="type">Event</span> <span class="variable">event</span> <span class="operator">=</span> ch.take();</span><br><span class="line">            <span class="keyword">if</span>(event==<span class="literal">null</span>)&#123;</span><br><span class="line">                status = Status.BACKOFF;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 将事件发送到外部存储</span></span><br><span class="line">            <span class="comment">// storeSomeData(e);</span></span><br><span class="line">            <span class="comment">// 打印事件</span></span><br><span class="line">            logger.info(<span class="keyword">new</span> <span class="title class_">String</span>(event.getBody()));</span><br><span class="line">            <span class="comment">// 提交事务</span></span><br><span class="line">            txn.commit();</span><br><span class="line">            status = Status.READY;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Throwable t) &#123;</span><br><span class="line">            txn.rollback();</span><br><span class="line">            <span class="comment">// 打印异常日志</span></span><br><span class="line">            status = Status.BACKOFF;</span><br><span class="line">            <span class="comment">// 抛出异常</span></span><br><span class="line">            <span class="keyword">if</span> (t <span class="keyword">instanceof</span> Error) &#123;</span><br><span class="line">                <span class="keyword">throw</span> (Error)t;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">            txn.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> status;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Context context)</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="修改上面的mySourceAgent-conf"><a href="#修改上面的mySourceAgent-conf" class="headerlink" title="修改上面的mySourceAgent.conf"></a>修改上面的mySourceAgent.conf</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">定义agent名称为a1</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置3个组件的名称</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置<span class="built_in">source</span>类型为mysource</span></span><br><span class="line">a1.sources.r1.type = com.itheima.flume.source.MySource</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置sink类型为MySink</span></span><br><span class="line">a1.sinks.k1.type = com.itheima.flume.sink.MySink</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置channel类型为内存，内存队列最大容量为1000，一个事务中从<span class="built_in">source</span>接收的Events数量或者发送给sink的Events数量最大为100</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将<span class="built_in">source</span>和sink绑定到channel上</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>



<h1 id="十六、Flume-综合实战案例"><a href="#十六、Flume-综合实战案例" class="headerlink" title="十六、Flume 综合实战案例"></a>十六、Flume 综合实战案例</h1><h2 id="（1）案例场景"><a href="#（1）案例场景" class="headerlink" title="（1）案例场景"></a>（1）案例场景</h2><p>A、B、C等日志服务机器实时生产日志，日志的内容分为多种类型：</p>
<p>  APP端用户行为日志</p>
<p>  微信小程序端用户行为日志</p>
<p>  PC端用户行为日志</p>
<h3 id="需求："><a href="#需求：" class="headerlink" title="需求："></a><strong>需求：</strong></h3><p>把日志服务器中的各类日志数据采集汇总到一个中转agent上，然后分类写入hdfs中。</p>
<p>在hdfs中要求的目录为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/source/logs/app_log/20160101/**</span><br><span class="line"></span><br><span class="line">/source/logs/wxapp_log/20160101/**</span><br><span class="line"></span><br><span class="line">/source/logs/pcweb_log/20160101/**</span><br></pre></td></tr></table></figure>

<p><em><strong>并要求可以按指定的字段名，将对应字段内容加密！</strong></em></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230130235444507.png" alt="image-20230130235444507"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">linux命令操作替换jar包中的文件：</span><br><span class="line"></span><br><span class="line">显示jar中的文件列表：  jar -tvf xx.jar</span><br><span class="line"></span><br><span class="line">解压jar中的指定文件：  jar -xvf xx.jar yy.properties</span><br><span class="line"></span><br><span class="line">更新jar中的指定文件：  jar -uvf xx.jar yy.properties</span><br></pre></td></tr></table></figure>

<h2 id="（2）实现思路"><a href="#（2）实现思路" class="headerlink" title="（2）实现思路"></a>（2）实现思路</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1. 每台日志服务器上部署一个flume agent - - -&gt; 上游，每个agent配置2个source对应2类数据</span><br><span class="line"></span><br><span class="line">2. 上游的agent在采集数据时，添加一个header，指定数据的类别</span><br><span class="line"></span><br><span class="line">3. 上游的agent要配置两个avro sink，各自对接一个下级的agent</span><br><span class="line"></span><br><span class="line">4. 上游还要配置sink processor，fail over sink processor，控制两个sink中只有一个avro sink在工作，如果失败再切换到另一个avro sink</span><br><span class="line"></span><br><span class="line">5. 上游还要配置字段加密拦截器</span><br><span class="line"></span><br><span class="line">6. 下游配置两个flume agent，使用avro source接收数据</span><br><span class="line"></span><br><span class="line">7. 下游的hdfs sink，目录配置使用动态通配符，取到event中的类别header，以便于将不同类别数据写入不同hdfs目录 </span><br></pre></td></tr></table></figure>

<h2 id="（3）操作步骤"><a href="#（3）操作步骤" class="headerlink" title="（3）操作步骤"></a>（3）操作步骤</h2><h3 id="1）部署行为日志生产模拟器"><a href="#1）部署行为日志生产模拟器" class="headerlink" title="1）部署行为日志生产模拟器"></a>1）部署行为日志生产模拟器</h3><p><strong>1.准备一个mysql服务器，并创建一个库：realtimedw</strong></p>
<img src=".\md图\flume.assets\image-20230201000514636.png" alt="image-20230201000514636" style="zoom:80%;" />

<p><strong>2.将realtimedw.sql这个脚本，导入到realtimedw库中</strong></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230201005947135.png" alt="image-20230201005947135"></p>
<p><strong>3.将t_md_areas.sql这个脚本，导入到realtimedw库中</strong></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230201010005829.png" alt="image-20230201010005829"></p>
<p><strong>4.准备其他数据文件目录</strong></p>
<p>新建目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/data/flume-example-data/loginit</span><br></pre></td></tr></table></figure>

<img src=".\md图\flume.assets\image-20230201010138833.png" alt="image-20230201010138833" style="zoom:80%;" />

<p><strong>5.修改jar包中的配置文件</strong></p>
<p><strong>log_gen_app.jar包中的配置文件修改</strong></p>
<ul>
<li>other.properties</li>
</ul>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#logger,kafka</span></span><br><span class="line"><span class="attr">sink.type</span>=<span class="string">logger</span></span><br><span class="line"><span class="comment">#roll console dayroll</span></span><br><span class="line"><span class="attr">logger.type</span>=<span class="string">dayroll</span></span><br><span class="line"></span><br><span class="line"><span class="attr">initdata.releasechannel</span>=<span class="string">/export/data/flume-example-data/loginit/releasechannel.txt</span></span><br><span class="line"><span class="attr">initdata.phoneinfo</span>=<span class="string">/export/data/flume-example-data/loginit/phoneinfo.txt</span></span><br><span class="line"><span class="attr">initdata.eventIds</span>=<span class="string">/export/data/flume-example-data/loginit/eventIds.txt</span></span><br><span class="line"><span class="attr">init.user.area</span>=<span class="string">/export/data/flume-example-data/loginit/area.txt</span></span><br><span class="line"></span><br><span class="line"><span class="attr">db.url</span>=<span class="string">jdbc:mysql://127.0.0.1:3306/realtimedw?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=false</span></span><br><span class="line"><span class="attr">db.user</span>=<span class="string">root</span></span><br><span class="line"><span class="attr">db.password</span>=<span class="string">hadoop</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># max concurrent accessor amount</span></span><br><span class="line"><span class="attr">online.max.num</span>=<span class="string">1000</span></span><br></pre></td></tr></table></figure>

<ul>
<li>log4j.properties</li>
</ul>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">log4j.rootLogger</span>=<span class="string">INFO,trace</span></span><br><span class="line"></span><br><span class="line"><span class="attr">log4j.appender.trace</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="attr">log4j.appender.trace.Threshold</span>=<span class="string">DEBUG</span></span><br><span class="line"><span class="attr">log4j.appender.trace.ImmediateFlush</span>=<span class="string">true</span></span><br><span class="line"><span class="attr">log4j.appender.trace.Target</span>=<span class="string">System.out</span></span><br><span class="line"><span class="attr">log4j.appender.trace.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="attr">log4j.appender.trace.layout.ConversionPattern</span>=<span class="string">[%-5p] %d(%r) --&gt; [%t] %l: %m %x %n</span></span><br><span class="line"></span><br><span class="line"><span class="attr">log4j.logger.console</span> = <span class="string">INFO,console</span></span><br><span class="line"><span class="attr">log4j.additivity.console</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">log4j.appender.console</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="attr">log4j.appender.console.Threshold</span>=<span class="string">DEBUG</span></span><br><span class="line"><span class="attr">log4j.appender.console.ImmediateFlush</span>=<span class="string">true</span></span><br><span class="line"><span class="attr">log4j.appender.console.Target</span>=<span class="string">System.out</span></span><br><span class="line"><span class="attr">log4j.appender.console.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="attr">log4j.appender.console.layout.ConversionPattern</span>=<span class="string">%m%n</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># log4j.logger.roll = INFO,rollingFile</span></span><br><span class="line"><span class="comment"># log4j.additivity.roll=false</span></span><br><span class="line"><span class="comment"># log4j.appender.rollingFile=org.apache.log4j.RollingFileAppender</span></span><br><span class="line"><span class="comment"># log4j.appender.rollingFile.Threshold=DEBUG</span></span><br><span class="line"><span class="comment"># log4j.appender.rollingFile.ImmediateFlush=true</span></span><br><span class="line"><span class="comment"># log4j.appender.rollingFile.Append=true</span></span><br><span class="line"><span class="comment"># log4j.appender.rollingFile.File=/loggen/logdata/wx/event.log</span></span><br><span class="line"><span class="comment"># log4j.appender.rollingFile.MaxFileSize=120MB</span></span><br><span class="line"><span class="comment"># log4j.appender.rollingFile.MaxBackupIndex=50</span></span><br><span class="line"><span class="comment"># log4j.appender.rollingFile.layout=org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="comment"># log4j.appender.rollingFile.layout.ConversionPattern=%m%n</span></span><br><span class="line"></span><br><span class="line"><span class="attr">log4j.logger.dayroll</span> = <span class="string">INFO,DailyRolling</span></span><br><span class="line"><span class="attr">log4j.additivity.dayroll</span>=<span class="string">false</span></span><br><span class="line"><span class="attr">log4j.appender.DailyRolling</span>=<span class="string">org.apache.log4j.DailyRollingFileAppender</span></span><br><span class="line"><span class="attr">log4j.appender.DailyRolling.File</span>=<span class="string">/export/data/flume-example-data/gen_logdata/event_log_app</span></span><br><span class="line"><span class="attr">log4j.appender.DailyRolling.DatePattern</span>=<span class="string">yyyy-MM-dd&#x27;.log&#x27;</span></span><br><span class="line"><span class="attr">log4j.appender.DailyRolling.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="attr">log4j.appender.DailyRolling.layout.ConversionPattern</span>=<span class="string">%m%n</span></span><br></pre></td></tr></table></figure>

<p><strong>log_gen_wx.jar包中的配置文件修改</strong></p>
<ul>
<li><p>other.properties与上文一致</p>
</li>
<li><p>log4j.properties 其中一行修改为</p>
</li>
</ul>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">log4j.appender.DailyRolling.File</span>=<span class="string">/export/data/flume-example-data/gen_logdata/event_log_wx</span></span><br></pre></td></tr></table></figure>

<p><strong>6.上传jar包，执行启动命令</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">在/export/data/flume-example-data/loginit 中添加shell文件用以启动日志生成器</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim genapplog.sh</span><br><span class="line"></span><br><span class="line">java -Xss102400k -<span class="built_in">cp</span> log_gen_app.jar cn.doitedu.loggen.entry.GenAppLog 1 &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim genwxlog.sh</span><br><span class="line"></span><br><span class="line">java -Xss102400k -<span class="built_in">cp</span> log_gen_wx.jar cn.doitedu.loggen.entry.GenWxAppLog 1 &gt; /dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sh genapplog.sh</span><br><span class="line"></span><br><span class="line">sh genwxlog.sh</span><br></pre></td></tr></table></figure>

<img src=".\md图\flume.assets\image-20230201010312246.png" alt="image-20230201010312246" style="zoom: 67%;" />

<p><strong>7.查看日志文件生成效果</strong></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230201010531192.png" alt="image-20230201010531192"></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230201010622186.png" alt="image-20230201010622186"></p>
<h3 id="2）上游配置文件：-node1-node2"><a href="#2）上游配置文件：-node1-node2" class="headerlink" title="2）上游配置文件：(node1 node2)"></a>2）上游配置文件：(node1 node2)</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example14-1-Comprehensive-practical.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1 k2</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">TAILDIR</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.positionFile</span> = <span class="string">/export/data/flume-example-data/flumedata/taildir_position.json</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups</span> = <span class="string">g1</span></span><br><span class="line"><span class="attr">a1.sources.r1.filegroups.g1</span> =  <span class="string">/export/data/flume-example-data/gen_logdata/event_.*</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors</span> = <span class="string">i1 i2 i3</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i1.type</span> = <span class="string">ccjz.rgzn.flume.EncryptSpecifiedFieldInterceptor$EncryptInterceptorBuilder</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i1.toEncryFieldName</span> = <span class="string">account</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i2.type</span> = <span class="string">ccjz.rgzn.flume.EventTimeStampExtractInterceptor$EventTimestampInterceptorBuilder</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i2.tsFiledName</span> = <span class="string">timeStamp</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i2.keyName</span> = <span class="string">timestamp</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#拥有openid的是wx小程序用户日志</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i3.type</span> = <span class="string">ccjz.rgzn.flume.LogTypeInterceptor$LogTypeInterceptorBuilder</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i3.flag.fieldname</span> = <span class="string">openid</span></span><br><span class="line"><span class="attr">a1.sources.r1.interceptors.i3.headerKey</span> = <span class="string">logtype</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">file</span></span><br><span class="line"><span class="attr">a1.channels.c1.checkpointDir</span> =  <span class="string">/export/data/flume-example-data/flumedata/checkpoint</span></span><br><span class="line"><span class="attr">a1.channels.c1.dataDirs</span> =  <span class="string">/export/data/flume-example-data/flumedata/data</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">2000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hostname</span> = <span class="string">node2</span></span><br><span class="line"><span class="attr">a1.sinks.k1.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="attr">a1.sinks.k1.batch-size</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k2.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k2.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sinks.k2.hostname</span> = <span class="string">node3</span></span><br><span class="line"><span class="attr">a1.sinks.k2.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="attr">a1.sinks.k2.batch-size</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinkgroups</span> = <span class="string">g1</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.sinks</span> = <span class="string">k1 k2</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.type</span> = <span class="string">failover</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.priority.k1</span> = <span class="string">200</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.priority.k2</span> = <span class="string">100</span></span><br><span class="line"><span class="attr">a1.sinkgroups.g1.processor.maxpenalty</span> = <span class="string">5000</span></span><br></pre></td></tr></table></figure>

<h3 id="3）下游配置文件-node2-node3"><a href="#3）下游配置文件-node2-node3" class="headerlink" title="3）下游配置文件(node2 node3)"></a>3）下游配置文件(node2 node3)</h3><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example14-2-Comprehensive-practical.conf</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.bind</span> = <span class="string">0.0.0.0</span></span><br><span class="line"><span class="attr">a1.sources.r1.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="attr">a1.sources.r1.threads</span> = <span class="string">10</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">file</span></span><br><span class="line"><span class="attr">a1.channels.c1.checkpointDir</span> =  <span class="string">/export/data/flume-example-data/flumedata_2/checkpoint</span></span><br><span class="line"><span class="attr">a1.channels.c1.dataDirs</span> =  <span class="string">/export/data/flume-example-data/flumedata_2/data</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">2000</span></span><br><span class="line"></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.path</span> = <span class="string">hdfs://node1:8020/gen_logdata/%&#123;logtype&#125;/%Y-%m-%d/</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.filePrefix</span> = <span class="string">logdata_</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileSuffix</span> = <span class="string">.log</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollInterval</span> = <span class="string">300</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollSize</span> = <span class="string">268435456</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollCount</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.codeC</span> = <span class="string">gzip</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileType</span> = <span class="string">CompressedStream</span></span><br></pre></td></tr></table></figure>



<h2 id="（4）启动测试"><a href="#（4）启动测试" class="headerlink" title="（4）启动测试"></a>（4）启动测试</h2><ol>
<li><p>先把自定义拦截器代码jar包放入上游（node1）flume的lib目录中；</p>
</li>
<li><p>将各台机器上之前的一些checkpoint、缓存等目录清除；</p>
</li>
<li><p>启动下游的两个agent（node2、node3上）；</p>
</li>
<li><p>在上游的机器上，创建日志数据目录，并写脚本模拟往3类日志中写入日志：</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Cflume.assets%5Cimage-20230131000318670.png" alt="image-20230131000318670"></p>
</li>
<li><p>在上游的所有机器上启动flume agent</p>
</li>
<li><p>到hdfs上观察结果</p>
</li>
<li><p>尝试kill掉下游node2的 agent，看是否能够故障切换</p>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/19/flume/" data-id="clj25kfyf000an0urhlb71afb" data-title="Flume" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Docker实用篇" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/19/Docker%E5%AE%9E%E7%94%A8%E7%AF%87/" class="article-date">
  <time class="dt-published" datetime="2023-06-19T00:49:49.439Z" itemprop="datePublished">2023-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/19/Docker%E5%AE%9E%E7%94%A8%E7%AF%87/">Docker</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Docker实用篇"><a href="#Docker实用篇" class="headerlink" title="Docker实用篇"></a>Docker实用篇</h1><h1 id="1-初识Docker"><a href="#1-初识Docker" class="headerlink" title="1.初识Docker"></a>1.初识Docker</h1><h2 id="1-1-什么是Docker"><a href="#1-1-什么是Docker" class="headerlink" title="1.1.什么是Docker"></a>1.1.什么是Docker</h2><p><code>微服务</code>虽然具备各种各样的优势，但<code>服务的拆分通用</code>给<code>部署</code>带来了很大的麻烦。</p>
<blockquote>
<p><code>微服务</code>是一种<code>基于服务架构</code>的<code>软件开发模式</code>，它将应用程序拆分成一组<code>小型、自治的服务</code>，每个服务只负责完成特定的业务功能。每个服务都可以<code>独立部署、升级、扩展和替换</code>，服务之间通过轻量级的通信机制进行通信，如REST（Representational State Transfer）、RPC（Remote Procedure Call）等。</p>
<p>微服务的目标是提高应用程序的可伸缩性、可维护性和可重用性，使团队能够更快地开发、测试和部署应用程序。微服务架构还可以帮助企业实现更快的创新和更好的业务灵活性，因为它可以让企业更容易地在不同的技术栈之间切换，更容易地实现新功能和新服务。</p>
</blockquote>
<ul>
<li>分布式系统中，依赖的组件非常多，<code>不同组件之间部署时往往会产生一些冲突</code>。</li>
<li><code>在数百上千台服务中重复部署，环境不一定一致，会遇到各种问题</code></li>
</ul>
<img src=".\assets\image-20230204231611871.png" alt="image-20230204231611871" style="zoom:80%;" />

<p><img src="/.%5Cassets%5Cimage-20230203163959671.png" alt="image-20230203163959671"></p>
<ul>
<li>Docker 是一个开源的<code>应用容器引擎</code></li>
<li>诞生于2013 年初，基于<code>Go 语言</code>实现，dotCloud 公司出品（后改名为Docker Inc）</li>
<li><strong>Docker 可以让开发者<code>打包</code>他们的<code>应用以及依赖</code>包到一个<code>轻量级、可移植的容器</code>中，然后<code>发布到任何流行的Linux 机器上</code>。</strong></li>
<li>容器是完全使用<code>沙箱机制，相互隔离</code></li>
<li>容器<code>性能开销极低</code>。</li>
<li>Docker 从<code>17.03 版本</code>之后分为<code>CE（Community Edition: 社区版）</code>和<code>EE（Enterprise Edition: 企业版）</code></li>
</ul>
<h3 id="1-1-1-应用部署的环境问题"><a href="#1-1-1-应用部署的环境问题" class="headerlink" title="1.1.1.应用部署的环境问题"></a>1.1.1.应用部署的环境问题</h3><p>大型项目组件较多，运行环境也较为复杂，部署时会碰到一些问题：</p>
<ul>
<li><p>依赖关系复杂，容易出现兼容性问题</p>
</li>
<li><p>开发、测试、生产环境有差异</p>
</li>
</ul>
<img src="assets/image-20210731141907366.png" alt="image-20210731141907366"  />

<p>例如一个项目中，部署时需要依赖于node.js、Redis、RabbitMQ、MySQL等，这些服务部署时所需要的函数库、依赖项各不相同，甚至会有冲突。给部署带来了极大的困难。</p>
<p><strong>软件跨环境迁移的问题</strong></p>
<p><img src="/.%5Cassets%5Cimage-20230203162944320.png" alt="image-20230203162944320"></p>
<p><img src="/.%5Cassets%5Cimage-20230204232834835.png" alt="image-20230204232834835"></p>
<h3 id="1-1-2-Docker解决依赖兼容问题"><a href="#1-1-2-Docker解决依赖兼容问题" class="headerlink" title="1.1.2.Docker解决依赖兼容问题"></a>1.1.2.Docker解决依赖兼容问题</h3><p>而Docker确巧妙的解决了这些问题，Docker是如何实现的呢？</p>
<p>Docker为了<code>解决依赖的兼容问题</code>的，采用了<code>两个手段</code>：</p>
<ul>
<li><p>将应用的<code>Libs（函数库）</code>、<code>Deps（依赖）</code>、<code>配置</code>与<code>应用一起打包</code></p>
</li>
<li><p>将每个应用放到一个隔离**&#x3D;&#x3D;容器&#x3D;&#x3D;**去运行，<code>避免互相干扰</code></p>
</li>
</ul>
<p><img src="/assets/image-20210731142219735.png" alt="image-20210731142219735"></p>
<p>这样<code>打包好的应用包中</code>，<code>既包含应用本身</code>，也保护应用<code>所需要的Libs、Deps</code>，无需再在操作系统上安装这些，自然就<code>不存在不同应用之间的兼容问题了</code>。</p>
<p>虽然解决了不同应用的兼容问题，但是<code>开发、测试等环境会存在差异</code>，<code>操作系统版本也会有差异</code>，怎么解决这些问题呢？</p>
<h3 id="1-1-3-Docker解决操作系统环境差异"><a href="#1-1-3-Docker解决操作系统环境差异" class="headerlink" title="1.1.3.Docker解决操作系统环境差异"></a>1.1.3.Docker解决操作系统环境差异</h3><p>要解决不同操作系统环境差异问题，必须先了解操作系统结构。以一个Ubuntu操作系统为例，结构如下：</p>
<p><img src="/assets/image-20210731143401460.png" alt="image-20210731143401460"></p>
<p>结构包括：</p>
<ul>
<li><code>计算机硬件</code>：例如CPU、内存、磁盘等</li>
<li><code>系统内核</code>：所有<code>Linux发行版的内核都是Linux</code>，例如<code>CentOS、Ubuntu、Fedora等</code>。内核可以与计算机硬件交互，对外提供&#x3D;&#x3D;<strong>内核指令</strong>&#x3D;&#x3D;，用于操作计算机硬件。</li>
<li><code>系统应用</code>：操作系统本身提供的应用、函数库。这些函数库是对内核指令的封装，使用更加方便。</li>
</ul>
<p>应用于计算机交互的流程如下：</p>
<p>1）<code>应用</code>&#x3D;&#x3D;调用&#x3D;&#x3D;<code>操作系统应用（函数库）</code>，实现各种功能</p>
<p>2）<code>系统函数库</code>是对<code>内核指令集的封装</code>，会&#x3D;&#x3D;调用&#x3D;&#x3D;<code>内核指令</code></p>
<p>3）<code>内核指令</code>&#x3D;&#x3D;操作&#x3D;&#x3D;<code>计算机硬件</code></p>
<p>Ubuntu和CentOS都是基于Linux内核，无非是<code>系统应用不同</code>，提供的函数库有差异：</p>
<img src="assets/image-20210731144304990.png" alt="image-20210731144304990"  />



<p>此时，如果<code>将一个Ubuntu版本</code>的<code>MySQL应用安装到CentOS系统</code>，<code>MySQL在调用Ubuntu函数库时</code>，会发现<code>找不到或者不匹配</code>，就会报错了：</p>
<img src="assets/image-20210731144458680.png" alt="image-20210731144458680"  />



<p>Docker如何解决不同系统环境的问题？</p>
<ul>
<li>Docker将<code>用户程序</code>与<code>所需要调用的系统(比如Ubuntu)函数库</code>&#x3D;&#x3D;一起打包&#x3D;&#x3D;</li>
<li>Docker<code>运行到不同操作系统时</code>，直接<code>基于打包的函数库</code>，借助于操作系统的Linux内核来运行</li>
</ul>
<p>如图：</p>
<img src="assets/image-20210731144820638.png" alt="image-20210731144820638"  />



<h3 id="1-1-4-小结"><a href="#1-1-4-小结" class="headerlink" title="1.1.4.小结"></a>1.1.4.小结</h3><p>Docker如何解决大型项目依赖关系复杂，不同组件依赖的兼容性问题？</p>
<ul>
<li>Docker允许开发中将应用、依赖、函数库、配置一起<strong>打包</strong>，形成可移植镜像</li>
<li>Docker应用运行在容器中，使用沙箱机制，相互<strong>隔离</strong></li>
</ul>
<p>Docker如何解决开发、测试、生产环境有差异的问题？</p>
<ul>
<li>Docker镜像中包含完整运行环境，包括系统函数库，仅依赖系统的Linux内核，因此可以在任意Linux操作系统上运行</li>
</ul>
<p>Docker是一个快速交付应用、运行应用的技术，具备下列优势：</p>
<ul>
<li>可以将程序及其依赖、运行环境一起打包为一个镜像，可以迁移到任意Linux操作系统</li>
<li>运行时利用沙箱机制形成隔离容器，各个应用互不干扰</li>
<li>启动、移除都可以通过一行命令完成，方便快捷</li>
</ul>
<h2 id="1-2-Docker和虚拟机的区别"><a href="#1-2-Docker和虚拟机的区别" class="headerlink" title="1.2.Docker和虚拟机的区别"></a>1.2.Docker和虚拟机的区别</h2><p>Docker可以让一个<code>应用在任何操作系统中非常方便的运行</code>。而以前我们接触的虚拟机，也能在一个操作系统中，运行另外一个操作系统，保护系统中的任何应用。</p>
<p><code>两者有什么差异呢？</code></p>
<p><strong><code>虚拟机</code><strong>（virtual machine）是在操作系统中</strong><code>模拟</code></strong><code>硬件设备</code>，然后运行另一个操作系统，比如在 Windows 系统里面运行 Ubuntu 系统，这样就可以运行任意的Ubuntu应用了。</p>
<p>**<code>Docker</code>**仅仅是封装函数库，<code>并没有模拟完整的操作系统</code>，如图：</p>
<img src="assets/image-20210731145914960.png" alt="image-20210731145914960" style="zoom: 25%;" />

<p>对比来看：</p>
<img src=".\assets\image-20230204232020530.png" alt="image-20230204232020530" style="zoom: 67%;" />



<p>小结：</p>
<p>Docker和虚拟机的差异：</p>
<ul>
<li><p>docker是一个系统进程；虚拟机是在操作系统中的操作系统</p>
</li>
<li><p>docker体积小、启动速度快、性能好；虚拟机体积大、启动速度慢、性能一般</p>
</li>
</ul>
<h2 id="1-3-Docker架构"><a href="#1-3-Docker架构" class="headerlink" title="1.3.Docker架构"></a>1.3.Docker架构</h2><h3 id="1-3-1-镜像和容器"><a href="#1-3-1-镜像和容器" class="headerlink" title="1.3.1.镜像和容器"></a>1.3.1.镜像和容器</h3><img src=".\assets\image-20230203164559342.png" alt="image-20230203164559342" style="zoom:80%;" />

<p>Docker中有几个重要的概念：</p>
<ul>
<li>**<code>镜像（Image）</code>**：Docker<code>将应用程序及其所需的依赖、函数库、环境、配置等文件打包在一起</code>，称为镜像。就相当于是一个root 文件系统。比如官方镜像ubuntu:16.04 就包含了完整的一套Ubuntu16.04 最小系统的root 文件系统。</li>
<li>**<code>容器（Container）</code><strong>：<code>镜像中的应用程序运行后形成的进程就是</code></strong><code>容器</code>**，只是Docker会给容器进程做隔离，对外不可见。镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的类和对象一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。</li>
<li>**<code>仓库（Repository）</code>**：仓库可看成一个<code>代码控制中心</code>，用来保存镜像。</li>
</ul>
<p>一切应用最终都是代码组成，都是硬盘中的一个个的字节形成的<strong>文件</strong>。只有运行时，才会加载到内存，形成进程。</p>
<p>而**<code>镜像</code>**，就是把一个应用在硬盘上的文件、及其运行环境、部分系统函数库文件一起打包形成的文件包。这个文件包是只读的。</p>
<p>**<code>容器</code>**呢，就是将这些文件中编写的程序、函数加载到内存中允许，形成进程，只不过要隔离起来。因此一个镜像可以启动多次，形成多个容器进程。</p>
<img src="assets/image-20210731153059464.png" alt="image-20210731153059464" style="zoom:25%;" />



<p>例如你下载了一个QQ，如果我们将QQ在磁盘上的运行<strong>文件</strong>及其运行的操作系统依赖打包，形成QQ镜像。然后你可以启动多次，双开、甚至三开QQ，跟多人聊天。</p>
<h3 id="1-3-2-DockerHub"><a href="#1-3-2-DockerHub" class="headerlink" title="1.3.2.DockerHub"></a>1.3.2.DockerHub</h3><p>开源应用程序非常多，打包这些应用往往是重复的劳动。为了避免这些重复劳动，人们就会将自己打包的应用镜像，例如Redis、MySQL镜像放到网络上，共享使用，就像GitHub的代码共享一样。</p>
<ul>
<li><p>DockerHub：DockerHub是一个官方的Docker镜像的托管平台。这样的平台称为Docker Registry。</p>
</li>
<li><p>国内也有类似于DockerHub 的公开服务，比如 <a target="_blank" rel="noopener" href="https://c.163yun.com/hub">网易云镜像服务</a>、<a target="_blank" rel="noopener" href="https://cr.console.aliyun.com/">阿里云镜像库</a>等。</p>
</li>
</ul>
<p>我们一方面可以将自己的镜像共享到DockerHub，另一方面也可以从DockerHub拉取镜像：</p>
<img src="assets/image-20210731153743354.png" alt="image-20210731153743354"  />

<p>默认情况下，将来从docker hub（<a target="_blank" rel="noopener" href="https://hub.docker.com/%EF%BC%89%E4%B8%8A%E4%B8%8B%E8%BD%BDdocker%E9%95%9C%E5%83%8F%EF%BC%8C%E5%A4%AA%E6%85%A2%E3%80%82%E4%B8%80%E8%88%AC%E9%83%BD%E4%BC%9A%E9%85%8D%E7%BD%AE%E9%95%9C%E5%83%8F%E5%8A%A0%E9%80%9F%E5%99%A8%EF%BC%9A">https://hub.docker.com/）上下载docker镜像，太慢。一般都会配置镜像加速器：</a></p>
<ul>
<li>USTC：中科大镜像加速器（<a target="_blank" rel="noopener" href="https://docker.mirrors.ustc.edu.cn)/">https://docker.mirrors.ustc.edu.cn）</a></li>
<li>阿里云</li>
<li>网易云</li>
<li>腾讯云</li>
</ul>
<h3 id="1-3-3-Docker架构"><a href="#1-3-3-Docker架构" class="headerlink" title="1.3.3.Docker架构"></a>1.3.3.Docker架构</h3><p>我们要使用Docker来操作镜像、容器，就必须要安装Docker。</p>
<p>Docker是一个CS架构的程序，由两部分组成：</p>
<ul>
<li><p>服务端(server)：Docker守护进程，负责处理Docker指令，管理镜像、容器等</p>
</li>
<li><p>客户端(client)：通过命令或RestAPI向Docker服务端发送指令。可以在本地或远程向服务端发送指令。</p>
</li>
</ul>
<p>如图：</p>
<p><img src="/assets/image-20210731154257653.png" alt="image-20210731154257653"></p>
<h3 id="1-3-4-小结"><a href="#1-3-4-小结" class="headerlink" title="1.3.4.小结"></a>1.3.4.小结</h3><p>镜像：</p>
<ul>
<li>将应用程序及其依赖、环境、配置打包在一起</li>
</ul>
<p>容器：</p>
<ul>
<li>镜像运行起来就是容器，一个镜像可以运行多个容器</li>
</ul>
<p>Docker结构：</p>
<ul>
<li><p>服务端：接收命令或远程请求，操作镜像或容器</p>
</li>
<li><p>客户端：发送命令或者请求到Docker服务端</p>
</li>
</ul>
<p>DockerHub：</p>
<ul>
<li>一个镜像托管的服务器，类似的还有阿里云镜像服务，统称为DockerRegistry</li>
</ul>
<h2 id="1-4-安装Docker"><a href="#1-4-安装Docker" class="headerlink" title="1.4.安装Docker"></a>1.4.安装Docker</h2><p><strong>Docker Engine是C&#x2F;S架构的</strong></p>
<img src=".\assets\image-20230204233058255.png" alt="image-20230204233058255" style="zoom:67%;" />

<h3 id="1-4-1Docker由以下几个部件组成："><a href="#1-4-1Docker由以下几个部件组成：" class="headerlink" title="1.4.1Docker由以下几个部件组成："></a>1.4.1<strong>Docker由以下几个部件组成：</strong></h3><h4 id="（1）Docker-Daemon"><a href="#（1）Docker-Daemon" class="headerlink" title="（1）Docker Daemon"></a>（1）Docker Daemon</h4><p>安装使用Docker，得先运行Docker Daemon进程，用于管理docker，如:</p>
<ul>
<li>镜像 images</li>
<li>容器 containers</li>
<li>网络 network</li>
<li>数据卷 Data Volumes</li>
</ul>
<h4 id="（2）Rest接口"><a href="#（2）Rest接口" class="headerlink" title="（2）Rest接口"></a>（2）Rest接口</h4><p>提供和Daemon交互的API接口</p>
<h4 id="（3）Docker-Client"><a href="#（3）Docker-Client" class="headerlink" title="（3）Docker Client"></a>（3）Docker Client</h4><p>客户端使用REST API和Docker Daemon进行访问</p>
<h3 id="1-4-2-Docker平台组成"><a href="#1-4-2-Docker平台组成" class="headerlink" title="1.4.2 Docker平台组成"></a>1.4.2 Docker平台组成</h3><img src=".\assets\image-20230204233350282.png" alt="image-20230204233350282" style="zoom:67%;" />

<h4 id="（1）Images"><a href="#（1）Images" class="headerlink" title="（1）Images"></a>（1）Images</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">镜像是一个只读模板，用于创建容器，也可以通过Dockerfile文本描述镜像的内容。</span><br><span class="line"></span><br><span class="line">镜像的概念类似于编程开发里面向对象的类，从一个基类开始(基础镜像Base lmage)</span><br><span class="line">构建容器的过程，就是运行镜像，生成容器实例。</span><br><span class="line"></span><br><span class="line">Docker镜像的描述文件是Dockerfile，包含了如下的指令</span><br><span class="line"></span><br><span class="line">- FROM 定义基础镜像</span><br><span class="line">- MAINTAINER 作者</span><br><span class="line">- RUN 运行Linux命令</span><br><span class="line">- ADD 添加文件/目录</span><br><span class="line">- ENV 环境变量</span><br><span class="line">- CMD 运行进程</span><br></pre></td></tr></table></figure>

<h4 id="（2）Container"><a href="#（2）Container" class="headerlink" title="（2）Container"></a>（2）Container</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">容器是一个镜像的运行实例，镜像 &gt; 容器</span><br><span class="line">创建容器的过程</span><br><span class="line"></span><br><span class="line">- 获取镜像，如 docker pull centos ,从镜像仓库拉取</span><br><span class="line">- 使用镜像创建容器</span><br><span class="line">- 分配文件系统，挂载一个读写层，在读写层加载镜像分配网络/网桥接口，创建一个网络接口，让容器和宿主机通信</span><br><span class="line">- 容器获取IP地址</span><br><span class="line">- 执行容器命令，如/bin/bash</span><br><span class="line">- 反馈容器启动结果。</span><br></pre></td></tr></table></figure>

<h4 id="（3）Registry"><a href="#（3）Registry" class="headerlink" title="（3）Registry"></a>（3）Registry</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Docker镜像需要进行管理，docker提供了Registry仓库，其实它也是一个容器。可以用于可以基于该容器运行私有仓库。</span><br><span class="line">也可以使用Docker Hub互联网公有镜像仓库</span><br></pre></td></tr></table></figure>





<h3 id="1-4-3-CentOS安装Docker"><a href="#1-4-3-CentOS安装Docker" class="headerlink" title="1.4.3 CentOS安装Docker"></a>1.4.3 CentOS安装Docker</h3><h4 id="（1）版本管理"><a href="#（1）版本管理" class="headerlink" title="（1）版本管理"></a>（1）版本管理</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Docker 分为 CE 和 EE 两大版本。CE 即社区版（免费，支持周期 7 个月），EE 即企业版，强调安全，付费使用，支持周期 24 个月。</span><br><span class="line"></span><br><span class="line">每个季度(1-3,4-6,7-9,10-12)，企业版和社区版都会发布一个稳定版本(Stable)。社区版本会提供 4 个月的支持，而企业版本会提供 12个月的支持</span><br><span class="line"></span><br><span class="line">每个月社区版还会通过 Edge 方式发布月度版</span><br><span class="line"></span><br><span class="line">从2017 年第一季度开始，Docker 版本号遵循 YY.MM-xx 格式，类似于 Ubuntu 等项目。例如，2018 年6 月第一次发布的社区版本为18.06.0-ce</span><br><span class="line"></span><br><span class="line">Docker CE 分为 `stable` `test` 和 `nightly` 三个更新频道。</span><br><span class="line"></span><br><span class="line">官方网站上有各种环境下的 [安装指南](https://docs.docker.com/install/)，这里主要介绍 Docker CE 在 CentOS上的安装。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Docker CE 支持 64 位版本 CentOS 7，并且要求内核版本不低于 3.10， CentOS 7 满足最低内核的要求，所以我们在CentOS 7安装Docker。</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cassets%5Cimage-20230204234218336.png" alt="image-20230204234218336"></p>
<h4 id="（2）机器环境初始化"><a href="#（2）机器环境初始化" class="headerlink" title="（2）机器环境初始化"></a>（2）机器环境初始化</h4><h5 id="1-卸载（可选）"><a href="#1-卸载（可选）" class="headerlink" title="1.卸载（可选）"></a>1.卸载（可选）</h5><p>如果之前安装过旧版本的Docker，可以使用下面命令卸载：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">yum remove docker \</span><br><span class="line">                  docker-client \</span><br><span class="line">                  docker-client-latest \</span><br><span class="line">                  docker-common \</span><br><span class="line">                  docker-latest \</span><br><span class="line">                  docker-latest-logrotate \</span><br><span class="line">                  docker-logrotate \</span><br><span class="line">                  docker-selinux \</span><br><span class="line">                  docker-engine-selinux \</span><br><span class="line">                  docker-engine \</span><br><span class="line">                  docker-ce</span><br></pre></td></tr></table></figure>

<h5 id="2-yum源配置"><a href="#2-yum源配置" class="headerlink" title="2.yum源配置"></a>2.yum源配置</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">备份配置文件</span></span><br><span class="line">mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup</span><br><span class="line"></span><br><span class="line">wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line"></span><br><span class="line">wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br><span class="line"></span><br><span class="line">yum clean all</span><br><span class="line">yum makecache</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">可选操作</span></span><br><span class="line">yum install -y bash-completion vim lrzsz wget expect net-tools nc nmap treedos2unix htop iftop iotop unzip telnet sl psmisc nethogs glances bc ntpdate openldap-devel</span><br></pre></td></tr></table></figure>



<h4 id="（3）安装docker"><a href="#（3）安装docker" class="headerlink" title="（3）安装docker"></a>（3）安装docker</h4><h5 id="1-首先需要虚拟机联网，安装yum工具"><a href="#1-首先需要虚拟机联网，安装yum工具" class="headerlink" title="1.首先需要虚拟机联网，安装yum工具"></a>1.首先需要虚拟机联网，安装yum工具</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install -y yum-utils \</span><br><span class="line">           device-mapper-persistent-data \</span><br><span class="line">           lvm2 --skip-broken</span><br></pre></td></tr></table></figure>

<h5 id="2-配置网卡转发"><a href="#2-配置网卡转发" class="headerlink" title="2.配置网卡转发"></a>2.配置网卡转发</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">1.docker必须安装在centos7平台，内核版本不低于3.10在centos平台运行docker可能会遇见些告警信息，修改内核配置参数，打开内核转发功能</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">写入</span></span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/sysctl.d/docker.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.ipv4.conf.default.rp_filter = 0</span><br><span class="line">net.ipv4.conf.all.rp_filter = 0</span><br><span class="line">net.ipv4.ip_forward=1</span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">2.重新加载内核参数</span></span><br><span class="line">modprobe br_netfilter</span><br><span class="line">sysctl -p /etc/sysctl.d/docker.conf</span><br></pre></td></tr></table></figure>

<h5 id="3-利用yum进行docker安装"><a href="#3-利用yum进行docker安装" class="headerlink" title="3.利用yum进行docker安装"></a>3.利用yum进行docker安装</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">提前配置好yum仓库</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1.阿里云自带仓库 2.阿里云提供的docker专属repo仓库</span></span><br><span class="line"></span><br><span class="line">curl -o /etc/yum.repos.d/docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line">curl -o /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">更新yum缓存</span></span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">可以直接yum安装docker了</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 查看源中可用版本</span></span></span><br><span class="line">yum list docker-ce --showduplicates | sort -r</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># yum安装</span></span></span><br><span class="line">yum install docker-ce -y</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 如果需要安装旧版本</span></span></span><br><span class="line">yum install -y docker-ce-20.10.6</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#查看docker版本，验证是否验证成功</span></span></span><br><span class="line">docker -v</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#如果要卸载</span></span></span><br><span class="line">yum remove -y docker-ce-xxx</span><br></pre></td></tr></table></figure>

<h5 id="4-配置镜像加速器"><a href="#4-配置镜像加速器" class="headerlink" title="4.配置镜像加速器"></a>4.配置镜像加速器</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">用于加速镜像文件下载,选用阿里云镜像站</span></span><br><span class="line">mkdir -p /etc/docker</span><br><span class="line">touch /etc/docker/daemon.json</span><br><span class="line">vim /etc/docker/daemon.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">&quot;registry-mirrors&quot; : [</span><br><span class="line">&quot;https://8xpk5wnt.mirror.aliyuncs.com&quot;</span><br><span class="line">]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="5-启动docker"><a href="#5-启动docker" class="headerlink" title="5.启动docker"></a>5.启动docker</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启动docker前，一定要关闭防火墙后！！</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">关闭</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">禁止开机启动防火墙</span></span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#通过命令启动docker：</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">systemctl start docker  <span class="comment"># 启动docker服务</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">systemctl stop docker  <span class="comment"># 停止docker服务</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">systemctl restart docker  <span class="comment"># 重启docker服务</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">我们使用如下命令进行docker启动</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#docker配置文件重新加载</span></span></span><br><span class="line">systemctl daemon-reload</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#设置开启自启动</span></span></span><br><span class="line">systemctl enable docker</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启动docker</span></span><br><span class="line">systemctl start docker</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 查看docker信息</span></span></span><br><span class="line">docker info</span><br><span class="line">docker ps</span><br><span class="line">docker images</span><br><span class="line">docker version</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># docker-client</span></span></span><br><span class="line">which docker</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># docker daemon</span></span></span><br><span class="line">ps aux |grep docker</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># containerd</span></span></span><br><span class="line">ps aux|grep containerd</span><br><span class="line">systemctl status containerd</span><br></pre></td></tr></table></figure>



<h5 id="或者这样安装"><a href="#或者这样安装" class="headerlink" title="***.或者这样安装"></a>***.或者这样安装</h5><p>更新本地镜像源：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置docker镜像源</span></span><br><span class="line">yum-config-manager \</span><br><span class="line">    --add-repo \</span><br><span class="line">    https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line">    </span><br><span class="line">sed -i &#x27;s/download.docker.com/mirrors.aliyun.com\/docker-ce/g&#x27; /etc/yum.repos.d/docker-ce.repo</span><br><span class="line"></span><br><span class="line">yum makecache fast</span><br></pre></td></tr></table></figure>

<p>然后输入命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y docker-ce</span><br></pre></td></tr></table></figure>

<p>docker-ce为社区免费版本。稍等片刻，docker即可安装成功。</p>
<h1 id="2-Docker的基本操作"><a href="#2-Docker的基本操作" class="headerlink" title="2.Docker的基本操作"></a>2.Docker的基本操作</h1><h2 id="2-0-docker初体验"><a href="#2-0-docker初体验" class="headerlink" title="2.0 docker初体验"></a>2.0 docker初体验</h2><h3 id="（1）docker快速体验"><a href="#（1）docker快速体验" class="headerlink" title="（1）docker快速体验"></a>（1）docker快速体验</h3><p>心中有数</p>
<p><img src="/.%5Cassets%5Cimage-20230205224930726.png" alt="image-20230205224930726"></p>
<p>启动第一个docker容器</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.获取镜像	</span></span><br><span class="line"><span class="comment">#2.运行镜像，生成容器，你想要的容器，就运行在容器中</span></span><br></pre></td></tr></table></figure>

<p>Nginx web服务器，运行一个80端口的网站</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在宿主机上，运行Nginx</span></span><br><span class="line">1.开启服务器</span><br><span class="line">2.在服务器上安装好运行nginx所需的依赖关系</span><br><span class="line">3.安装nginx yum install nginx -y</span><br><span class="line">4.修改nginx配置文件</span><br><span class="line">5.启动nginx</span><br><span class="line">6.客户端去访问nginx</span><br><span class="line"></span><br><span class="line">比较耗时</span><br></pre></td></tr></table></figure>

<p>如果使用docker运行，该如何做？</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.查看本地的docker镜像有哪些</span></span><br><span class="line">docker image <span class="built_in">ls</span> 或 docker images</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.可选择删除旧版本</span></span><br><span class="line">docker rmi 镜像<span class="built_in">id</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.搜索一下远程仓库中的镜像文件是否存在</span></span><br><span class="line">docker search nginx</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.拉取，下载镜像</span></span><br><span class="line">docerk pull nginx</span><br><span class="line">[root@node3 yum.repos.d]<span class="comment"># docker pull nginx</span></span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/nginx</span><br><span class="line">a2abf6c4d29d: Pull complete </span><br><span class="line">a9edb18cadd1: Pull complete </span><br><span class="line">589b7251471a: Pull complete </span><br><span class="line">186b1aaa4aa6: Pull complete </span><br><span class="line">b4df32aa5a72: Pull complete </span><br><span class="line">a0bcbecc962e: Pull complete </span><br><span class="line">Digest: sha256:0d17b565c37bcbd895e9d92315a05c1c3c9a29f762b011a10c54a66cd53c9b31</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> nginx:latest</span><br><span class="line">docker.io/library/nginx:latest</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.再次查看镜像</span></span><br><span class="line">docker images</span><br><span class="line">REPOSITORY   TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">nginx        latest    605c77e624dd   13 months ago   141MB</span><br><span class="line"></span><br><span class="line"><span class="comment">#6.运行镜像，运行出具体内容，在容器中就跑着一个nginx服务</span></span><br><span class="line">docker run 参数 镜像的名字/id</span><br><span class="line"><span class="comment">#-d 后台运行容器</span></span><br><span class="line"><span class="comment">#-p 80:80 端口映射，宿主机端口：容器内端口，访问宿主机的80端口，也就访问到容器中的80端口</span></span><br><span class="line">docker run -d -p 80:80 nginx</span><br><span class="line"><span class="comment">#会返回一个容器的id</span></span><br><span class="line">[root@node3 ~]<span class="comment"># docker run -d -p 80:80 nginx</span></span><br><span class="line">199b29ec5a2732e9c87b6557fa8aed66c4b0f5ef45bd91dd04a38510a7be55ca</span><br><span class="line"></span><br><span class="line"><span class="comment">#7.查看容器是否在运行</span></span><br><span class="line">docker ps</span><br><span class="line">[root@node3 ~]<span class="comment"># docker ps</span></span><br><span class="line">CONTAINER ID   IMAGE     COMMAND                   CREATED          STATUS          PORTS                               NAMES</span><br><span class="line">199b29ec5a27   nginx     <span class="string">&quot;/docker-entrypoint.…&quot;</span>   14 seconds ago   Up 13 seconds   0.0.0.0:80-&gt;80/tcp, :::80-&gt;80/tcp   affectionate_panini</span><br><span class="line"></span><br><span class="line"><span class="comment">#8.访问网站</span></span><br><span class="line">192.168.88.163:80</span><br><span class="line"></span><br><span class="line"><span class="comment">#9.停止容器</span></span><br><span class="line">docker stop 容器<span class="built_in">id</span></span><br><span class="line">docker stop 199b29ec5a27</span><br><span class="line">[root@node3 ~]<span class="comment"># docker stop 199b29ec5a27</span></span><br><span class="line">199b29ec5a27</span><br><span class="line"></span><br><span class="line"><span class="comment">#10.恢复容器</span></span><br><span class="line">docker start 199b29ec5a27</span><br></pre></td></tr></table></figure>

<img src=".\assets\image-20230206000042136.png" alt="image-20230206000042136" style="zoom: 67%;" />

<h3 id="（2）docker生命周期"><a href="#（2）docker生命周期" class="headerlink" title="（2）docker生命周期"></a>（2）<strong>docker生命周期</strong></h3><p><img src="/.%5Cassets%5Cimage-20230206134540197.png" alt="image-20230206134540197"></p>
<h3 id="（3）彻底学明白docker镜像的原理"><a href="#（3）彻底学明白docker镜像的原理" class="headerlink" title="（3）彻底学明白docker镜像的原理"></a>（3）彻底学明白docker镜像的原理</h3><h4 id="1-我们用的centos7系统长什么样"><a href="#1-我们用的centos7系统长什么样" class="headerlink" title="1.我们用的centos7系统长什么样?"></a>1.我们用的centos7系统长什么样?</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">我们一直以来，使用vmware虚拟机，安装的系统，是一个完整的系统文件，包括2部分。</span><br><span class="line"></span><br><span class="line">-linux内核，作用是提供操作系统的基本功能，和机器硬件交互，读取磁盘数据，管理网络。（硬件交互）</span><br><span class="line"></span><br><span class="line">-centos7发行版，作用是提供软件功能，例如yum安装包管理等（软件交互）</span><br><span class="line"></span><br><span class="line">因此，linux内核+centos发行版，就组成了一个系统，让我们用户使用</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cassets%5Cimage-20230206140401819.png" alt="image-20230206140401819"></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看发行版</span></span><br><span class="line"><span class="built_in">cat</span> /etc/redhat-release</span><br><span class="line">[root@node3 ~]<span class="comment"># cat /etc/redhat-release </span></span><br><span class="line">CentOS Linux release 7.9.2009 (Core)</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看内核</span></span><br><span class="line"><span class="built_in">uname</span> -r</span><br><span class="line">[root@node3 ~]<span class="comment"># uname -r</span></span><br><span class="line">3.10.0-1160.el7.x86_64</span><br><span class="line"></span><br><span class="line">是否有一个办法，可以灵活的替换发行版，让我们使用不同的[系统]?</span><br><span class="line"></span><br><span class="line"><span class="comment">##答：内核使用宿主机的内核，上行发行版可以自有替换--》这就是docker</span></span><br><span class="line"></span><br><span class="line">那么docker就实现了这个功能，技术手段就是dockerimages</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cassets%5Cimage-20230206141146364.png" alt="image-20230206141146364"></p>
<h4 id="2-快速实践，使用docker切换不同发行版，内核都是宿主机内核"><a href="#2-快速实践，使用docker切换不同发行版，内核都是宿主机内核" class="headerlink" title="2.快速实践，使用docker切换不同发行版，内核都是宿主机内核"></a>2.快速实践，使用docker切换不同发行版，内核都是宿主机内核</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.利用docker获取不同的发行版镜像</span></span><br><span class="line">docker pull centos:7.8.2003</span><br><span class="line">docker pull ubuntu</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.确认当前宿主机的发行版</span></span><br><span class="line"><span class="built_in">cat</span> /etc/redhat-release</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.运行centos:7.8.2003发行版本</span></span><br><span class="line"><span class="comment">#运行容器，且进入容器内部</span></span><br><span class="line"><span class="comment">#参数解释，-i 交互式命令操作 -t 开启一个终端 bash 进入容器后执行的命令</span></span><br><span class="line">docker run -it 镜像<span class="built_in">id</span> bash</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.查看容器内的发行版</span></span><br><span class="line"><span class="built_in">cat</span> /etc/redhat-release</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.退出容器空间</span></span><br><span class="line"><span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#6.如果想切换至Ubuntu</span></span><br><span class="line">docker run -it ubuntu bash</span><br><span class="line"><span class="built_in">cat</span> /etc/lsb-release</span><br><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#小结：</span></span><br><span class="line">1.一个完整的系统，是由linux的内核+发行版，才组成了一个可以使用的完整的系统</span><br><span class="line">2.利用docker容器，可以获取不同的发行版镜像，然后基于该镜像，运行出各种容器去使用</span><br></pre></td></tr></table></figure>

<h4 id="3-docker具体解决问题实例"><a href="#3-docker具体解决问题实例" class="headerlink" title="3.docker具体解决问题实例"></a>3.docker具体解决问题实例</h4><p>一个开发老王，写代码，mysql，etcd，redis，elk，等等等。。。</p>
<p>在机器上，直接安装这些工具，win，mac</p>
<p><img src="/.%5Cassets%5Cimage-20230206150021008.png" alt="image-20230206150021008"></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">如果你直接在宿主机上，安装这些工具，那受限于宿主机的环境</span><br><span class="line">可能有哪些麻烦</span><br><span class="line">1.环境不兼容，比如软件需要运行在linux下，但是你是indows，只能去安装一个vmware虚拟机，或者再去买一个云服务器，安装</span><br><span class="line">2.会将你当前系统的环境，搞的一团糟</span><br><span class="line">3.比如你想卸载这些工具。。麻烦了，不会卸载。。</span><br><span class="line">docker彻底解决了老王的问题，以后可以不加班了。</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cassets%5Cimage-20230206150131475.png" alt="image-20230206150131475"></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.解决了环境的兼容问题，在容器中运行linux发行版，以及各种软件， [windows+docker+容器1 centos)+容器2(ubuntu)]</span><br><span class="line">2.环境很干净，你安装的所有内容，都在容器里，不想要了，就直接删除容器，不影响你宿主机</span><br><span class="line">3.比如你想把mysal容器内的数据，配置，全部迁移到服务器上，只需要提交该容器，生成镜像，镜像放到服务器上，docker run，就运行了!</span><br></pre></td></tr></table></figure>



<h4 id="4-docker镜像原理-1"><a href="#4-docker镜像原理-1" class="headerlink" title="4.docker镜像原理(1)"></a>4.docker镜像原理(1)</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#docker images搜索地址</span></span><br><span class="line"><span class="comment">#https://hub.docker.com/_/centos?tab=tags&amp;page=1&amp;ordering=last_updated</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##我们在获取redis镜像的时候，发现下载了多行信息，最终仅得到了一个完整的镜像文件</span></span><br><span class="line"></span><br><span class="line">[root@node3 ~]<span class="comment"># docker pull redis</span></span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/redis</span><br><span class="line">a2abf6c4d29d: Already exists </span><br><span class="line">c7a4e4382001: Pull complete </span><br><span class="line">4044b9ba67c9: Pull complete </span><br><span class="line">c8388a79482f: Pull complete </span><br><span class="line">413c8bb60be2: Pull complete </span><br><span class="line">1abfd3011519: Pull complete </span><br><span class="line">Digest: sha256:db485f2e245b5b3329fdc7eff4eb00f913e09d8feb9ca720788059fdc2ed8339</span><br><span class="line">Status: Downloaded newer image <span class="keyword">for</span> redis:latest</span><br><span class="line">docker.io/library/redis:latest</span><br><span class="line"></span><br><span class="line">[root@node3 ~]<span class="comment"># docker images</span></span><br><span class="line">REPOSITORY   TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">nginx        latest    605c77e624dd   13 months ago   141MB</span><br><span class="line">redis        latest    7614ae9453d1   13 months ago   113MB</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>dockerfile 去构建这个镜像 特点就是一层一层的构建</strong></p>
<p><img src="/.%5Cassets%5Cimage-20230206153010970.png" alt="image-20230206153010970"></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Union File System</span></span><br><span class="line"><span class="comment">#docker通过联合文件系统，将上述的不同的每一层，整合为一个文件系统,为用户隐藏了多层的视角</span></span><br><span class="line"><span class="comment">#docker iamges</span></span><br><span class="line"><span class="comment">#docker run 镜像id</span></span><br><span class="line"></span><br><span class="line">小结:</span><br><span class="line">1.当通过一个image启动容器时，docker会在该image最顶层，添加一个读写文件系统作为容器，然后运行该容器</span><br><span class="line">2.docker镜像本质是基于UnionFS管理的分层文件系统</span><br><span class="line">3.docker镜像为什么才几百兆?</span><br><span class="line">答:因为docker镜像只有rootfs和其他镜像层，共用宿主机的linux内核 (bootfs)，因此很小!</span><br><span class="line">4.为什么下载一个docker的nginx镜像，需要133MB? nginx安装包不是才几兆吗?</span><br><span class="line">答:因为docker的nginx镜像是分层的，nginx安装包的确就几M，但是一个用于运行nginx的像文件，依赖于父镜像(上一层)，和基础像(发行版)，所以下载的nginx镜像有一百多兆</span><br><span class="line"></span><br><span class="line"><span class="comment">#eg：基于nginx镜像，运行处一个nginx的容器nginx是一个软件，必须依赖于操作系统才能运行</span></span><br><span class="line"><span class="comment">#1.用的是 windows 宿主机，但是提供了内核</span></span><br><span class="line"><span class="comment">#2.利用 docker 下载镜像，(镜像提供了发行版，centos)</span></span><br><span class="line"><span class="comment">#3.该nginx就可以运行在该 centos 发行版环境中了</span></span><br><span class="line"><span class="comment">#因此这个结构形式是 win+ [docker+centos+nginx]</span></span><br></pre></td></tr></table></figure>

<h4 id="5-docker镜像原理-2"><a href="#5-docker镜像原理-2" class="headerlink" title="5.docker镜像原理(2)"></a>5.docker镜像原理(2)</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#我们如果自定义镜像，刚才已经讲过，docker镜像不包含linux内核，和宿主机共用。</span></span><br><span class="line"><span class="comment">#我们如果想要定义一个mysql5.6镜像，我们会这么做</span></span><br><span class="line">- 获取基础镜像，选择一个发行版平台 (ubutu，centos)</span><br><span class="line">- 在centos镜像中安装mysql5.6软件</span><br><span class="line"></span><br><span class="line">导出镜像，可以命名为mysql:5.6镜像文件</span><br><span class="line">从这个过程，我们可以感觉出这是一层一层的添加的，docker镜像的层级概念就出来了，底层是centos镜像，上层是mysql镜像，centos镜像层属于父镜像。</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cassets%5Cimage-20230206170048363.png" alt="image-20230206170048363"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Docker镜像是在基础镜像之后，然后安装软件，配置软件添加新的层，构建出来。</span><br><span class="line">这种现象在学习dockerfile构建时候，更为清晰。</span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#实例</span></span><br><span class="line"><span class="comment">#进入正在运行的容器内，命令式docker exec</span></span><br><span class="line">docker <span class="built_in">exec</span> -it 容器<span class="built_in">id</span> bash</span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> /etc/os-release</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="（1）docker为什么要分层镜像"><a href="#（1）docker为什么要分层镜像" class="headerlink" title="（1）docker为什么要分层镜像"></a>（1）docker为什么要分层镜像</h5><p>镜像分享一大好处就是共享资源，例如有多个镜像都来自于同一个base镜像，那么在docker host只需要存储一份base镜像。</p>
<p>内存里也只需要加载一份host，即可为多个容器服务</p>
<p>即使多个容器共享一个base镜像，某个容器修改了base镜像的内容，例如修改&#x2F;etc&#x2F;下配置文件，其他容器的&#x2F;etc&#x2F;下内容是不会被修改的，修改动作只限制在单个容器内，这就是容器的写入时复制特性 (Copy-on-write)</p>
<p><img src="/.%5Cassets%5Cimage-20230206171151486.png" alt="image-20230206171151486"></p>
<h5 id="（2）可写的容器层和只读的镜像层"><a href="#（2）可写的容器层和只读的镜像层" class="headerlink" title="（2）可写的容器层和只读的镜像层"></a>（2）可写的容器层和只读的镜像层</h5><p>当容器启动后，一个新的可写层被加载到镜像的顶部，这一层通常被称为 容器层，容器层下的都称为 镜像层。</p>
<img src=".\assets\image-20230206171324713.png" alt="image-20230206171324713" style="zoom:67%;" />

<p>所有对容器的修改动作，都只会发生在 容器层 里，只有 容器层 是可写的，其余 镜像层 都是只读的。</p>
<p><img src="/.%5Cassets%5Cimage-20230206171404547.png" alt="image-20230206171404547"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">只有当需要修改时才复制一份数据，这种特性被称作 Copy-on-Write。可见，容器层保存的是镜像变化的部分，不会对镜像本身进行任何修改。</span><br><span class="line">这样就解释了我们前面提出的问题: 容器层记录对镜像的修改，所有镜像层都是只读的，不会被容器修改，所以镜像可以被多个容器共享。</span><br></pre></td></tr></table></figure>

<h5 id="（3）docker镜像的内容"><a href="#（3）docker镜像的内容" class="headerlink" title="（3）docker镜像的内容"></a>（3）docker镜像的内容</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker镜像层级管理的方式大大便捷了Docker镜像的分发和存储。</span><br><span class="line"><span class="comment">#Docker hub是为全世界的镜像仓库。</span></span><br><span class="line">-Docker镜像代表一个容器的文件系统内容</span><br><span class="line">-镜像层级技术属于 联合文件系统</span><br><span class="line">-容器是一个动态的环境，每一层镜像里的文件都属于静态内容。 </span><br><span class="line">  -dockerfile里的ENV、VOLUME、CMD等内容都会落实到容器环境里</span><br></pre></td></tr></table></figure>

<h5 id="（4）UnionFS"><a href="#（4）UnionFS" class="headerlink" title="（4）UnionFS"></a>（4）UnionFS</h5><img src="assets\image-20230206171659108.png" alt="image-20230206171659108" style="zoom:67%;" />

<h2 id="2-1-镜像操作"><a href="#2-1-镜像操作" class="headerlink" title="2.1.镜像操作"></a>2.1.镜像操作</h2><h3 id="2-1-1-镜像名称"><a href="#2-1-1-镜像名称" class="headerlink" title="2.1.1.镜像名称"></a>2.1.1.镜像名称</h3><p>首先来看下镜像的名称组成：</p>
<ul>
<li>镜名称一般分两部分组成：[repository]:[tag]。</li>
<li>在没有指定tag时，默认是latest，代表最新版本的镜像</li>
</ul>
<p>如图：</p>
<img src="assets/image-20210731155141362.png" alt="image-20210731155141362" style="zoom: 33%;" />

<p>这里的<code>mysql就是repository</code>，<code>5.7就是tag</code>，合一起就是镜像名称，代表5.7版本的MySQL镜像。</p>
<p><img src="/assets/image-20210731155649535.png" alt="image-20210731155649535"></p>
<h3 id="2-1-2-获取镜像"><a href="#2-1-2-获取镜像" class="headerlink" title="2.1.2 获取镜像"></a>2.1.2 获取镜像</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.从dockerhub获取镜像</span><br><span class="line">2.本地镜像导入导出</span><br><span class="line">3.私有的docker仓库</span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.获取镜像，镜像托管仓库，好比yum源一样</span></span><br><span class="line"><span class="comment"># 默认的docker仓库是，dockerhub ，有大量的优质的镜像，以及用户自己上传的镜像 centos容器 vim nginx 。。提交为镜像，上传到dockehub</span></span><br><span class="line"></span><br><span class="line">docker search 镜像名:tag tag就是具体的标签版本</span><br><span class="line">docker search centos</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.查看本地的镜像文件有哪些</span></span><br><span class="line"> docker images </span><br><span class="line"> docker image <span class="built_in">ls</span></span><br><span class="line"><span class="comment">#3.下载docker镜像</span></span><br><span class="line">docker pull centos <span class="comment"># 默认的是 centos:latest</span></span><br><span class="line">docker pull centos:7.8.2003</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.查看docker镜像的存储路径</span></span><br><span class="line">docker info |grep Root</span><br><span class="line"><span class="comment">#Docker Root Dir: /var/lib/docker</span></span><br><span class="line"><span class="comment">#具体位置</span></span><br><span class="line"><span class="built_in">ls</span> /var/lib/docker/image/overlay2/imagedb/content/sha256</span><br><span class="line"></span><br><span class="line">[root@node3 ~]<span class="comment"># ls /var/lib/docker/image/overlay2/imagedb/content/sha256</span></span><br><span class="line">5d9483f9a7b21c87e0f5b9776c3e06567603c28c0062013eda127c968175f5e8</span><br><span class="line">605c77e624ddb75e6110f997c58876baa13f8754486b461117934b24a9dc3a85</span><br><span class="line">7614ae9453d1d87e740a2056257a6de7135c84037c367e1fffa92ae922784631</span><br><span class="line"></span><br><span class="line">[root@node3 ~]<span class="comment"># docker images</span></span><br><span class="line">REPOSITORY   TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">nginx        latest    605c77e624dd   13 months ago   141MB</span><br><span class="line">redis        latest    7614ae9453d1   13 months ago   113MB</span><br><span class="line">mysql        5.7.29    5d9483f9a7b2   2 years ago     455MB</span><br><span class="line"><span class="comment">#5.该文件作用是</span></span><br><span class="line">记录 镜像 和容器的配置关系</span><br><span class="line"><span class="comment"># 使用不同的镜像，生成容器# -it 开启一个交互式的终端--rm 容器退出时删除该容器</span></span><br><span class="line"><span class="comment">#再运行一个7.8centos</span></span><br><span class="line">docker run -it --<span class="built_in">rm</span> centos bash</span><br></pre></td></tr></table></figure>

<h3 id="2-1-3-查看镜像"><a href="#2-1-3-查看镜像" class="headerlink" title="2.1.3 查看镜像"></a>2.1.3 查看镜像</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.查看所有镜像</span></span><br><span class="line">docker images</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.查看具体镜像</span></span><br><span class="line">docker images nginx</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.指定tag查看</span></span><br><span class="line">docker images centos:7.8.2003</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.只列出镜像id</span></span><br><span class="line">-q --quiet 只列出<span class="built_in">id</span></span><br><span class="line">docker images -q</span><br><span class="line">[root@node3 ~]<span class="comment"># docker images -q</span></span><br><span class="line">605c77e624dd</span><br><span class="line">7614ae9453d1</span><br><span class="line">5d9483f9a7b2</span><br><span class="line"></span><br><span class="line"><span class="comment">#5.格式化显示镜像</span></span><br><span class="line"><span class="comment"># 这是docker的模板语言，--format</span></span><br><span class="line">docker images --format <span class="string">&quot;&#123;&#123;.ID&#125;&#125;--&#123;&#123;.Repository&#125;&#125;&quot;</span></span><br><span class="line">[root@node3 ~]<span class="comment"># docker images --format &quot;&#123;&#123;.ID&#125;&#125;--&#123;&#123;.Repository&#125;&#125;&quot;</span></span><br><span class="line">605c77e624dd--nginx</span><br><span class="line">7614ae9453d1--redis</span><br><span class="line">5d9483f9a7b2--mysql</span><br><span class="line"></span><br><span class="line"><span class="comment">#6.搜索镜像</span></span><br><span class="line">docker search 镜像名</span><br></pre></td></tr></table></figure>

<h3 id="2-1-4-删除镜像"><a href="#2-1-4-删除镜像" class="headerlink" title="2.1.4 删除镜像"></a>2.1.4 删除镜像</h3><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#1.如果觉得docker下载速度较慢，可使用下面命令更换docker加速器</span></span><br><span class="line"><span class="comment"># curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io</span></span><br><span class="line"></span><br><span class="line">docker version &gt;= 1.12</span><br><span class="line">&#123;<span class="string">&quot;registry-mirrors&quot;</span>: [<span class="string">&quot;http://f1361db2.m.daocloud.io&quot;</span>]&#125;</span><br><span class="line">Success.</span><br><span class="line">You need to restart docker to take effect: sudo systemctl restart docker</span><br><span class="line"></span><br><span class="line">systemctl restart docker</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.下载 体积较小的hello-world镜像进行验证</span></span><br><span class="line">docker pull hello-world</span><br><span class="line"><span class="comment">#根据镜像的 id，名字，摘要等</span></span><br><span class="line"><span class="comment">#被删除的镜像，不得有依赖的容器记录docker rmi hello-world</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#3.删除容器记录</span></span><br><span class="line">docker <span class="built_in">rm</span> 容器<span class="built_in">id</span></span><br><span class="line"><span class="comment">#4.指定id的前三位即可</span></span><br><span class="line">docker rmi 镜像<span class="built_in">id</span></span><br></pre></td></tr></table></figure>

<h3 id="2-1-5-镜像管理"><a href="#2-1-5-镜像管理" class="headerlink" title="2.1.5 镜像管理"></a>2.1.5 镜像管理</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">1.批量删除镜像，慎用</span></span><br><span class="line">docker rmi &#x27;docker images -aq&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">2.批量删除容器</span></span><br><span class="line">docker rm &#x27;docker ps -aq&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">3.导出镜像</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">比如默认运行的centos镜像，不提供vim功能，运行该容器后，在容器内安装vim</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">然后提交该镜像，再导出该镜像为压缩文件，可以发给其他人用</span></span><br><span class="line">docker commit --&gt;后续讲解</span><br><span class="line"></span><br><span class="line">docker image save centos:7.8.2003 &gt; /export/software/centos1.8.2003.tgz</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">或者</span></span><br><span class="line">docker save -o [保存的目标文件名称] [镜像名称]</span><br><span class="line">docker save -o nginx.tgz nginx:latest</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">4.导入镜像</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">先删除本地的nginx镜像：</span></span><br><span class="line">docker rmi centos:7.8.2003</span><br><span class="line">docker image load -i /export/software/centos1.8.2003.tgz</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">5.查看cocker服务的信息</span></span><br><span class="line">docker info</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">6.查看镜像详细信息</span></span><br><span class="line">docker image inspact 镜像id</span><br></pre></td></tr></table></figure>


<p><img src="/assets/image-20210731161354344.png" alt="image-20210731161354344"></p>
<h3 id="2-1-6-练习"><a href="#2-1-6-练习" class="headerlink" title="2.1.6 练习"></a>2.1.6 练习</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">需求：去DockerHub搜索并拉取一个Redis镜像</span><br><span class="line">目标：</span><br><span class="line">1）去DockerHub搜索Redis镜像</span><br><span class="line">2）查看Redis镜像的名称和版本</span><br><span class="line">3）利用docker pull命令拉取镜像</span><br><span class="line">4）利用docker save命令将 redis:latest打包为一个redis.tar包</span><br><span class="line">5）利用docker rmi 删除本地的redis:latest</span><br><span class="line">6）利用docker load 重新加载 redis.tar文件</span><br></pre></td></tr></table></figure>



<h2 id="2-2-容器操作"><a href="#2-2-容器操作" class="headerlink" title="2.2.容器操作"></a>2.2.容器操作</h2><h3 id="2-2-1-容器相关命令"><a href="#2-2-1-容器相关命令" class="headerlink" title="2.2.1.容器相关命令"></a>2.2.1.容器相关命令</h3><h5 id="（1）容器基本命令"><a href="#（1）容器基本命令" class="headerlink" title="（1）容器基本命令"></a>（1）容器基本命令</h5><p>容器操作的命令如图：</p>
<img src="assets/image-20210731161950495.png" alt="image-20210731161950495" style="zoom: 33%;" />

<p><code>容器保护三个状态：</code></p>
<ul>
<li><code>运行：进程正常运行</code></li>
<li><code>暂停：进程暂停，CPU不再运行，并不释放内存</code></li>
<li><code>停止：进程终止，回收进程占用的内存、CPU等资源</code></li>
</ul>
<p><code>其中：</code></p>
<ul>
<li><p><code>docker run</code>：创建并运行一个容器，处于运行状态</p>
</li>
<li><p><code>docker pause</code>：让一个运行的容器暂停</p>
</li>
<li><p><code>docker unpause</code>：让一个容器从暂停状态恢复运行</p>
</li>
<li><p><code>docker stop</code>：停止一个运行的容器</p>
</li>
<li><p><code>docker start</code>：让一个停止的容器再次运行</p>
</li>
<li><p><code>docker rm</code>：删除一个容器</p>
</li>
</ul>
<h5 id="（2）容器初体验"><a href="#（2）容器初体验" class="headerlink" title="（2）容器初体验"></a>（2）容器初体验</h5><p><code>docker run</code> 等于创建+启动</p>
<blockquote>
<p>docker run镜像名，如果镜像不存在本地，则会去在线下载该镜像</p>
</blockquote>
<blockquote>
<p><code>注意</code>:容器内的进程必须处于前台运行状态，否则容器就会直接退出，自己部署一个容器运行，命令不得后台运行，前台运行即可</p>
<p>如果容器内，什么事也没做，容器也会挂掉，<code>容器内，必须有一个进程在前台运行</code></p>
<p>我们运行nginx基础镜像，没有运行任何程序，因此容器直接挂掉</p>
</blockquote>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#运行容器的玩法</span></span><br><span class="line"><span class="comment">#1.运行一个挂掉的容器（从错误示范学起）</span></span><br><span class="line">docker run centos:7.8:2003</span><br><span class="line"><span class="comment">#这个写法会产生多个独立的容器记录，且容器内没有程序在跑，因此挂了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.运行容器，且进入容器内，且在容器内执行某个命令</span></span><br><span class="line">[root@node1 ~]<span class="comment"># docker run -it centos:7.8.2003 sh</span></span><br><span class="line">sh-4.2<span class="comment">#</span></span><br><span class="line">sh-4.2<span class="comment">#</span></span><br><span class="line">sh-4.2<span class="comment"># cat /etc/redhat-release</span></span><br><span class="line">Centos Linux release 7.8.2003 (Core)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.开启一个容器，让它帮你运行某个程序，属于前台运行，会卡住一个终端</span></span><br><span class="line">[root@node1 ~]<span class="comment"># docker run centos:7.8.2003 ping baidu.com</span></span><br><span class="line">PING baidu.com (198.18,0.13) 56(84) bytes of data.</span><br><span class="line"></span><br><span class="line"><span class="comment">#4.运行一个活着的容器，docker ps可以看到的容器</span></span><br><span class="line"><span class="comment"># -d 参数，让容器在后台跑着 (针对宿主机而言)</span></span><br><span class="line"><span class="comment"># 返回容器id</span></span><br><span class="line">docker run -d centos:7.8.2003 ping baidu.com</span><br><span class="line">608dd314f21c931c35c2a0ebf29b021c7634dfd26e4d757cf542f0eff6c64727</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.丰富docker运行的参数</span></span><br><span class="line"><span class="comment"># -d 后台运行</span></span><br><span class="line"><span class="comment"># --rm 容器挂掉后自动被删除</span></span><br><span class="line"><span class="comment"># --name 给容器起个名字</span></span><br><span class="line">[root@node1 ~]<span class="comment"># docker run -d --rm --name pythonav centos:7.8.2003 pingpythonav.cn</span></span><br><span class="line">e3a7f6a7d0c6902d6af565b4bb9cf81a62d4840bbac978105b6d8da233f532a5</span><br><span class="line">[root@node1 ~]<span class="comment"># docker ps</span></span><br><span class="line">CONTAINER 		IDIMAGE			COMMAND			CREATED				STATUS</span><br><span class="line">PORTS		NAMES</span><br><span class="line">e3a7f6a7d0c6 centos:7.8.2003	<span class="string">&quot;ping pythonav.cn&quot;</span>	5 seconds ago	  Up 5 seconds	 		pythonav</span><br><span class="line"></span><br><span class="line"><span class="comment">#6.查看容器日志的玩法，刷新日志</span></span><br><span class="line"><span class="comment"># docker logs -f 容器id</span></span><br><span class="line">docker logs -f f2598cb26363</span><br><span class="line"><span class="comment">#查看最后五条</span></span><br><span class="line">docker logs f2598cb26363 | <span class="built_in">tail</span> -5</span><br><span class="line"></span><br><span class="line"><span class="comment">#7.进入正在运行的容器空间内</span></span><br><span class="line"><span class="comment">#exec指令用于进入容器空间内</span></span><br><span class="line">docker <span class="built_in">exec</span> -it f2598cb26363 bash</span><br><span class="line"></span><br><span class="line"><span class="comment">#8.查看容器的详细信息，用于高级的调试</span></span><br><span class="line">docker container inspect 容器<span class="built_in">id</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 9.容器的端口映射</span></span><br><span class="line"><span class="comment"># 后台运行nginx容器，且起个名字，且端口映射宿主机的80端口，访问到容器内的80端口</span></span><br><span class="line">docker run -d --name bigdata_nginx -p 85:80 nginx</span><br><span class="line"><span class="comment"># 查看容器</span></span><br><span class="line">[root@yc_docker81 ~]<span class="comment"># docker ps</span></span><br><span class="line">CONTAINER ID	IMAGE	COMMAND		CREATED		STATUS		PORTS	NAMES</span><br><span class="line">2e73fac44507	nginx	<span class="string">&quot;/docker-entrypoint ....&quot;</span>  5 seconds ago	Up 4 seconds</span><br><span class="line">0.0.0.0:80-&gt;80/tcp,:::80-&gt;80/tcp  bigdata_nginx</span><br><span class="line"></span><br><span class="line"><span class="comment">#9.1查看容器的端口转发情况</span></span><br><span class="line">docker port 容器<span class="built_in">id</span></span><br><span class="line"><span class="comment"># docker port 2e73fac44507</span></span><br><span class="line">80/tcp -&gt; 0.0.0.0:85</span><br><span class="line">80/tcp -&gt; :::85</span><br><span class="line"></span><br><span class="line"><span class="comment">#9.2随机端口映射 -P 随机访问一个宿主机的空闲端口，映射到容器内打开的端口</span></span><br><span class="line">docker run -d --name bigdata_nginx_random -P nginx</span><br><span class="line"></span><br><span class="line"><span class="comment">#10.容器的提交</span></span><br><span class="line"><span class="comment"># 运行基础的centos:7.8.2003 ，在容器内安装vim，然后提交新的镜像</span></span><br><span class="line"><span class="comment">#新的镜像，再运行出的容器，默认就携带vim了</span></span><br><span class="line">[root@node1 ~]<span class="comment"># docker run -it centos:7.8.2003 bash</span></span><br><span class="line"><span class="comment">#提交容器</span></span><br><span class="line">docker commit 容器<span class="built_in">id</span> 新的镜像名</span><br><span class="line"></span><br><span class="line">[root@node1 ~]<span class="comment"># docker run -it bigdata163/centos-vim-7.8.2003 bash</span></span><br><span class="line">[root@f01fd5201723 /]<span class="comment"># vim xiake.txt</span></span><br><span class="line">[root@f01fd5201723 /]<span class="comment"># cat xiake.txt</span></span><br><span class="line">xin ku le peng you men ! xiake xiu xi !</span><br><span class="line"></span><br><span class="line"><span class="comment">#11.停止容器运行</span></span><br><span class="line">docker stop 容器<span class="built_in">id</span></span><br></pre></td></tr></table></figure>

<img src=".\assets\image-20230207155553757.png" alt="image-20230207155553757" style="zoom: 80%;" />





<h3 id="2-2-2-案例-创建并运行一个容器"><a href="#2-2-2-案例-创建并运行一个容器" class="headerlink" title="2.2.2 案例-创建并运行一个容器"></a>2.2.2 案例-创建并运行一个容器</h3><p>创建并运行nginx容器的命令：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name containerName -p 80:80 -d nginx</span><br></pre></td></tr></table></figure>

<p>命令解读：</p>
<ul>
<li>docker run ：创建并运行一个容器</li>
<li>–name : 给容器起一个名字，比如叫做mn</li>
<li>-p ：将宿主机端口与容器端口映射，冒号左侧是宿主机端口，右侧是容器端口</li>
<li>-d：后台运行容器</li>
<li>nginx：镜像名称，例如nginx</li>
</ul>
<p>这里的<code>-p</code>参数，是将容器端口映射到宿主机端口。</p>
<p>默认情况下，容器是隔离环境，我们直接访问宿主机的80端口，肯定访问不到容器中的nginx。</p>
<p>现在，将容器的80与宿主机的80关联起来，当我们访问宿主机的80端口时，就会被映射到容器的80，这样就能访问到nginx了：</p>
<img src="assets/image-20210731163255863.png" alt="image-20210731163255863" style="zoom:33%;" />



<h3 id="2-2-3案例-进入容器，修改文件"><a href="#2-2-3案例-进入容器，修改文件" class="headerlink" title="2.2.3案例-进入容器，修改文件"></a>2.2.3案例-进入容器，修改文件</h3><p><strong>需求</strong>：进入Nginx容器，修改HTML文件内容，添加“人工智能学院欢迎您”</p>
<p><strong>提示</strong>：进入容器要用到docker exec命令。</p>
<p><strong>步骤</strong>：</p>
<p>1）进入容器。进入我们刚刚创建的nginx容器的命令为：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mn bash</span><br></pre></td></tr></table></figure>

<p>命令解读：</p>
<ul>
<li><p>docker exec ：进入容器内部，执行一个命令</p>
</li>
<li><p>-it : 给当前进入的容器创建一个标准输入、输出终端，允许我们与容器交互</p>
</li>
<li><p>mn ：要进入的容器的名称</p>
</li>
<li><p>bash：进入容器后执行的命令，bash是一个linux终端交互命令</p>
</li>
</ul>
<p>2）进入nginx的HTML所在目录 &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html</p>
<p>容器内部会模拟一个独立的Linux文件系统，看起来如同一个linux服务器一样：</p>
<p><img src="/assets/image-20210731164159811.png" alt="image-20210731164159811"></p>
<p>nginx的环境、配置、运行文件全部都在这个文件系统中，包括我们要修改的html文件。</p>
<p>查看DockerHub网站中的nginx页面，可以知道nginx的html目录位置在<code>/usr/share/nginx/html</code></p>
<p>我们执行命令，进入该目录：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/share/nginx/html</span><br></pre></td></tr></table></figure>

<p> 查看目录下文件：</p>
<p><img src="/assets/image-20210731164455818.png" alt="image-20210731164455818"></p>
<p>3）修改index.html的内容</p>
<p>容器内没有vi命令，无法直接修改，我们用下面的命令来修改：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i -e <span class="string">&#x27;s#Welcome to nginx#人工智能学院欢迎您#g&#x27;</span> -e <span class="string">&#x27;s#&lt;head&gt;#&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;&gt;#g&#x27;</span> index.html</span><br></pre></td></tr></table></figure>

<p>在浏览器访问自己的虚拟机地址，例如我的是：<a href="http://192.168.88.163，即可看到结果：">http://192.168.88.163，即可看到结果：</a></p>
<h3 id="2-2-2-小结"><a href="#2-2-2-小结" class="headerlink" title="2.2.2.小结"></a>2.2.2.小结</h3><p>docker run命令的常见参数有哪些？</p>
<ul>
<li><code>--name</code>：指定容器名称</li>
<li><code>-p</code>：指定端口映射</li>
<li><code>-d</code>：让容器后台运行</li>
</ul>
<p>查看容器日志的命令：</p>
<ul>
<li><code>docker logs</code></li>
<li>添加<code>-f</code>参数可以持续查看日志</li>
</ul>
<p>查看容器状态：</p>
<ul>
<li><code>docker ps</code></li>
<li><code>docker ps -a </code>查看所有容器，包括已经停止的</li>
</ul>
<h2 id="2-3-数据卷（容器数据管理）"><a href="#2-3-数据卷（容器数据管理）" class="headerlink" title="2.3.数据卷（容器数据管理）"></a>2.3.数据卷（容器数据管理）</h2><p>在之前的nginx案例中，修改nginx的html页面时，需要进入nginx内部。并且因为没有编辑器，修改文件也很麻烦。</p>
<p>这就是因为容器与数据（容器内文件）耦合带来的后果。</p>
<img src="assets/image-20210731172440275.png" alt="image-20210731172440275" style="zoom: 33%;" />

<p>要解决这个问题，必须将数据与容器解耦，这就要用到数据卷了。</p>
<h3 id="2-3-1-什么是数据卷"><a href="#2-3-1-什么是数据卷" class="headerlink" title="2.3.1.什么是数据卷"></a>2.3.1.什么是数据卷</h3><p><strong>数据卷（volume）</strong>是一个虚拟目录，指向宿主机文件系统中的某个目录。</p>
<img src="assets/image-20210731173541846.png" alt="image-20210731173541846" style="zoom: 50%;" />

<p>一旦完成数据卷挂载，对容器的一切操作都会作用在数据卷对应的宿主机目录了。</p>
<p>这样，我们操作宿主机的<code>/var/lib/docker/volumes/html</code>目录，就等于操作容器内的<code>/usr/share/nginx/html</code>目录了</p>
<p><img src="/.%5Cassets%5Cimage-20230207170058954.png" alt="image-20230207170058954"></p>
<h3 id="2-3-2-数据集操作命令"><a href="#2-3-2-数据集操作命令" class="headerlink" title="2.3.2.数据集操作命令"></a>2.3.2.数据集操作命令</h3><p>数据卷操作的基本语法如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker volume [COMMAND]</span><br></pre></td></tr></table></figure>

<p><code>docker volume</code>命令是数据卷操作，根据命令后跟随的<code>command</code>来确定下一步的操作：</p>
<ul>
<li><code>create </code>创建一个<code>volume</code></li>
<li><code>inspect </code>显示一个或多个<code>volume</code>的信息</li>
<li><code>ls </code>列出所有的<code>volume</code></li>
<li><code>prune </code>删除未使用的<code>volume</code></li>
<li><code>rm </code>删除一个或多个指定的<code>volume</code></li>
</ul>
<h3 id="2-3-3-创建和查看数据卷"><a href="#2-3-3-创建和查看数据卷" class="headerlink" title="2.3.3.创建和查看数据卷"></a>2.3.3.创建和查看数据卷</h3><p><strong>需求</strong>：创建一个数据卷，并查看数据卷在宿主机的目录位置</p>
<p>① 创建数据卷</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker volume create html</span><br></pre></td></tr></table></figure>

<p>② 查看所有数据</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker volume <span class="built_in">ls</span></span><br></pre></td></tr></table></figure>

<p>结果：</p>
<p><img src="/assets/image-20210731173746910.png" alt="image-20210731173746910"></p>
<p>③ 查看数据卷详细信息卷</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker volume inspect html</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<p><img src="/assets/image-20210731173809877.png" alt="image-20210731173809877"></p>
<p>可以看到，我们创建的html这个数据卷关联的宿主机目录为<code>/var/lib/docker/volumes/html/_data</code>目录。</p>
<p><strong>小结</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">数据卷的作用：</span><br><span class="line"></span><br><span class="line">- 将容器与数据分离，解耦合，方便操作容器内数据，保证数据安全</span><br><span class="line"></span><br><span class="line">数据卷操作：</span><br><span class="line"></span><br><span class="line">- docker volume create：创建数据卷</span><br><span class="line">- docker volume ls：查看所有数据卷</span><br><span class="line">- docker volume inspect：查看数据卷详细信息，包括关联的宿主机目录位置</span><br><span class="line">- docker volume rm：删除指定数据卷</span><br><span class="line">- docker volume prune：删除所有未使用的数据卷</span><br></pre></td></tr></table></figure>



<h3 id="2-3-4-挂载数据卷"><a href="#2-3-4-挂载数据卷" class="headerlink" title="2.3.4.挂载数据卷"></a>2.3.4.挂载数据卷</h3><p>我们在创建容器时，可以通过 -v 参数来挂载一个数据卷到某个容器内目录，命令格式如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run \</span><br><span class="line">  -it</span><br><span class="line">  --name bigdata \</span><br><span class="line">  -v /export/data/docker-data/centos/:/root/data_container \</span><br><span class="line">  centos:7.8.2003 \</span><br></pre></td></tr></table></figure>

<p>这里的-v就是挂载数据卷的命令：</p>
<ul>
<li><code>-v /export/data/docker-data/centos/:/root/data_container</code> ：把<code>/export/data/docker-data/centos/</code>数据卷挂载到容器内的<code>/root/data_container</code>这个目录中</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">注意事项：</span><br><span class="line">1. 目录必须是绝对路径</span><br><span class="line">2. 如果目录不存在，会自动创建</span><br><span class="line">3. 可以挂载多个数据卷</span><br></pre></td></tr></table></figure>

<img src=".\assets\image-20230207173504996.png" alt="image-20230207173504996" style="zoom:67%;" />

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">实现多容器进行数据交换的方法</span><br><span class="line"></span><br><span class="line"><span class="comment">#1. 多个容器挂载同一个数据卷</span></span><br><span class="line"><span class="comment">#2. 数据卷容器</span></span><br></pre></td></tr></table></figure>

<img src=".\assets\image-20230207173246227.png" alt="image-20230207173246227" style="zoom:67%;" />

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">配置数据卷容器</span><br><span class="line"></span><br><span class="line"><span class="comment">#1.创建启动c3数据卷容器，使用–v 参数设置数据卷</span></span><br><span class="line"><span class="comment">#（这里-v /volume是容器目录，系统会自动分配一个宿主机数据卷目录）</span></span><br><span class="line">docker run –it --name=c3 –v /volume centos:7</span><br><span class="line"></span><br><span class="line"><span class="comment">#2. 创建启动c1 c2 容器，使用--volumes-from 参数设置数据卷</span></span><br><span class="line">docker run –it --name=c1 --volumes-from c3 centos:7</span><br><span class="line">docker run –it --name=c2 --volumes-from c3 centos:7</span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">小结</span><br><span class="line">1.数据卷概念</span><br><span class="line">	•宿主机的一个目录或文件</span><br><span class="line">2.数据卷作用</span><br><span class="line">	•容器数据持久化</span><br><span class="line">	•客户端和容器数据交换</span><br><span class="line">	•容器间数据交换</span><br><span class="line">3.数据卷容器</span><br><span class="line">	•创建一个容器，挂载一个目录，让其他容器继承自该容器( --volume-from )。</span><br><span class="line">	•通过简单方式实现数据卷配置</span><br></pre></td></tr></table></figure>

<h3 id="2-3-5-案例-给nginx挂载数据卷"><a href="#2-3-5-案例-给nginx挂载数据卷" class="headerlink" title="2.3.5.案例-给nginx挂载数据卷"></a>2.3.5.案例-给nginx挂载数据卷</h3><p><strong>需求</strong>：创建一个nginx容器，修改容器内的html目录内的index.html内容</p>
<p><strong>分析</strong>：上个案例中，我们进入nginx容器内部，已经知道nginx的html目录所在位置<code>/usr/share/nginx/html</code> ，我们需要把这个目录挂载到<code>/export/data/docker-data/nginx-html/</code>这个数据卷上，方便操作其中的内容。</p>
<p><strong>提示</strong>：运行容器时使用 -v 参数挂载数据卷</p>
<p>步骤：</p>
<p>① 创建容器并挂载数据卷到容器内的HTML目录</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name mn -v /export/data/docker-data/nginx-html/:/usr/share/nginx/html -p 80:80 -d nginx</span><br></pre></td></tr></table></figure>

<p>② 进入html数据卷所在位置，并修改HTML内容</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看html数据卷的位置</span></span><br><span class="line">docker volume inspect /export/data/docker-data/nginx-html/</span><br><span class="line"><span class="comment"># 进入该目录</span></span><br><span class="line"><span class="built_in">cd</span> /export/data/docker-data/nginx-html/_data</span><br><span class="line"><span class="comment"># 修改文件</span></span><br><span class="line">vi index.html</span><br></pre></td></tr></table></figure>



<h3 id="2-3-6-案例-给MySQL挂载本地目录"><a href="#2-3-6-案例-给MySQL挂载本地目录" class="headerlink" title="2.3.6.案例-给MySQL挂载本地目录"></a>2.3.6.案例-给MySQL挂载本地目录</h3><p>容器不仅仅可以挂载数据卷，也可以直接挂载到宿主机目录上。关联关系如下：</p>
<ul>
<li>带数据卷模式：宿主机目录 –&gt; 数据卷 —&gt; 容器内目录</li>
<li>直接挂载模式：宿主机目录 —&gt; 容器内目录</li>
</ul>
<p>如图：</p>
<p><img src="/assets/image-20210731175155453.png" alt="image-20210731175155453"></p>
<p><strong>语法</strong>：</p>
<p>目录挂载与数据卷挂载的语法是类似的：</p>
<ul>
<li>-v [宿主机目录]:[容器内目录]</li>
<li>-v [宿主机文件]:[容器内文件]</li>
</ul>
<p><strong>需求</strong>：创建并运行一个MySQL容器，将宿主机目录直接挂载到容器</p>
<p>实现思路如下：</p>
<p>1）在将课前资料中的mysql.tar文件上传到虚拟机，通过load命令加载为镜像</p>
<p>2）创建目录&#x2F;tmp&#x2F;mysql&#x2F;data</p>
<p>3）创建目录&#x2F;tmp&#x2F;mysql&#x2F;conf，将课前资料提供的hmy.cnf文件上传到&#x2F;tmp&#x2F;mysql&#x2F;conf</p>
<p>4）去DockerHub查阅资料，创建并运行MySQL容器，要求：</p>
<p>① 挂载&#x2F;tmp&#x2F;mysql&#x2F;data到mysql容器内数据存储目录</p>
<p>② 挂载&#x2F;tmp&#x2F;mysql&#x2F;conf&#x2F;hmy.cnf到mysql容器的配置文件</p>
<p>③ 设置MySQL密码</p>
<h3 id="2-3-7-小结"><a href="#2-3-7-小结" class="headerlink" title="2.3.7.小结"></a>2.3.7.小结</h3><p>docker run的命令中通过 -v 参数挂载文件或目录到容器中：</p>
<ul>
<li><code>-v volume</code>名称:容器内目录</li>
<li><code>-v</code> 宿主机文件:容器内文</li>
<li><code>-v </code>宿主机目录:容器内目录</li>
</ul>
<p>数据卷挂载与目录直接挂载的</p>
<ul>
<li>数据卷挂载耦合度低，由docker来管理目录，但是目录较深，不好找</li>
<li>目录挂载耦合度高，需要我们自己管理目录，不过目录容易寻找查看</li>
</ul>
<h1 id="3-Docker应用部署"><a href="#3-Docker应用部署" class="headerlink" title="3.Docker应用部署"></a>3.Docker应用部署</h1><h3 id="（1）部署MySQL"><a href="#（1）部署MySQL" class="headerlink" title="（1）部署MySQL"></a>（1）部署MySQL</h3><ol>
<li>搜索mysql镜像</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search mysql</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>拉取mysql镜像</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull mysql:5.6</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>创建容器，设置端口映射、目录映射</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在/export/data/docker-data/目录下创建mysql目录用于存储mysql数据信息</span></span><br><span class="line">mkdir -p /export/data/docker-data/mysql</span><br><span class="line">cd /export/data/docker-data/mysql</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run -id \</span><br><span class="line">-p 3306:3306 \</span><br><span class="line">--name=bigdata_mysql \</span><br><span class="line">-v $PWD/conf:/etc/mysql/conf.d \</span><br><span class="line">-v $PWD/logs:/logs \</span><br><span class="line">-v $PWD/data:/var/lib/mysql \</span><br><span class="line">-e MYSQL_ROOT_PASSWORD=hadoop \</span><br><span class="line">mysql:5.7.29</span><br></pre></td></tr></table></figure>

<ul>
<li>参数说明：<ul>
<li><strong>-p 3306:3306</strong>：将容器的 3306 端口映射到宿主机的 33076端口。</li>
<li><strong>-v $PWD&#x2F;conf:&#x2F;etc&#x2F;mysql&#x2F;conf.d</strong>：将主机当前目录下的 conf&#x2F;my.cnf 挂载到容器的 &#x2F;etc&#x2F;mysql&#x2F;my.cnf。配置目录</li>
<li><strong>-v $PWD&#x2F;logs:&#x2F;logs</strong>：将主机当前目录下的 logs 目录挂载到容器的 &#x2F;logs。日志目录</li>
<li><strong>-v $PWD&#x2F;data:&#x2F;var&#x2F;lib&#x2F;mysql</strong> ：将主机当前目录下的data目录挂载到容器的 &#x2F;var&#x2F;lib&#x2F;mysql 。数据目录</li>
<li><strong>-e MYSQL_ROOT_PASSWORD&#x3D;hadoop：</strong>初始化 root 用户的密码。</li>
</ul>
</li>
</ul>
<ol start="4">
<li>进入容器，操作mysql</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec –it bigdata_mysql /bin/bash</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>使用外部机器连接容器中的mysql</li>
</ol>
<p><img src="/.%5Cassets%5C1573636765632.png" alt="1573636765632"></p>
<h3 id="（2）部署Tomcat"><a href="#（2）部署Tomcat" class="headerlink" title="（2）部署Tomcat"></a>（2）部署Tomcat</h3><ol>
<li>搜索tomcat镜像</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search tomcat</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>拉取tomcat镜像</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull tomcat</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>创建容器，设置端口映射、目录映射</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在/root目录下创建tomcat目录用于存储tomcat数据信息</span></span><br><span class="line">mkdir ~/tomcat</span><br><span class="line">cd ~/tomcat</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -id --name=c_tomcat \</span><br><span class="line">-p 8080:8080 \</span><br><span class="line">-v $PWD:/usr/local/tomcat/webapps \</span><br><span class="line">tomcat </span><br></pre></td></tr></table></figure>

<ul>
<li><p>参数说明：</p>
<ul>
<li><p><strong>-p 8080:8080：</strong>将容器的8080端口映射到主机的8080端口</p>
<p><strong>-v $PWD:&#x2F;usr&#x2F;local&#x2F;tomcat&#x2F;webapps：</strong>将主机中当前目录挂载到容器的webapps</p>
</li>
</ul>
</li>
</ul>
<ol start="4">
<li>使用外部机器访问tomcat</li>
</ol>
<p><img src="/.%5Cassets%5C1573649804623.png" alt="1573649804623"></p>
<h3 id="（3）部署Nginx"><a href="#（3）部署Nginx" class="headerlink" title="（3）部署Nginx"></a>（3）部署Nginx</h3><ol>
<li>搜索nginx镜像</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search nginx</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>拉取nginx镜像</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull nginx</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>创建容器，设置端口映射、目录映射</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在/root目录下创建nginx目录用于存储nginx数据信息</span></span><br><span class="line">mkdir ~/nginx</span><br><span class="line">cd ~/nginx</span><br><span class="line">mkdir conf</span><br><span class="line">cd conf</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在~/nginx/conf/下创建nginx.conf文件,粘贴下面内容</span></span><br><span class="line">vim nginx.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">user  nginx;</span><br><span class="line">worker_processes  1;</span><br><span class="line"></span><br><span class="line">error_log  /var/log/nginx/error.log warn;</span><br><span class="line">pid        /var/run/nginx.pid;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    include       /etc/nginx/mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line"></span><br><span class="line">    log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;</span><br><span class="line">                      &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;</span><br><span class="line">                      &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;</span><br><span class="line"></span><br><span class="line">    access_log  /var/log/nginx/access.log  main;</span><br><span class="line"></span><br><span class="line">    sendfile        on;</span><br><span class="line">    #tcp_nopush     on;</span><br><span class="line"></span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line"></span><br><span class="line">    #gzip  on;</span><br><span class="line"></span><br><span class="line">    include /etc/nginx/conf.d/*.conf;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>




<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -id --name=c_nginx \</span><br><span class="line">-p 80:80 \</span><br><span class="line">-v $PWD/conf/nginx.conf:/etc/nginx/nginx.conf \</span><br><span class="line">-v $PWD/logs:/var/log/nginx \</span><br><span class="line">-v $PWD/html:/usr/share/nginx/html \</span><br><span class="line">nginx</span><br></pre></td></tr></table></figure>

<ul>
<li>参数说明：<ul>
<li><strong>-p 80:80</strong>：将容器的 80端口映射到宿主机的 80 端口。</li>
<li><strong>-v $PWD&#x2F;conf&#x2F;nginx.conf:&#x2F;etc&#x2F;nginx&#x2F;nginx.conf</strong>：将主机当前目录下的 &#x2F;conf&#x2F;nginx.conf 挂载到容器的 :&#x2F;etc&#x2F;nginx&#x2F;nginx.conf。配置目录</li>
<li><strong>-v $PWD&#x2F;logs:&#x2F;var&#x2F;log&#x2F;nginx</strong>：将主机当前目录下的 logs 目录挂载到容器的&#x2F;var&#x2F;log&#x2F;nginx。日志目录</li>
</ul>
</li>
</ul>
<ol start="4">
<li>使用外部机器访问nginx</li>
</ol>
<p><img src="/.%5Cassets%5C1573652396669.png" alt="1573652396669"></p>
<h3 id="（4）部署Redis"><a href="#（4）部署Redis" class="headerlink" title="（4）部署Redis"></a>（4）部署Redis</h3><ol>
<li>搜索redis镜像</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker search redis</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>拉取redis镜像</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull redis:5.0</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>创建容器，设置端口映射</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -id --name=c_redis -p 6379:6379 redis:5.0</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>使用外部机器连接redis</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./redis-cli.exe -h 192.168.149.135 -p 6379</span><br></pre></td></tr></table></figure>



<h1 id="4-Dockerfile自定义镜像"><a href="#4-Dockerfile自定义镜像" class="headerlink" title="4.Dockerfile自定义镜像"></a>4.Dockerfile自定义镜像</h1><p>常见的镜像在DockerHub就能找到，但是我们自己写的项目就必须自己构建镜像了。</p>
<p>而要自定义镜像，就必须先了解镜像的结构才行。</p>
<h2 id="4-1-镜像结构"><a href="#4-1-镜像结构" class="headerlink" title="4.1.镜像结构"></a>4.1.镜像结构</h2><p>镜像是将应用程序及其需要的系统函数库、环境、配置、依赖打包而成。</p>
<p>我们以MySQL为例，来看看镜像的组成结构：</p>
<img src="assets/image-20210731175806273.png" alt="image-20210731175806273" style="zoom: 25%;" />



<p>简单来说，镜像就是在系统函数库、运行环境基础上，添加应用程序文件、配置文件、依赖文件等组合，然后编写好启动脚本打包在一起形成的文件。</p>
<p>我们要构建镜像，其实就是实现上述打包的过程。</p>
<h2 id="4-2-Dockerfile语法"><a href="#4-2-Dockerfile语法" class="headerlink" title="4.2.Dockerfile语法"></a>4.2.Dockerfile语法</h2><p>构建自定义的镜像时，并不需要一个个文件去拷贝，打包。</p>
<p>我们只需要告诉Docker，我们的镜像的组成，需要哪些BaseImage、需要拷贝什么文件、需要安装什么依赖、启动脚本是什么，将来Docker会帮助我们构建镜像。</p>
<p>而描述上述信息的文件就是Dockerfile文件。</p>
<p><strong>Dockerfile</strong>就是一个文本文件，其中包含一个个的**指令(Instruction)**，用指令来说明要执行什么操作来构建镜像。每一个指令都会形成一层Layer。</p>
<img src="assets/image-20210731180321133.png" alt="image-20210731180321133" style="zoom:33%;" />



<p>更新详细语法说明，请参考官网文档： <a target="_blank" rel="noopener" href="https://docs.docker.com/engine/reference/builder">https://docs.docker.com/engine/reference/builder</a></p>
<h2 id="4-3-构建Java项目"><a href="#4-3-构建Java项目" class="headerlink" title="4.3.构建Java项目"></a>4.3.构建Java项目</h2><h3 id="4-3-1-基于Ubuntu构建Java项目"><a href="#4-3-1-基于Ubuntu构建Java项目" class="headerlink" title="4.3.1.基于Ubuntu构建Java项目"></a>4.3.1.基于Ubuntu构建Java项目</h3><p>需求：基于Ubuntu镜像构建一个新镜像，运行一个java项目</p>
<ul>
<li><p>步骤1：新建一个空文件夹docker-demo</p>
<p><img src="/assets/image-20210801101207444.png" alt="image-20210801101207444"></p>
</li>
<li><p>步骤2：拷贝课前资料中的docker-demo.jar文件到docker-demo这个目录</p>
<p><img src="/assets/image-20210801101314816.png" alt="image-20210801101314816"></p>
</li>
<li><p>步骤3：拷贝课前资料中的jdk8.tar.gz文件到docker-demo这个目录</p>
<p><img src="/assets/image-20210801101410200.png" alt="image-20210801101410200"></p>
</li>
<li><p>步骤4：拷贝课前资料提供的Dockerfile到docker-demo这个目录</p>
<p><img src="/assets/image-20210801101455590.png" alt="image-20210801101455590"></p>
<p>其中的内容如下：</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定基础镜像</span></span><br><span class="line"><span class="keyword">FROM</span> ubuntu:<span class="number">16.04</span></span><br><span class="line"><span class="comment"># 配置环境变量，JDK的安装目录</span></span><br><span class="line"><span class="keyword">ENV</span> JAVA_DIR=/usr/local</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拷贝jdk和java项目的包</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> ./jdk8.tar.gz <span class="variable">$JAVA_DIR</span>/</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> ./docker-demo.jar /tmp/app.jar</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装JDK</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">cd</span> <span class="variable">$JAVA_DIR</span> \</span></span><br><span class="line"><span class="language-bash"> &amp;&amp; tar -xf ./jdk8.tar.gz \</span></span><br><span class="line"><span class="language-bash"> &amp;&amp; <span class="built_in">mv</span> ./jdk1.8.0_144 ./java8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置环境变量</span></span><br><span class="line"><span class="keyword">ENV</span> JAVA_HOME=$JAVA_DIR/java8</span><br><span class="line"><span class="keyword">ENV</span> PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 暴露端口</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8090</span></span><br><span class="line"><span class="comment"># 入口，java项目的启动命令</span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> java -jar /tmp/app.jar</span></span><br></pre></td></tr></table></figure>


</li>
<li><p>步骤5：进入docker-demo</p>
<p>将准备好的docker-demo上传到虚拟机任意目录，然后进入docker-demo目录下</p>
</li>
<li><p>步骤6：运行命令：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t javaweb:1.0 .</span><br></pre></td></tr></table></figure></li>
</ul>
<p>最后访问 <a target="_blank" rel="noopener" href="http://192.168.150.101:8090/hello/count%EF%BC%8C%E5%85%B6%E4%B8%AD%E7%9A%84ip%E6%94%B9%E6%88%90%E4%BD%A0%E7%9A%84%E8%99%9A%E6%8B%9F%E6%9C%BAip">http://192.168.150.101:8090/hello/count，其中的ip改成你的虚拟机ip</a></p>
<h3 id="4-3-2-基于java8构建Java项目"><a href="#4-3-2-基于java8构建Java项目" class="headerlink" title="4.3.2.基于java8构建Java项目"></a>4.3.2.基于java8构建Java项目</h3><p>虽然我们可以基于Ubuntu基础镜像，添加任意自己需要的安装包，构建镜像，但是却比较麻烦。所以大多数情况下，我们都可以在一些安装了部分软件的基础镜像上做改造。</p>
<p>例如，构建java项目的镜像，可以在已经准备了JDK的基础镜像基础上构建。</p>
<p>需求：基于java:8-alpine镜像，将一个Java项目构建为镜像</p>
<p>实现思路如下：</p>
<ul>
<li><p>① 新建一个空的目录，然后在目录中新建一个文件，命名为Dockerfile</p>
</li>
<li><p>② 拷贝课前资料提供的docker-demo.jar到这个目录中</p>
</li>
<li><p>③ 编写Dockerfile文件：</p>
<ul>
<li><p>a ）基于java:8-alpine作为基础镜像</p>
</li>
<li><p>b ）将app.jar拷贝到镜像中</p>
</li>
<li><p>c ）暴露端口</p>
</li>
<li><p>d ）编写入口ENTRYPOINT</p>
<p>内容如下：</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> java:<span class="number">8</span>-alpine</span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> ./app.jar /tmp/app.jar</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">8090</span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> java -jar /tmp/app.jar</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>④ 使用docker build命令构建镜像</p>
</li>
<li><p>⑤ 使用docker run创建容器并运行</p>
</li>
</ul>
<h2 id="4-4-小结"><a href="#4-4-小结" class="headerlink" title="4.4.小结"></a>4.4.小结</h2><p>小结：</p>
<ol>
<li><p>Dockerfile的本质是一个文件，通过指令描述镜像的构建过程</p>
</li>
<li><p>Dockerfile的第一行必须是FROM，从一个基础镜像来构建</p>
</li>
<li><p>基础镜像可以是基本操作系统，如Ubuntu。也可以是其他人制作好的镜像，例如：java:8-alpine</p>
</li>
</ol>
<h1 id="5-Docker-Compose"><a href="#5-Docker-Compose" class="headerlink" title="5.Docker-Compose"></a>5.Docker-Compose</h1><p>Docker Compose可以基于Compose文件帮我们快速的部署分布式应用，而无需手动一个个创建和运行容器！</p>
<p><img src="/assets/image-20210731180921742.png" alt="image-20210731180921742"></p>
<h2 id="5-1-初识DockerCompose"><a href="#5-1-初识DockerCompose" class="headerlink" title="5.1.初识DockerCompose"></a>5.1.初识DockerCompose</h2><p>Compose文件是一个文本文件，通过指令定义集群中的每个容器如何运行。格式如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">version<span class="punctuation">:</span> <span class="string">&quot;3.8&quot;</span></span><br><span class="line"> services<span class="punctuation">:</span></span><br><span class="line">  mysql<span class="punctuation">:</span></span><br><span class="line">    image<span class="punctuation">:</span> mysql<span class="punctuation">:</span><span class="number">5.7</span><span class="number">.25</span></span><br><span class="line">    environment<span class="punctuation">:</span></span><br><span class="line">     MYSQL_ROOT_PASSWORD<span class="punctuation">:</span> <span class="number">123</span> </span><br><span class="line">    volumes<span class="punctuation">:</span></span><br><span class="line">     - <span class="string">&quot;/tmp/mysql/data:/var/lib/mysql&quot;</span></span><br><span class="line">     - <span class="string">&quot;/tmp/mysql/conf/hmy.cnf:/etc/mysql/conf.d/hmy.cnf&quot;</span></span><br><span class="line">  web<span class="punctuation">:</span></span><br><span class="line">    build<span class="punctuation">:</span> .</span><br><span class="line">    ports<span class="punctuation">:</span></span><br><span class="line">     - <span class="string">&quot;8090:8090&quot;</span></span><br></pre></td></tr></table></figure>

<p>上面的Compose文件就描述一个项目，其中包含两个容器：</p>
<ul>
<li>mysql：一个基于<code>mysql:5.7.25</code>镜像构建的容器，并且挂载了两个目录</li>
<li>web：一个基于<code>docker build</code>临时构建的镜像容器，映射端口时8090</li>
</ul>
<p>DockerCompose的详细语法参考官网：<a target="_blank" rel="noopener" href="https://docs.docker.com/compose/compose-file/">https://docs.docker.com/compose/compose-file/</a></p>
<p>其实DockerCompose文件可以看做是将多个docker run命令写到一个文件，只是语法稍有差异。</p>
<h2 id="5-2-安装DockerCompose"><a href="#5-2-安装DockerCompose" class="headerlink" title="5.2.安装DockerCompose"></a>5.2.安装DockerCompose</h2><p>参考课前资料</p>
<h4 id="1-下载"><a href="#1-下载" class="headerlink" title="1.下载"></a>1.下载</h4><p>Linux下需要通过命令下载：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">curl -L https://github.com/docker/compose/releases/download/1.23.1/docker-compose-`<span class="built_in">uname</span> -s`-`<span class="built_in">uname</span> -m` &gt; /usr/local/bin/docker-compose</span><br></pre></td></tr></table></figure>

<p>如果下载速度较慢，或者下载失败，可以使用课前资料提供的docker-compose文件：</p>
<p><img src="D:\222课程\spark+kafka教学文件\spark&kafka资料\容器化技术Docker精讲\day03-Docker\资料\assets\image-20210417133020614.png" alt="image-20210417133020614"></p>
<p>上传到<code>/usr/local/bin/</code>目录也可以。</p>
<h4 id="2-修改文件权限"><a href="#2-修改文件权限" class="headerlink" title="2.修改文件权限"></a>2.修改文件权限</h4><p>修改文件权限：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改权限</span></span><br><span class="line"><span class="built_in">chmod</span> +x /usr/local/bin/docker-compose</span><br></pre></td></tr></table></figure>

<h4 id="3-Base自动补全命令："><a href="#3-Base自动补全命令：" class="headerlink" title="3.Base自动补全命令："></a>3.Base自动补全命令：</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 补全命令</span></span><br><span class="line">curl -L https://raw.githubusercontent.com/docker/compose/1.29.1/contrib/completion/bash/docker-compose &gt; /etc/bash_completion.d/docker-compose</span><br></pre></td></tr></table></figure>

<p>如果这里出现错误，需要修改自己的hosts文件：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&quot;199.232.68.133 raw.githubusercontent.com&quot;</span> &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure>

<h2 id="5-3-部署微服务集群"><a href="#5-3-部署微服务集群" class="headerlink" title="5.3.部署微服务集群"></a>5.3.部署微服务集群</h2><p><strong>需求</strong>：将之前学习的cloud-demo微服务集群利用DockerCompose部署</p>
<p><strong>实现思路</strong>：</p>
<p>① 查看课前资料提供的cloud-demo文件夹，里面已经编写好了docker-compose文件</p>
<p>② 修改自己的cloud-demo项目，将数据库、nacos地址都命名为docker-compose中的服务名</p>
<p>③ 使用maven打包工具，将项目中的每个微服务都打包为app.jar</p>
<p>④ 将打包好的app.jar拷贝到cloud-demo中的每一个对应的子目录中</p>
<p>⑤ 将cloud-demo上传至虚拟机，利用 docker-compose up -d 来部署</p>
<h3 id="5-3-1-compose文件"><a href="#5-3-1-compose文件" class="headerlink" title="5.3.1.compose文件"></a>5.3.1.compose文件</h3><p>查看课前资料提供的cloud-demo文件夹，里面已经编写好了docker-compose文件，而且每个微服务都准备了一个独立的目录：</p>
<p><img src="/assets/image-20210731181341330.png" alt="image-20210731181341330"></p>
<p>内容如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&quot;3.2&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">nacos:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nacos/nacos-server</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">MODE:</span> <span class="string">standalone</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;8848:8848&quot;</span></span><br><span class="line">  <span class="attr">mysql:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">mysql:5.7.25</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">MYSQL_ROOT_PASSWORD:</span> <span class="number">123</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;$PWD/mysql/data:/var/lib/mysql&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;$PWD/mysql/conf:/etc/mysql/conf.d/&quot;</span></span><br><span class="line">  <span class="attr">userservice:</span></span><br><span class="line">    <span class="attr">build:</span> <span class="string">./user-service</span></span><br><span class="line">  <span class="attr">orderservice:</span></span><br><span class="line">    <span class="attr">build:</span> <span class="string">./order-service</span></span><br><span class="line">  <span class="attr">gateway:</span></span><br><span class="line">    <span class="attr">build:</span> <span class="string">./gateway</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;10010:10010&quot;</span></span><br></pre></td></tr></table></figure>

<p>可以看到，其中包含5个service服务：</p>
<ul>
<li><code>nacos</code>：作为注册中心和配置中心<ul>
<li><code>image: nacos/nacos-server</code>： 基于nacos&#x2F;nacos-server镜像构建</li>
<li><code>environment</code>：环境变量<ul>
<li><code>MODE: standalone</code>：单点模式启动</li>
</ul>
</li>
<li><code>ports</code>：端口映射，这里暴露了8848端口</li>
</ul>
</li>
<li><code>mysql</code>：数据库<ul>
<li><code>image: mysql:5.7.25</code>：镜像版本是mysql:5.7.25</li>
<li><code>environment</code>：环境变量<ul>
<li><code>MYSQL_ROOT_PASSWORD: 123</code>：设置数据库root账户的密码为123</li>
</ul>
</li>
<li><code>volumes</code>：数据卷挂载，这里挂载了mysql的data、conf目录，其中有我提前准备好的数据</li>
</ul>
</li>
<li><code>userservice</code>、<code>orderservice</code>、<code>gateway</code>：都是基于Dockerfile临时构建的</li>
</ul>
<p>查看mysql目录，可以看到其中已经准备好了cloud_order、cloud_user表：</p>
<p><img src="/assets/image-20210801095205034.png" alt="image-20210801095205034"></p>
<p>查看微服务目录，可以看到都包含Dockerfile文件：</p>
<p><img src="/assets/image-20210801095320586.png" alt="image-20210801095320586"></p>
<p>内容如下：</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> java:<span class="number">8</span>-alpine</span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> ./app.jar /tmp/app.jar</span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> java -jar /tmp/app.jar</span></span><br></pre></td></tr></table></figure>





<h3 id="5-3-2-修改微服务配置"><a href="#5-3-2-修改微服务配置" class="headerlink" title="5.3.2.修改微服务配置"></a>5.3.2.修改微服务配置</h3><p>因为微服务将来要部署为docker容器，而容器之间互联不是通过IP地址，而是通过容器名。这里我们将order-service、user-service、gateway服务的mysql、nacos地址都修改为基于容器名的访问。</p>
<p>如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">datasource:</span></span><br><span class="line">    <span class="attr">url:</span> <span class="string">jdbc:mysql://mysql:3306/cloud_order?useSSL=false</span></span><br><span class="line">    <span class="attr">username:</span> <span class="string">root</span></span><br><span class="line">    <span class="attr">password:</span> <span class="number">123</span></span><br><span class="line">    <span class="attr">driver-class-name:</span> <span class="string">com.mysql.jdbc.Driver</span></span><br><span class="line">  <span class="attr">application:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">orderservice</span></span><br><span class="line">  <span class="attr">cloud:</span></span><br><span class="line">    <span class="attr">nacos:</span></span><br><span class="line">      <span class="attr">server-addr:</span> <span class="string">nacos:8848</span> <span class="comment"># nacos服务地址</span></span><br></pre></td></tr></table></figure>



<h3 id="5-3-3-打包"><a href="#5-3-3-打包" class="headerlink" title="5.3.3.打包"></a>5.3.3.打包</h3><p>接下来需要将我们的每个微服务都打包。因为之前查看到Dockerfile中的jar包名称都是app.jar，因此我们的每个微服务都需要用这个名称。</p>
<p>可以通过修改pom.xml中的打包名称来实现，每个微服务都需要修改：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 服务打包的最终名称 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">finalName</span>&gt;</span>app<span class="tag">&lt;/<span class="name">finalName</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>打包后：</p>
<p><img src="/assets/image-20210801095951030.png" alt="image-20210801095951030"></p>
<h3 id="5-3-4-拷贝jar包到部署目录"><a href="#5-3-4-拷贝jar包到部署目录" class="headerlink" title="5.3.4.拷贝jar包到部署目录"></a>5.3.4.拷贝jar包到部署目录</h3><p>编译打包好的app.jar文件，需要放到Dockerfile的同级目录中。注意：每个微服务的app.jar放到与服务名称对应的目录，别搞错了。</p>
<p>user-service：</p>
<p><img src="/assets/image-20210801100201253.png" alt="image-20210801100201253"></p>
<p>order-service：</p>
<p><img src="/assets/image-20210801100231495.png" alt="image-20210801100231495"></p>
<p>gateway：</p>
<p><img src="/assets/image-20210801100308102.png" alt="image-20210801100308102"></p>
<h3 id="5-3-5-部署"><a href="#5-3-5-部署" class="headerlink" title="5.3.5.部署"></a>5.3.5.部署</h3><p>最后，我们需要将文件整个cloud-demo文件夹上传到虚拟机中，理由DockerCompose部署。</p>
<p>上传到任意目录：</p>
<p><img src="/assets/image-20210801100955653.png" alt="image-20210801100955653"></p>
<p>部署：</p>
<p>进入cloud-demo目录，然后运行下面的命令：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure>







<h1 id="6-Docker镜像仓库"><a href="#6-Docker镜像仓库" class="headerlink" title="6.Docker镜像仓库"></a>6.Docker镜像仓库</h1><h2 id="6-1-搭建私有镜像仓库"><a href="#6-1-搭建私有镜像仓库" class="headerlink" title="6.1.搭建私有镜像仓库"></a>6.1.搭建私有镜像仓库</h2><p>参考课前资料《CentOS7安装Docker.md》</p>
<h3 id="Docker镜像仓库"><a href="#Docker镜像仓库" class="headerlink" title="Docker镜像仓库"></a>Docker镜像仓库</h3><p>搭建镜像仓库可以基于Docker官方提供的DockerRegistry来实现。</p>
<p>官网地址：<a target="_blank" rel="noopener" href="https://hub.docker.com/_/registry">https://hub.docker.com/_/registry</a></p>
<h4 id="1-简化版镜像仓库"><a href="#1-简化版镜像仓库" class="headerlink" title="1.简化版镜像仓库"></a>1.简化版镜像仓库</h4><p>Docker官方的Docker Registry是一个基础版本的Docker镜像仓库，具备仓库管理的完整功能，但是没有图形化界面。</p>
<p>搭建方式比较简单，命令如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -d \</span><br><span class="line">    --restart=always \</span><br><span class="line">    --name registry	\</span><br><span class="line">    -p 5000:5000 \</span><br><span class="line">    -v registry-data:/var/lib/registry \</span><br><span class="line">    registry</span><br></pre></td></tr></table></figure>

<p>命令中挂载了一个数据卷registry-data到容器内的&#x2F;var&#x2F;lib&#x2F;registry 目录，这是私有镜像库存放数据的目录。</p>
<p>访问<a target="_blank" rel="noopener" href="http://yourip:5000/v2/_catalog">http://YourIp:5000/v2/_catalog</a> 可以查看当前私有镜像服务中包含的镜像</p>
<h4 id="2-带有图形化界面版本"><a href="#2-带有图形化界面版本" class="headerlink" title="2.带有图形化界面版本"></a>2.带有图形化界面版本</h4><p>使用DockerCompose部署带有图象界面的DockerRegistry，命令如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;3.0&#x27;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">registry:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">registry</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">./registry-data:/var/lib/registry</span></span><br><span class="line">  <span class="attr">ui:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">joxit/docker-registry-ui:static</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">8080</span><span class="string">:80</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">REGISTRY_TITLE=传智教育私有仓库</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">REGISTRY_URL=http://registry:5000</span></span><br><span class="line">    <span class="attr">depends_on:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">registry</span></span><br></pre></td></tr></table></figure>

<h4 id="3-配置Docker信任地址"><a href="#3-配置Docker信任地址" class="headerlink" title="3.配置Docker信任地址"></a>3.配置Docker信任地址</h4><p>我们的私服采用的是http协议，默认不被Docker信任，所以需要做一个配置：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 打开要修改的文件</span></span><br><span class="line">vi /etc/docker/daemon.json</span><br><span class="line"><span class="comment"># 添加内容：</span></span><br><span class="line"><span class="string">&quot;insecure-registries&quot;</span>:[<span class="string">&quot;http://192.168.150.101:8080&quot;</span>]</span><br><span class="line"><span class="comment"># 重加载</span></span><br><span class="line">systemctl daemon-reload</span><br><span class="line"><span class="comment"># 重启docker</span></span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>



<h2 id="6-2-推送、拉取镜像"><a href="#6-2-推送、拉取镜像" class="headerlink" title="6.2.推送、拉取镜像"></a>6.2.推送、拉取镜像</h2><p>推送镜像到私有镜像服务必须先tag，步骤如下：</p>
<p>① 重新tag本地镜像，名称前缀为私有仓库的地址：192.168.150.101:8080&#x2F;</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker tag nginx:latest 192.168.150.101:8080/nginx:1.0 </span><br></pre></td></tr></table></figure>



<p>② 推送镜像</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push 192.168.150.101:8080/nginx:1.0 </span><br></pre></td></tr></table></figure>



<p>③ 拉取镜像</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull 192.168.150.101:8080/nginx:1.0 </span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/19/Docker%E5%AE%9E%E7%94%A8%E7%AF%87/" data-id="clj25kfya0003n0ur1o5i9m9t" data-title="Docker" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Day02：可视化ETL工具Kettle" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/14/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle/" class="article-date">
  <time class="dt-published" datetime="2023-06-14T00:06:25.681Z" itemprop="datePublished">2023-06-14</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/14/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle/">Kettle</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="可视化ETL工具Kettle"><a href="#可视化ETL工具Kettle" class="headerlink" title="可视化ETL工具Kettle"></a>可视化ETL工具Kettle</h1><h2 id="一、数据仓库与ETL"><a href="#一、数据仓库与ETL" class="headerlink" title="一、数据仓库与ETL"></a>一、数据仓库与ETL</h2><h3 id="1、数据仓库"><a href="#1、数据仓库" class="headerlink" title="1、数据仓库"></a>1、数据仓库</h3><ul>
<li>本质：专门针对于数据存储模型</li>
<li>实现：MySQL、Oracle、Hive……</li>
<li>应用：专门用于实现将各种各样数据进行统一化规范化的数据存储，为所有数据应用提供数据<ul>
<li>数据分析</li>
<li>数据挖掘</li>
<li>用户画像</li>
<li>推荐系统</li>
<li>风控系统</li>
</ul>
</li>
<li>特点<ul>
<li>本身不产生数据</li>
<li>本身也不使用数据</li>
<li>用于实现复杂数据的存储</li>
</ul>
</li>
<li>与数据库区别<ul>
<li>数据库：一般用于支撑业务数据的存储<ul>
<li>网站后台：用户数据、商品数据、订单数据</li>
</ul>
</li>
<li>数据仓库：专门为数据数据处理提供数据的<ul>
<li>业务数据</li>
<li>用户行为</li>
<li>爬虫数据</li>
<li>第三方数据</li>
<li>日志数据</li>
</ul>
</li>
</ul>
</li>
<li>问题<ul>
<li>数据种类非常的多，每一种数据的内容或者格式都不一样<ul>
<li>有结构化、有非结构化</li>
<li>有合法的，有非法的</li>
<li>有需要的，有不需要的</li>
</ul>
</li>
<li>MySQL是一个专门用于存储结构化数据的数据存储工具<ul>
<li>·结构化</li>
<li>需要</li>
<li>合法</li>
</ul>
</li>
<li>如何将各种各样的数据存储在MYSQL中？</li>
</ul>
</li>
<li>解决<ul>
<li>数据产生以后，不能直接放入数据仓库【MySQL】中存储</li>
<li>对原始数据进行一步预处理，将需要的、合法的数据放入数据仓库中</li>
<li>这一步预处理：ETL【数据清洗】</li>
</ul>
</li>
</ul>
<h3 id="2、ETL"><a href="#2、ETL" class="headerlink" title="2、ETL"></a>2、ETL</h3><ul>
<li>功能：实现数据的预处理，数据清洗过程，将原始数据经过ETL处理变成想要的数据，进行下一步的应用</li>
<li>实现<ul>
<li>抽取：读取需要处理的原始数据</li>
<li>转换：将原始数据转换为目标数据<ul>
<li>过滤：将不需要的数据过滤掉<ul>
<li>原始数据中有100列</li>
<li>实际需要30列</li>
<li>过滤掉70列</li>
</ul>
</li>
<li>补全：将需要用到的数据补全<ul>
<li>每一个访问网站或者APP时，会有一个IP地址</li>
<li>后台通过IP能获取到我们当前所在的国家、省份、城市</li>
</ul>
</li>
<li>转换：原始数据的格式不是我们想要的格式，转换为想要的格式<ul>
<li>原始数据：22&#x2F;Aug&#x2F;2020:12:20:35</li>
<li>|</li>
<li>|  转换</li>
<li>|</li>
<li>目标格式：2020-08-22  12:20:35</li>
</ul>
</li>
</ul>
</li>
<li>加载：将处理好的目标数据放入数据仓库中</li>
</ul>
</li>
</ul>
<h3 id="3、Kettle"><a href="#3、Kettle" class="headerlink" title="3、Kettle"></a>3、Kettle</h3><ul>
<li>功能：实现可视化ETL<ul>
<li>可视化：不用写复杂的代码程序，可以通过图形化的界面来实现数据的处理</li>
</ul>
</li>
<li>特点<ul>
<li>学习以及使用的成本比较低</li>
<li>功能非常强大</li>
</ul>
</li>
</ul>
<h2 id="二、Windows版本部署"><a href="#二、Windows版本部署" class="headerlink" title="二、Windows版本部署"></a>二、Windows版本部署</h2><h3 id="1、JDK安装配置"><a href="#1、JDK安装配置" class="headerlink" title="1、JDK安装配置"></a>1、JDK安装配置</h3><ul>
<li><p>安装JDK</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722105223805.png" alt="image-20200722105223805"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722105243800.png" alt="image-20200722105243800"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722105303004.png" alt="image-20200722105303004"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722105358146.png" alt="image-20200722105358146"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722105506499.png" alt="image-20200722105506499"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722105609566.png" alt="image-20200722105609566"></p>
</li>
<li><p>配置JDK环境变量</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722105711385.png" alt="image-20200722105711385"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722105732574.png" alt="image-20200722105732574"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722105746214.png" alt="image-20200722105746214"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722105855306.png" alt="image-20200722105855306"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722105933177.png" alt="image-20200722105933177"></p>
<ul>
<li><p>添加bin目录的位置</p>
<ul>
<li>第一种界面</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">;C:\Program Files\Java\jdk1.8.0_241\bin</span><br></pre></td></tr></table></figure>

<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722110139880.png" alt="image-20200722110139880"></p>
<ul>
<li>第二种界面</li>
</ul>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722110249007.png" alt="image-20200722110249007"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Program Files\Java\jdk1.8.0_241\bin</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>验证安装的结果</p>
<ul>
<li><p>在windows命令行执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure></li>
</ul>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722110511213.png" alt="image-20200722110511213"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722110534198.png" alt="image-20200722110534198"></p>
</li>
</ul>
<h3 id="2、Kettle安装启动"><a href="#2、Kettle安装启动" class="headerlink" title="2、Kettle安装启动"></a>2、Kettle安装启动</h3><ul>
<li><p>解压安装</p>
<ul>
<li>解压到一个不包含中文的路径中即可</li>
</ul>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722110833924.png" alt="image-20200722110833924"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722113554732.png" alt="image-20200722113554732"></p>
</li>
<li><p>启动</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722113631022.png" alt="image-20200722113631022"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722113730624.png" alt="image-20200722113730624"></p>
</li>
</ul>
<h2 id="三、Kettle使用"><a href="#三、Kettle使用" class="headerlink" title="三、Kettle使用"></a>三、Kettle使用</h2><h3 id="1、转换"><a href="#1、转换" class="headerlink" title="1、转换"></a>1、转换</h3><ul>
<li>功能：实现一个转换的程序<ul>
<li>输入：要读取什么数据进行转换</li>
<li>转换：要对数据怎么进行处理【过滤、补全、转换】</li>
<li>输出：要将处理好的数据保存到什么地方</li>
</ul>
</li>
</ul>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722114144063.png" alt="image-20200722114144063"></p>
<h3 id="2、作业"><a href="#2、作业" class="headerlink" title="2、作业"></a>2、作业</h3><ul>
<li><p>功能：将多个转换根据需求构建任务流</p>
<ul>
<li>任务流：很多个任务【每一个转换程序】根据自动运行的条件来运行就是任务流</li>
<li>实际工作中，一次要执行很多个转换任务，如何实现这些任务的自动化执行</li>
<li>自动运行<ul>
<li>第一种：定时运行<ul>
<li>每天的00:01分开始自动运行</li>
</ul>
</li>
<li>第二种：依赖关系<ul>
<li>A先运行，A运行成功，B就自动运行</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>举例</p>
<ul>
<li><p>转换1：实现对数据的过滤</p>
</li>
<li><p>转换2：实现对数据的补全</p>
</li>
<li><p>转换3：实现对数据的转换</p>
</li>
<li><p>作业：一个任务流</p>
<ul>
<li>转换1：每天00:10分自动运行</li>
<li>转换2：转换1运行成功，转换2就开始运行</li>
<li>转换3：转换2运行成功，转换3就开始运行</li>
</ul>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722114924436.png" alt="image-20200722114924436"></p>
</li>
</ul>
</li>
</ul>
<h2 id="四、实战案例一"><a href="#四、实战案例一" class="headerlink" title="四、实战案例一"></a>四、实战案例一</h2><h3 id="1、需求"><a href="#1、需求" class="headerlink" title="1、需求"></a>1、需求</h3><ul>
<li>将txt文件中的数据写入Excel表格中</li>
</ul>
<h3 id="2、分析"><a href="#2、分析" class="headerlink" title="2、分析"></a>2、分析</h3><ul>
<li>任务：一个转换程序<ul>
<li>输入：读取txt文件中内容</li>
<li>转换：不需要</li>
<li>输出：将内容加载到一个Excel文件中</li>
</ul>
</li>
</ul>
<h3 id="3、实现"><a href="#3、实现" class="headerlink" title="3、实现"></a>3、实现</h3><ul>
<li><p>step1：构建转换流程图</p>
<ul>
<li><p>新建一个转换任务</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722142250319.png" alt="image-20200722142250319"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722142310820.png" alt="image-20200722142310820"></p>
</li>
<li><p>将输入和输出拖入流程图的面板中</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722142458479.png" alt="image-20200722142458479"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722142542150.png" alt="image-20200722142542150"></p>
</li>
<li><p>连线</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722142722676.png" alt="image-20200722142722676"></p>
</li>
</ul>
</li>
<li><p>step2：配置输入</p>
<ul>
<li>关联文件</li>
</ul>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722143501694.png" alt="image-20200722143501694"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722143632745.png" alt="image-20200722143632745"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722143734940.png" alt="image-20200722143734940"></p>
<ul>
<li><p>配置文件的格式</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722144128741.png" alt="image-20200722144128741"></p>
</li>
<li><p>选择输出到下一步的数据</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722144224932.png" alt="image-20200722144224932"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722144239822.png" alt="image-20200722144239822"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722144446344.png" alt="image-20200722144446344"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722144456628.png" alt="image-20200722144456628"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722144531292.png" alt="image-20200722144531292"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722144543660.png" alt="image-20200722144543660"></p>
</li>
</ul>
</li>
<li><p>step3：配置输出</p>
<ul>
<li><p>输出目标文件</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722144706570.png" alt="image-20200722144706570"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722144814951.png" alt="image-20200722144814951"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722144918502.png" alt="image-20200722144918502"></p>
</li>
<li><p>预览输出信息</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722145005181.png" alt="image-20200722145005181"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722145020311.png" alt="image-20200722145020311"></p>
</li>
</ul>
</li>
<li><p>step4：测试运行</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722145128899.png" alt="image-20200722145128899"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722145136836.png" alt="image-20200722145136836"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722145204498.png" alt="image-20200722145204498"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722145234929.png" alt="image-20200722145234929"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722145300553.png" alt="image-20200722145300553"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722145342524.png" alt="image-20200722145342524"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722145349915.png" alt="image-20200722145349915"></p>
</li>
</ul>
<h2 id="五、实战案例二"><a href="#五、实战案例二" class="headerlink" title="五、实战案例二"></a>五、实战案例二</h2><h3 id="1、需求-1"><a href="#1、需求-1" class="headerlink" title="1、需求"></a>1、需求</h3><ul>
<li>读取Excel文件中的数据，存储MySQL中</li>
</ul>
<h3 id="2、分析-1"><a href="#2、分析-1" class="headerlink" title="2、分析"></a>2、分析</h3><ul>
<li><p>这是一个转换程序</p>
</li>
<li><p>输入：读取这个Excel文件</p>
</li>
<li><p>转换：不需要实现转换</p>
</li>
<li><p>输出：存储到MySQL中的表中</p>
<ul>
<li><p>数据库：kettle_demo</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database kettle_demo;</span><br></pre></td></tr></table></figure>

<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722155531955.png" alt="image-20200722155531955"></p>
</li>
<li><p>表：t_user</p>
<ul>
<li>让Kettle根据上游的数据自行创建</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3、实现-1"><a href="#3、实现-1" class="headerlink" title="3、实现"></a>3、实现</h3><ul>
<li><p>step1：构建转换流程图</p>
<ul>
<li><p>新建保存</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722153632700.png" alt="image-20200722153632700"></p>
</li>
<li><p>定义输入</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722153825528.png" alt="image-20200722153825528"></p>
</li>
<li><p>定义输出</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722153849321.png" alt="image-20200722153849321"></p>
</li>
</ul>
</li>
<li><p>step2：配置输入</p>
<ul>
<li><p>定义输入的数据</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722154052274.png" alt="image-20200722154052274"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722154130148.png" alt="image-20200722154130148"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722154146185.png" alt="image-20200722154146185"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722154157423.png" alt="image-20200722154157423"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722154236283.png" alt="image-20200722154236283"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722154317003.png" alt="image-20200722154317003"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722154343906.png" alt="image-20200722154343906">、</p>
</li>
<li><p>定义输出的字段</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722154439414.png" alt="image-20200722154439414"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722154624225.png" alt="image-20200722154624225"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722154639730.png" alt="image-20200722154639730"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722154716523.png" alt="image-20200722154716523"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722154730317.png" alt="image-20200722154730317"></p>
</li>
</ul>
</li>
<li><p>step3：配置输出</p>
<ul>
<li><p>将连接MySQL的工具放入Kettle的lib目录中</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160018987.png" alt="image-20200722160018987"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160104092.png" alt="image-20200722160104092"></p>
</li>
<li><p>&#x3D;&#x3D;<strong>重新启动Kettle</strong>&#x3D;&#x3D;</p>
</li>
<li><p>构建MySQL连接：地址、用户名、密码、数据库</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722155803639.png" alt="image-20200722155803639"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160250214.png" alt="image-20200722160250214"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160310389.png" alt="image-20200722160310389"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160508104.png" alt="image-20200722160508104"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160529093.png" alt="image-20200722160529093"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160549087.png" alt="image-20200722160549087"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160559340.png" alt="image-20200722160559340"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160642263.png" alt="image-20200722160642263"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160657900.png" alt="image-20200722160657900"></p>
</li>
</ul>
</li>
<li><p>step4：测试运行</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160730805.png" alt="image-20200722160730805"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160739216.png" alt="image-20200722160739216"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160759504.png" alt="image-20200722160759504"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722160844136.png" alt="image-20200722160844136"></p>
</li>
</ul>
<h2 id="六、常用组件"><a href="#六、常用组件" class="headerlink" title="六、常用组件"></a>六、常用组件</h2><h3 id="1、共享数据库连接"><a href="#1、共享数据库连接" class="headerlink" title="1、共享数据库连接"></a>1、共享数据库连接</h3><ul>
<li>新建的数据库连接都只属于某一个转换程序</li>
<li>如果你想让所有的转换程序都能使用这个连接，需要开启共享</li>
</ul>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722165840910.png" alt="image-20200722165840910"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722165859797.png" alt="image-20200722165859797"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722165919365.png" alt="image-20200722165919365"></p>
<h3 id="2、表输入组件"><a href="#2、表输入组件" class="headerlink" title="2、表输入组件"></a>2、表输入组件</h3><ul>
<li><p>需求：将t_user中的数据，同步到t_user1这张表中</p>
</li>
<li><p>分析</p>
<ul>
<li>这是一个转换任务</li>
<li>输入：读取t_user表的数据</li>
<li>转换：没有转换过程</li>
<li>输出：将结果写入t_user1表中</li>
</ul>
</li>
<li><p>实现</p>
<ul>
<li><p>开发程序</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722164702600.png" alt="image-20200722164702600"></p>
</li>
<li><p>配置输入</p>
<ul>
<li>先配置数据库连接共享</li>
</ul>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722165958428.png" alt="image-20200722165958428"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722170047492.png" alt="image-20200722170047492"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722170105618.png" alt="image-20200722170105618"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722170215354.png" alt="image-20200722170215354"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722170330357.png" alt="image-20200722170330357"></p>
</li>
<li><p>配置输出</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722170419413.png" alt="image-20200722170419413"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722170438223.png" alt="image-20200722170438223"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722170452804.png" alt="image-20200722170452804"></p>
</li>
<li><p>测试运行</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722170536458.png" alt="image-20200722170536458"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722170549399.png" alt="image-20200722170549399"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722170622208.png" alt="image-20200722170622208"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722170632510.png" alt="image-20200722170632510"></p>
</li>
</ul>
</li>
</ul>
<h3 id="3、插入更新组件"><a href="#3、插入更新组件" class="headerlink" title="3、插入更新组件"></a>3、插入更新组件</h3><ul>
<li><p>工作需求：将 A表的数据同步到B表中，保证B表的数据与A表的数据一致，实现是不断更新的操作</p>
<ul>
<li>A表发生了更新，更新的数据也会同步到B表中</li>
<li>A表没有发生更新，即使程序运行，B表也不发生改变</li>
<li>数据同步的过程<ul>
<li>每次只同步更新的数据</li>
<li>已经同步过的数据，就不会再进行同步</li>
</ul>
</li>
<li>工作中一般一天会同步一次，程序就每天执行一次</li>
</ul>
</li>
<li><p>解决：插入更新的输出组件</p>
</li>
<li><p>功能：只会同步发生更新的数据，已经同步过的数据不会再次同步</p>
<ul>
<li>数据更新<ul>
<li>插入一条新的数据</li>
<li>更改一条老的数据</li>
</ul>
</li>
</ul>
</li>
<li><p>实现：将t_user表的数据同步到t_user2中【任何的时刻，这两张表 数据同步时是一致的】</p>
<ul>
<li><p>开发转化 任务流程图</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722172727176.png" alt="image-20200722172727176"></p>
</li>
<li><p>定义输入</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722172804528.png" alt="image-20200722172804528"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722172825165.png" alt="image-20200722172825165"></p>
</li>
<li><p>定义输出</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722173646566.png" alt="image-20200722173646566"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722173715767.png" alt="image-20200722173715767"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722173750781.png" alt="image-20200722173750781"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722173859869.png" alt="image-20200722173859869"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722173918025.png" alt="image-20200722173918025"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722173939760.png" alt="image-20200722173939760"></p>
</li>
<li><p>测试运行</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722173956739.png" alt="image-20200722173956739"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722174010199.png" alt="image-20200722174010199"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722174019228.png" alt="image-20200722174019228"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722174035354.png" alt="image-20200722174035354"></p>
</li>
<li><p>如果修改了t_user的数据，重新执行程序，观察t_user2中的数据是否与t_user是一致的</p>
<ul>
<li><p>修改t_user的数据</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722174237259.png" alt="image-20200722174237259"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722174257213.png" alt="image-20200722174257213"></p>
</li>
<li><p>重新运行程序</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722174325258.png" alt="image-20200722174325258"></p>
</li>
<li><p>观察t_user2的数据</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722174412978.png" alt="image-20200722174412978"></p>
</li>
</ul>
</li>
<li><p>同步业务</p>
<ul>
<li>全量：每次将所有的数据都同步一份<ul>
<li>保证A和B是一致的<ul>
<li>每次先删除B所有内容，然后，再同步</li>
</ul>
</li>
<li>程序的性能比较差，数据量大了以后，非常慢，不建议使用</li>
<li>表输出：全量的组件</li>
</ul>
</li>
<li>增量：每次将发生更新的数据同步，没有发生更新就是已经同步过的数据不再同步<ul>
<li>保证A和B是一致的</li>
<li>工作中都使用增量的方式</li>
<li>插入更新：增量的组件</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="七、Kettle-Job"><a href="#七、Kettle-Job" class="headerlink" title="七、Kettle Job"></a>七、Kettle Job</h2><h3 id="1、Job的功能"><a href="#1、Job的功能" class="headerlink" title="1、Job的功能"></a>1、Job的功能</h3><ul>
<li>转换：实现一种数据的转换处理，是一个转换任务</li>
<li>作业：实现多个转换任务按照一定的规则运行，就是一个任务流<ul>
<li>时间规则：从00:10分开始，每5种运行一次</li>
<li>依赖规则：A成功了，就执行B</li>
</ul>
</li>
<li>功能：将多个转换根据彼此之间的 关系实现任务流运行</li>
</ul>
<h3 id="2、Job的开发"><a href="#2、Job的开发" class="headerlink" title="2、Job的开发"></a>2、Job的开发</h3><ul>
<li><p>需求：每5s就运行一个Kettle的转换任务</p>
</li>
<li><p>实现</p>
<ul>
<li><p>构建一个作业</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722184809959.png" alt="image-20200722184809959"></p>
</li>
<li><p>配置转换任务</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722185018655.png" alt="image-20200722185018655"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722184844483.png" alt="image-20200722184844483"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722184904596.png" alt="image-20200722184904596"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722184929577.png" alt="image-20200722184929577"></p>
</li>
<li><p>配置作业运行的规则</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722185004590.png" alt="image-20200722185004590"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722185211573.png" alt="image-20200722185211573"></p>
</li>
<li><p>运行</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722185247683.png" alt="image-20200722185247683"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722185310911.png" alt="image-20200722185310911"></p>
</li>
</ul>
</li>
</ul>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722185421352.png" alt="image-20200722185421352"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722185439558.png" alt="image-20200722185439558"></p>
<ul>
<li><p>扩展：多个转换任务，按照时间以及顺序运行</p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722185634596.png" alt="image-20200722185634596"></p>
<p><img src="/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle.assets/image-20200722185754113.png" alt="image-20200722185754113"></p>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/14/Day02%EF%BC%9A%E5%8F%AF%E8%A7%86%E5%8C%96ETL%E5%B7%A5%E5%85%B7Kettle/" data-id="clj25kfy10000n0urcd8egje0" data-title="Kettle" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Day01：数据库管理系统MySQL" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/13/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL/" class="article-date">
  <time class="dt-published" datetime="2023-06-13T08:06:42.989Z" itemprop="datePublished">2023-06-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/06/13/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL/">MySQL</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="数据库管理系统MySQL"><a href="#数据库管理系统MySQL" class="headerlink" title="数据库管理系统MySQL"></a>数据库管理系统MySQL</h1><h2 id="一、MySQL的介绍"><a href="#一、MySQL的介绍" class="headerlink" title="一、MySQL的介绍"></a>一、MySQL的介绍</h2><h3 id="1、大数据本质"><a href="#1、大数据本质" class="headerlink" title="1、大数据本质"></a>1、大数据本质</h3><ul>
<li>利用大数据的软件工具对大数据进行处理，从数据中挖掘价值</li>
</ul>
<h3 id="2、数据处理流程"><a href="#2、数据处理流程" class="headerlink" title="2、数据处理流程"></a>2、数据处理流程</h3><ul>
<li>数据采集：将产生各种数据进行统一化的存储</li>
<li>数据存储：将数据存储数据仓库中</li>
<li>数据处理：使用SQL开发语言开发程序对数据进行处理</li>
<li>数据应用：将处理好的结果进行 应用</li>
</ul>
<h3 id="3、数据存储及处理"><a href="#3、数据存储及处理" class="headerlink" title="3、数据存储及处理"></a>3、数据存储及处理</h3><ul>
<li>存储的形式：文件<ul>
<li>不能满足企业中对于数据处理需求</li>
</ul>
</li>
<li>工作需求：更加规范的数据存储、处理<ul>
<li>早期：Excel【表格，聚合统计分析，图表】</li>
<li>问题<ul>
<li>Excel能承载的数据量大小：MB<ul>
<li>实际工作中要处理的数据大小：GB</li>
</ul>
</li>
<li>Excel中提供的功能不能满足对数据处理的需求<ul>
<li>支持开发不同的功能</li>
<li>开发的方式不太友好</li>
</ul>
</li>
</ul>
</li>
<li>解决<ul>
<li>数据库管理系统</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="4、数据库管理系统"><a href="#4、数据库管理系统" class="headerlink" title="4、数据库管理系统"></a>4、数据库管理系统</h3><ul>
<li>功能<ul>
<li>专门用户数据存储、处理数据的工具</li>
</ul>
</li>
<li>特点<ul>
<li>承载数据量会更大</li>
<li>更加规范化</li>
<li>功能更加全面</li>
<li>开发接口更加优化：SQL</li>
</ul>
</li>
<li>应用场景<ul>
<li>网站后台中存储商品信息、订单信息、用户注册 信息</li>
</ul>
</li>
</ul>
<h3 id="5、MySQL介绍及概念"><a href="#5、MySQL介绍及概念" class="headerlink" title="5、MySQL介绍及概念"></a>5、MySQL介绍及概念</h3><ul>
<li><p>常见的数据库管理系统</p>
<ul>
<li>Oracle：Sun公司商业化数据库产品，性能功能是最强大，但是是收费的商业化产品</li>
<li>SQL Server：微软公司的产品，受Windows局限性比较大 ，市场占有率并不高，收费</li>
<li>MySQL：Sun公司的社区产品，体积小，速度快，总体使用的成本比较低</li>
</ul>
</li>
<li><p>MySQL的介绍</p>
<ul>
<li>典型的市场占有率是最高的数据库管理系统</li>
<li>在国内非常广泛<ul>
<li>所有网站后台的存储</li>
</ul>
</li>
</ul>
</li>
<li><p>概念</p>
<ul>
<li><p>数据库管理系统</p>
<ul>
<li>专门用户存储和处理数据【结构化数据】的工具</li>
<li>MySQL就是一个数据库管理系统</li>
</ul>
</li>
<li><p>结构化数据</p>
<ul>
<li>例如：表格，行和列是固定的</li>
<li>行和列是固定的结构，就是数据的格式存在一定的规律</li>
</ul>
</li>
<li><p>数据库：MySQL中用于管理和区分数据表的单元</p>
<ul>
<li>database</li>
<li>理解为对数据进行分类存放的划分</li>
<li>数据库1：存放用户的数据</li>
<li>数据库2：存放商品的数据</li>
<li>数据库3：存放订单的数据</li>
<li>类似于一个Excel文件<ul>
<li>人事：人事的Excel文件</li>
<li>财务：财务的Excel文件</li>
</ul>
</li>
</ul>
</li>
<li><p>表格：MySQL中用于在数据库中划分数据的单元</p>
<ul>
<li>将数据进行更细的划分</li>
<li>类似于一个Excel文件中会有多张表<ul>
<li>人事Excel文件 <ul>
<li>在职人员信息表</li>
<li>离职人员信息表</li>
</ul>
</li>
<li>财务Excel文件<ul>
<li>报销信息表</li>
<li>收入信息表</li>
<li>报税信息表</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>数据库管理系统与Excel对比</p>
<table>
<thead>
<tr>
<th align="center">Excel</th>
<th align="center">MySQL</th>
</tr>
</thead>
<tbody><tr>
<td align="center">一个Excel文件</td>
<td align="center">一个数据库</td>
</tr>
<tr>
<td align="center">可以有多个Excel的sheet表格</td>
<td align="center">可以有多张数据表</td>
</tr>
<tr>
<td align="center">表格有行和列</td>
<td align="center">表格中有行和列</td>
</tr>
</tbody></table>
<ul>
<li>区别：<ul>
<li>MySQL功能更加强大</li>
<li>Excel的开发比较复杂</li>
<li>MySQL对数据进行处理：SQL</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>MySQL的使用</p>
<ul>
<li>SQL：开发语言，实现数据的存储以及分析管理</li>
</ul>
</li>
</ul>
<h2 id="二、MySQL及DataGrip部署"><a href="#二、MySQL及DataGrip部署" class="headerlink" title="二、MySQL及DataGrip部署"></a>二、MySQL及DataGrip部署</h2><h3 id="1、MySQL安装"><a href="#1、MySQL安装" class="headerlink" title="1、MySQL安装"></a>1、MySQL安装</h3><ul>
<li>参考MySQL安装文档实现安装</li>
</ul>
<h3 id="2、DataGrip的安装"><a href="#2、DataGrip的安装" class="headerlink" title="2、DataGrip的安装"></a>2、DataGrip的安装</h3><ul>
<li>功能：使用图形化界面的方式来操作MySQL，进行数据的管理</li>
<li>参考DataGrip安装文档实现安装</li>
</ul>
<h3 id="3、DataGrip连接MySQL"><a href="#3、DataGrip连接MySQL" class="headerlink" title="3、DataGrip连接MySQL"></a>3、DataGrip连接MySQL</h3><ul>
<li><p>创建一个连接，配置连接MySQL即可</p>
<ul>
<li><p>MySQL所在机器的地址和端口</p>
<ul>
<li>地址：localhost</li>
<li>端口：3306</li>
</ul>
</li>
<li><p>MySQL的连接驱动</p>
<ul>
<li>下载</li>
</ul>
</li>
<li><p>MySQL用户名和密码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">用户名：root</span><br><span class="line">密码：123456</span><br></pre></td></tr></table></figure>
</li>
<li><p>MySQL连接地址属性</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jdbc:mysql://localhost:3306?serverTimezone=UTC</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>参考视频或者课件中的图片实现连接</p>
</li>
</ul>
<h2 id="三、SQL介绍及其规则"><a href="#三、SQL介绍及其规则" class="headerlink" title="三、SQL介绍及其规则"></a>三、SQL介绍及其规则</h2><h3 id="1、SQL的介绍"><a href="#1、SQL的介绍" class="headerlink" title="1、SQL的介绍"></a>1、SQL的介绍</h3><ul>
<li><p>Struct Qurey Language：结构化查询语言</p>
</li>
<li><p>一种编程语言，是一种命令，通过这种命令或者编程语言开发程序来实现数据处理</p>
<ul>
<li>MySQL使用SQL命令来管理MySQL中数据</li>
</ul>
</li>
<li><p>SQL是所有RDBMS【关系型数据库管理系统】通用语言</p>
<ul>
<li>在语法上有一点点区别</li>
</ul>
</li>
</ul>
<h3 id="2、SQL的分类"><a href="#2、SQL的分类" class="headerlink" title="2、SQL的分类"></a>2、SQL的分类</h3><ul>
<li><p>MySQL中的SQL根据不同的功能模块划分不同的命令的分类</p>
</li>
<li><p>DDL：数据定义语言</p>
<ul>
<li>如何管理我们的数据库和表</li>
<li>数据库的管理：创建、删除、切换<ul>
<li>学生信息数据库</li>
</ul>
</li>
<li>表的管理：创建、删除、清空、描述<ul>
<li>学生表</li>
<li>成绩表</li>
<li>学籍表</li>
</ul>
</li>
</ul>
</li>
<li><p>DML：数据操作语言</p>
<ul>
<li>如何管理表中的数据</li>
<li>对表中数据实现以下功能<ul>
<li>插入：insert</li>
<li>更改：update</li>
<li>删除：delete</li>
</ul>
</li>
<li>例如<ul>
<li>录入学生信息</li>
<li>更改学生信息</li>
<li>删除学生信息</li>
</ul>
</li>
</ul>
</li>
<li><p>DQL：数据查询语言</p>
<ul>
<li><p>实现对表中数据的查询和统计分析</p>
</li>
<li><p>我们在工作中60%的开发都是开发SQL，有90%都是在开发DQL</p>
</li>
<li><p>select</p>
</li>
</ul>
</li>
</ul>
<h3 id="3、SQL的规则"><a href="#3、SQL的规则" class="headerlink" title="3、SQL的规则"></a>3、SQL的规则</h3><ul>
<li><p>所有的SQL语句都需要以分号来作为结束符，表示这条命令结束了，可以提交运行</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> databases;</span><br><span class="line"><span class="keyword">show</span> tables;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> mysql.user;</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="四、SQL分析之DDL"><a href="#四、SQL分析之DDL" class="headerlink" title="四、SQL分析之DDL"></a>四、SQL分析之DDL</h2><h3 id="1、数据库管理"><a href="#1、数据库管理" class="headerlink" title="1、数据库管理"></a>1、数据库管理</h3><ul>
<li><p>创建</p>
<ul>
<li><p>功能：构建一个新的数据库</p>
</li>
<li><p>语法</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database [ if <span class="keyword">not</span> <span class="keyword">exists</span> ] 数据库的名字;</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<ul>
<li><p>创建一个新的数据库叫做：itcast01</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database itcast01;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200626202110922.png" alt="image-20200626202110922"></p>
</li>
<li><p>创建一个新的数据库：itcast02</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> itcast02;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200626202312950.png" alt="image-20200626202312950"></p>
</li>
<li><p>if not exists：如果不存在的情况下，就创建，如果已经存在就不会创建</p>
<ul>
<li><p>功能：为了避免程序报错</p>
</li>
<li><p>如果不加：数据库已存在，就会报错</p>
</li>
<li><p>如果加了：数据库已存在，不会报错</p>
<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200626202616343.png" alt="image-20200626202616343"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>列举</p>
<ul>
<li><p>功能：用于列举当前MySQL中所有的数据库名称</p>
</li>
<li><p>语法</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span>  databases;</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200626201927898.png" alt="image-20200626201927898"></p>
</li>
</ul>
</li>
<li><p>查看</p>
<ul>
<li><p>功能：查看当前所在的数据库</p>
</li>
<li><p>语法</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> database();</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200626202921195.png" alt="image-20200626202921195"></p>
<ul>
<li>null表示我们当前不在任何一个数据库中</li>
</ul>
</li>
</ul>
</li>
<li><p>切换</p>
<ul>
<li><p>功能：切换到某个数据库中</p>
</li>
<li><p>语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">use 数据库名称;</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<ul>
<li><p>切换到itcast01这个数据库中</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">use itcast01;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200626203115436.png" alt="image-20200626203115436"></p>
</li>
<li><p>切换到itcast02这个数据库中</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">use itcast02;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200626203205537.png" alt="image-20200626203205537"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>删除</p>
<ul>
<li><p>功能：删除已存在的一个数据库</p>
</li>
<li><p>语法</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> database [ if <span class="keyword">exists</span> ] 数据库名称;</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<ul>
<li><p>删除itcast01这个数据库</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span>  database itcast01;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200626203451258.png" alt="image-20200626203451258"></p>
</li>
<li><p>if exists ：如果存在，就删除，如果不存在就不删除</p>
<ul>
<li><p>功能：为了避免程序报错</p>
</li>
<li><p>如果不加：数据库不存在，删除就会报错</p>
</li>
<li><p>如果加了：数据库不存在，删除不会报错</p>
<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200626203741535.png" alt="image-20200626203741535"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2、数据表管理"><a href="#2、数据表管理" class="headerlink" title="2、数据表管理"></a>2、数据表管理</h3><ul>
<li><p>数据类型</p>
<ul>
<li><p>定义：用于描述表中列的一个数据格式</p>
</li>
<li><p>类型：</p>
<ul>
<li><p>字符类型：中文、英文或者比较长的数字、日期都可以使用字符串来存储</p>
<ul>
<li><p>字符类型是万能的类型</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x27;a&#x27;：这就是一个字符，一个数字、一个英文字母、一个符号</span><br><span class="line">&#x27;abc,12344&#x27;：这就是一个字符串，很多个字符构成一个整体</span><br></pre></td></tr></table></figure>

<ul>
<li>字符串表示的数字是不能参与计算的</li>
</ul>
</li>
<li><p>&#x3D;&#x3D;只要是字符类型，就使用varchar（N）&#x3D;&#x3D;</p>
<ul>
<li>N表示字符串的长度，只能大不能小</li>
<li>手机号码：varchar（11）<ul>
<li>varchar（20）：可以</li>
<li>varchar(10)：不可以</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>数字类型</p>
<ul>
<li>整数<ul>
<li>整形：int</li>
<li>&#x3D;&#x3D;只要是整数就用：int&#x3D;&#x3D;</li>
</ul>
</li>
<li>小数<ul>
<li>单精度：float</li>
<li>双精度：double</li>
<li>&#x3D;&#x3D;只要是小数就用：double&#x3D;&#x3D;</li>
</ul>
</li>
</ul>
</li>
<li><p>日期类型</p>
<ul>
<li>日期是一种特殊的格式</li>
<li>&#x3D;&#x3D;date：用于存储年月日&#x3D;&#x3D;<ul>
<li>yyyy-MM-dd</li>
<li>2020-01-01</li>
</ul>
</li>
<li>&#x3D;&#x3D;datetime：用于存储年月日，时分秒&#x3D;&#x3D;<ul>
<li>yyyy-MM-dd HH:mm:ss</li>
<li>2020-01-01 12:30:50</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>创建</p>
<ul>
<li><p>功能：在某个数据库中创建一张，定义表的结构【表中哪些列以及每一列的类型】</p>
</li>
<li><p>语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">create table [if not exists] [数据库名称.]表的名称(</span><br><span class="line">   col1 type1,</span><br><span class="line">   col2 type2,</span><br><span class="line">   col3 type3,</span><br><span class="line">   ……</span><br><span class="line">   colN typeN</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<ul>
<li>注意事项<ul>
<li>所有的符号都是英文的，不允许出现中文符号</li>
<li>除了最后一行就是结尾括号的前一号不用加逗号，其他的都要加逗号</li>
<li>每一列都要指定对应的类型<ul>
<li>字符串：varchar(N)</li>
<li>整数：int</li>
<li>小数：double</li>
<li>年月日日期：date</li>
<li>年月日时分秒：datetime</li>
</ul>
</li>
<li>如果不加数据库名称，表示在当前数据库中创建表</li>
</ul>
</li>
</ul>
</li>
<li><p>测试</p>
<ul>
<li><p>创建一张学生表student：学生学号、学生姓名、学生年龄、学生性别</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists student(</span><br><span class="line">   stuid varchar(10),</span><br><span class="line">   stuname varchar(10),</span><br><span class="line">   age int,</span><br><span class="line">   sex varchar(2)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200626211200711.png" alt="image-20200626211200711"></p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>列举</p>
<ul>
<li><p>功能：列举当前数据库中所有的表</p>
</li>
<li><p>语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show tables;</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200626210834791.png" alt="image-20200626210834791"></p>
<p>​	<img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200629210140287.png" alt="image-20200629210140287"></p>
</li>
</ul>
</li>
<li><p>描述</p>
<ul>
<li><p>功能：查看一张表的详细的结构信息</p>
</li>
<li><p>语法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc  [dbname.]tbname;</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">desc student;</span><br><span class="line">desc bigdata.student;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200629210418658.png" alt="image-20200629210418658"></p>
</li>
</ul>
</li>
<li><p>删除</p>
<ul>
<li><p>功能：删除一张不需要再使用的表</p>
</li>
<li><p>语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop table [ if exists ] [dbname.]tbname;</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop table if exists student;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200629211104002.png" alt="image-20200629211104002"></p>
</li>
</ul>
</li>
</ul>
<h2 id="五、SQL分析之DML"><a href="#五、SQL分析之DML" class="headerlink" title="五、SQL分析之DML"></a>五、SQL分析之DML</h2><h3 id="1、创建表格"><a href="#1、创建表格" class="headerlink" title="1、创建表格"></a>1、创建表格</h3><ul>
<li><p>创建一个商品的分类表：category</p>
<ul>
<li>分类编号：cid</li>
<li>分类名称：cname</li>
</ul>
</li>
<li><p>创建语句</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create table category(</span><br><span class="line">  cid varchar(5),</span><br><span class="line">  cname varchar(10)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200629212641728.png" alt="image-20200629212641728"></p>
</li>
</ul>
<h3 id="2、插入数据"><a href="#2、插入数据" class="headerlink" title="2、插入数据"></a>2、插入数据</h3><ul>
<li><p>功能：写入一条数据进入数据表</p>
</li>
<li><p>关键字：insert</p>
</li>
<li><p>语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into tbname(co11,col2,col3……)  values(value1,value2,value3……);</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">insert into category(cid,cname) values(&#x27;c001&#x27;,&#x27;电器&#x27;);</span><br><span class="line">insert into category(cname) values(&#x27;服饰&#x27;);</span><br><span class="line">insert into category(cid,cname) values(null,&#x27;化妆品&#x27;);</span><br><span class="line">insert into category values(&#x27;c002&#x27;,&#x27;书籍&#x27;);</span><br><span class="line">insert into category values(null,&#x27;蔬菜&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200629213403110.png" alt="image-20200629213403110"></p>
<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200629214132789.png" alt="image-20200629214132789"></p>
<ul>
<li><p>查询某张表的所有内容</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from category;</span><br></pre></td></tr></table></figure>
</li>
<li><p>&#x3D;&#x3D;注意事项&#x3D;&#x3D;</p>
<ul>
<li>所给定的列的名称必须与后面的值一一对应</li>
<li>给定值的时候，除了数值类型或者null，其他类型必须加上单引号</li>
<li>给定的值不能超过创建表时定义的长度</li>
<li>如果要给表中的每一列都赋值，就可以不写列名</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3、更新数据"><a href="#3、更新数据" class="headerlink" title="3、更新数据"></a>3、更新数据</h3><ul>
<li><p>功能：修改数据表中的数据</p>
</li>
<li><p>关键字：update</p>
</li>
<li><p>语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update 表的名称 set col1 = newValue,col2 = newValue …… [where 条件];</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试</p>
<ul>
<li><p>需求1：将服饰的分类id更改为c003</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update  category set  cid = &#x27;c003&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200629215253675.png" alt="image-20200629215253675"></p>
<ul>
<li>上面不能满足我们的需求，因为更改了其他不需要更改的数据</li>
<li>解决：加上where 条件<ul>
<li>只有满足where条件的数据才会被更改</li>
<li>没有指定where条件，就更改所有数据</li>
</ul>
</li>
</ul>
</li>
<li><p>需求2：将服饰的分类id更改为c004</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update category set cid = &#x27;c004&#x27; where cname = &#x27;服饰&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200629215715473.png" alt="image-20200629215715473"></p>
</li>
<li><p>需求3：将化妆品的分类id更改为c001，并且将分类名称更改为化妆</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update category set cid=&#x27;c001&#x27;,cname=&#x27;化妆&#x27; where cname = &#x27;化妆品&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200629215920737.png" alt="image-20200629215920737"></p>
</li>
<li><p>需求4：将所有 c003的分类名称更改为笔记本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">update category set cname = &#x27;笔记本&#x27; where cid = &#x27;c003&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200629220038167.png" alt="image-20200629220038167"></p>
</li>
<li><p>&#x3D;&#x3D;注意：&#x3D;&#x3D;</p>
<ul>
<li>更改的列的新的值必须与列的类型相符</li>
<li>新的值不能超过这一列的长度</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="4、删除数据"><a href="#4、删除数据" class="headerlink" title="4、删除数据"></a>4、删除数据</h3><ul>
<li><p>功能：删除数据表中的数据</p>
</li>
<li><p>关键字：delete</p>
</li>
<li><p>语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delete from 表的名称  [where 条件];</span><br></pre></td></tr></table></figure>

<ul>
<li><p>如果不加where条件，会删除整张表所有的数据</p>
</li>
<li><p>where 条件：符合条件的数据将会被删除</p>
</li>
<li><p>需求1：删除所有分类名称为笔记本的分类数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delete from category where cname = &#x27;笔记本&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200629221111664.png" alt="image-20200629221111664"></p>
</li>
<li><p>需求2：删除分类id不为c001的分类的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delete from category where cid != &#x27;c001&#x27;;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>清空表中所有的数据</p>
<ul>
<li><p>delete：用于删除表中的数据，一行一行删除</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">delete from category;</span><br></pre></td></tr></table></figure>
</li>
<li><p>truncate：用于清空整张表的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">truncate category;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200629221411664.png" alt="image-20200629221411664"></p>
</li>
<li><p>区别</p>
<ul>
<li>delete：DML命令，一条一条删除</li>
<li>truncate：DDL命令，类似于将整张表删除，然后重新创建一张一样的空表</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="六、SQL分析之DQL"><a href="#六、SQL分析之DQL" class="headerlink" title="六、SQL分析之DQL"></a>六、SQL分析之DQL</h2><h3 id="1、准备数据"><a href="#1、准备数据" class="headerlink" title="1、准备数据"></a>1、准备数据</h3><ul>
<li><p>创建测试数据库</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> database if <span class="keyword">exists</span> bigdata;</span><br><span class="line"><span class="keyword">create</span> database bigdata;</span><br><span class="line">use bigdata;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建商品表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> product(</span><br><span class="line"> pid <span class="type">int</span>,</span><br><span class="line"> pname <span class="type">varchar</span>(<span class="number">20</span>),</span><br><span class="line"> price <span class="keyword">double</span>,</span><br><span class="line"> category_id <span class="type">varchar</span>(<span class="number">32</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
</li>
<li><p>插入商品测试数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product(pid,pname,price,category_id) <span class="keyword">VALUES</span>(<span class="number">1</span>,<span class="string">&#x27;联想&#x27;</span>,<span class="number">5000</span>,<span class="string">&#x27;c001&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product(pid,pname,price,category_id) <span class="keyword">VALUES</span>(<span class="number">2</span>,<span class="string">&#x27;海尔&#x27;</span>,<span class="number">3000</span>,<span class="string">&#x27;c001&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product(pid,pname,price,category_id) <span class="keyword">VALUES</span>(<span class="number">3</span>,<span class="string">&#x27;雷神&#x27;</span>,<span class="number">5000</span>,<span class="string">&#x27;c001&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product(pid,pname,price,category_id) <span class="keyword">VALUES</span>(<span class="number">4</span>,<span class="string">&#x27;杰克琼斯&#x27;</span>,<span class="number">800</span>,<span class="string">&#x27;c002&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product(pid,pname,price,category_id) <span class="keyword">VALUES</span>(<span class="number">5</span>,<span class="string">&#x27;真维斯&#x27;</span>,<span class="number">200</span>,<span class="string">&#x27;c002&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product(pid,pname,price,category_id) <span class="keyword">VALUES</span>(<span class="number">6</span>,<span class="string">&#x27;花花公子&#x27;</span>,<span class="number">440</span>,<span class="string">&#x27;c002&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product(pid,pname,price,category_id) <span class="keyword">VALUES</span>(<span class="number">7</span>,<span class="string">&#x27;劲霸&#x27;</span>,<span class="number">2000</span>,<span class="string">&#x27;c002&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product(pid,pname,price,category_id) <span class="keyword">VALUES</span>(<span class="number">8</span>,<span class="string">&#x27;香奈儿&#x27;</span>,<span class="number">800</span>,<span class="string">&#x27;c003&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product(pid,pname,price,category_id) <span class="keyword">VALUES</span>(<span class="number">9</span>,<span class="string">&#x27;相宜本草&#x27;</span>,<span class="number">200</span>,<span class="string">&#x27;c003&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product(pid,pname,price,category_id) <span class="keyword">VALUES</span>(<span class="number">10</span>,<span class="string">&#x27;面霸&#x27;</span>,<span class="number">5</span>,<span class="string">&#x27;c003&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product(pid,pname,price,category_id) <span class="keyword">VALUES</span>(<span class="number">11</span>,<span class="string">&#x27;好想你枣&#x27;</span>,<span class="number">56</span>,<span class="string">&#x27;c004&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product(pid,pname,price,category_id) <span class="keyword">VALUES</span>(<span class="number">12</span>,<span class="string">&#x27;香飘飘奶茶&#x27;</span>,<span class="number">1</span>,<span class="string">&#x27;c005&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> product(pid,pname,price,category_id) <span class="keyword">VALUES</span>(<span class="number">13</span>,<span class="string">&#x27;海澜之家&#x27;</span>,<span class="number">1</span>,<span class="string">&#x27;c002&#x27;</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建商品分类类：categroy</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> category(</span><br><span class="line">  category_id <span class="type">varchar</span>(<span class="number">10</span>),</span><br><span class="line">  category_name <span class="type">varchar</span>(<span class="number">100</span>)</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
</li>
<li><p>插入商品分类测试数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> category <span class="keyword">values</span>(<span class="string">&#x27;c001&#x27;</span>,<span class="string">&#x27;电脑&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> category <span class="keyword">values</span>(<span class="string">&#x27;c002&#x27;</span>,<span class="string">&#x27;服装&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> category <span class="keyword">values</span>(<span class="string">&#x27;c003&#x27;</span>,<span class="string">&#x27;化妆品&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> category <span class="keyword">values</span>(<span class="string">&#x27;c004&#x27;</span>,<span class="string">&#x27;吃的&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> category <span class="keyword">values</span>(<span class="string">&#x27;c005&#x27;</span>,<span class="string">&#x27;喝的&#x27;</span>);</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="2、基本语法"><a href="#2、基本语法" class="headerlink" title="2、基本语法"></a>2、基本语法</h3><ul>
<li><p>功能：实现对于数据表中的数据的查询、统计分析、处理</p>
</li>
<li><p>关键字：select</p>
</li>
<li><p>语法</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="number">1</span> <span class="keyword">from</span> <span class="number">2</span> <span class="keyword">where</span> <span class="number">3</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="number">4</span> <span class="keyword">having</span> <span class="number">5</span> <span class="keyword">order</span> <span class="keyword">by</span> <span class="number">6</span> limit <span class="number">7</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>1：用于决定查询的结果中有哪些列，给定哪些列，结果就会显示这些列</p>
<ul>
<li>写列的名字，多列用逗号隔开</li>
<li>*号代表所有的列</li>
</ul>
</li>
<li><p>2：用于表示查询哪张表，给定表的名字</p>
</li>
<li><p>3：条件查询，只有满足条件的数据才会被返回</p>
<ul>
<li>不满足条件的数据会被过滤掉，不会在结果中显示</li>
<li>符合where条件的行才会在结果中显示</li>
</ul>
</li>
<li><p>4：用于实现分组的，将多条数据按照某一列或者多列进行分组，划分到同一组中</p>
<ul>
<li>用于实现统计分析</li>
<li>语法：group by col</li>
</ul>
</li>
<li><p>5：用于实现分组后的条件过滤</p>
<ul>
<li>功能类似于where</li>
<li>满足having后的条件就会出现在结果中</li>
<li>不满足条件就会被过滤掉</li>
<li>与where的区别<ul>
<li>where：分组之前过滤</li>
<li>having：分组之后过滤</li>
</ul>
</li>
</ul>
</li>
<li><p>6：用于实现将查询的结果按照某一列或者多列进行排序</p>
<ul>
<li>order by col  [ asc | desc]</li>
<li>asc：升序排序</li>
<li>desc：降序排序</li>
<li>如果不指定，默认是升序排序</li>
</ul>
</li>
<li><p>7：用于实现分页输出</p>
</li>
</ul>
</li>
</ul>
<h3 id="3、简单查询"><a href="#3、简单查询" class="headerlink" title="3、简单查询"></a>3、简单查询</h3><ul>
<li><p>查询所有的商品信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from product;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702152955689.png" alt="image-20200702152955689"></p>
</li>
<li><p>查询所有的商品名称和价格</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select pname,price from product;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702153232051.png" alt="image-20200702153232051"></p>
</li>
<li><p>查询所有的商品名称和价格，结果的列的名称分别为商品和价格</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select pname as &#x27;商品&#x27;, price as &#x27;价格&#x27; from product;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702153554316.png" alt="image-20200702153554316"></p>
<ul>
<li>as：用于给列或者表取别名</li>
</ul>
</li>
<li><p>查询所有商品的价格，并去掉重复价格</p>
<ul>
<li>查询所有商品价格</li>
</ul>
<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702153803436.png" alt="image-20200702153803436"></p>
<ul>
<li><p>去掉重复价格</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select distinct price from product;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702153930200.png" alt="image-20200702153930200"></p>
</li>
<li><p>distinct：用于对列值进行去重</p>
</li>
</ul>
</li>
<li><p>将所有商品的价格+10元显示</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select price as &#x27;价格&#x27; , price + 10 as &#x27;新价格&#x27; from product;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702154219464.png" alt="image-20200702154219464"></p>
<ul>
<li>直接对数值类型的列进行运算<ul>
<li>加：+</li>
<li>减：-</li>
<li>乘：*</li>
<li>除：&#x2F;</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="4、条件查询：where"><a href="#4、条件查询：where" class="headerlink" title="4、条件查询：where"></a>4、条件查询：where</h3><ul>
<li><p>功能：对于数据行的过滤</p>
</li>
<li><p>查询商品名称为“花花公子”的商品所有信息 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from product where pname = &#x27;花花公子&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702155631783.png" alt="image-20200702155631783"></p>
</li>
<li><p>查询价格为800商品  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from product where price = 800;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702155805675.png" alt="image-20200702155805675"></p>
</li>
<li><p>查询价格不是800的所有商品</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from  product where  price != 800;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702160012180.png" alt="image-20200702160012180"></p>
</li>
<li><p>查询商品价格大于60元的所有商品信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from product where price &gt; 60;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702160137164.png" alt="image-20200702160137164"></p>
<ul>
<li><p>等于：&#x3D;</p>
</li>
<li><p>不等于：！&#x3D;</p>
</li>
<li><p>小于：&lt;</p>
</li>
<li><p>大于：&gt;</p>
</li>
<li><p>小于等于：&lt;&#x3D;</p>
</li>
<li><p>大于等于：&gt;&#x3D;</p>
</li>
</ul>
</li>
<li><p>查询商品价格在200到1000之间所有商品</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select * from product where price &gt;= 200 and price &lt;= 1000;</span><br><span class="line">select * from product where price between 200 and 1000;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702160548070.png" alt="image-20200702160548070"></p>
<ul>
<li>and：并列关系，两个条件都要满足</li>
</ul>
</li>
<li><p>查询商品价格是200或800的所有商品</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from product where price = 200 or price = 800;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702160828528.png" alt="image-20200702160828528"></p>
<ul>
<li>or：或者关系，两个条件满足其中一个即可</li>
</ul>
</li>
<li><p>查询含有’霸’字的所有商品</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from product where pname like &#x27;%霸%&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702161145607.png" alt="image-20200702161145607"></p>
<ul>
<li>%：任意多个字符</li>
</ul>
</li>
<li><p>查询以’香’开头的所有商品</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from product where pname like &#x27;香%&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702161427047.png" alt="image-20200702161427047"></p>
</li>
<li><p>查询第二个字为’想’的所有商品</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from  product where pname like &#x27;_想%&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702161702350.png" alt="image-20200702161702350"></p>
<ul>
<li>_：表示一个字符</li>
</ul>
</li>
<li><p>查询没有分类的商品</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into product values(14,&#x27;weiC 100&#x27;,9.9,null);</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702162050786.png" alt="image-20200702162050786"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from product where category_id is null;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702162218157.png" alt="image-20200702162218157"></p>
</li>
<li><p>查询有分类的商品</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from product where category_id is not null;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702162327329.png" alt="image-20200702162327329"></p>
</li>
</ul>
<h3 id="5、聚合查询"><a href="#5、聚合查询" class="headerlink" title="5、聚合查询"></a>5、聚合查询</h3><ul>
<li><p>聚合函数</p>
<ul>
<li>函数：MySQL为你定义好的功能，你只要调用这个命令就可以实现聚合功能</li>
<li>MYSQL默认为我们提供的常见的聚合函数<ul>
<li>count(colname)：统计某一列的行数，统计个数，null不参与统计</li>
<li>sum（colname）：计算某一列的所有值的和，只能对数值类型求和，如果不是数值，结果为0</li>
<li>max（colname）：计算某一列的所有值中的最大值</li>
<li>min（colname）：计算某一列的所有值中的最小值</li>
<li>avg（colname）：计算某一列的平均值</li>
</ul>
</li>
</ul>
</li>
<li><p>查询商品的总条数 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select count(pid) as &#x27;总个数&#x27; from product;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702201119336.png" alt="image-20200702201119336"></p>
</li>
<li><p>查询价格大于200商品的总条数 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select count(pid) as &#x27;大于200的商品个数&#x27; from product where price &gt; 200;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702201447950.png" alt="image-20200702201447950"></p>
</li>
<li><p>查询分类为’c001’的所有商品价格的总和  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select sum(price) as totalPrice from product where category_id = &#x27;c001&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702201649601.png" alt="image-20200702201649601"></p>
</li>
<li><p>查询分类为’c002’所有商品的平均价格 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select avg(price) as &#x27;平均价格&#x27; from product where category_id = &#x27;c002&#x27;;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702201929937.png" alt="image-20200702201929937"></p>
</li>
<li><p>查询商品的最大价格和最小价格  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select max(price) as &#x27;最大价格&#x27;,min(price) as &#x27;最小价格&#x27; from product;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702202344072.png" alt="image-20200702202344072"></p>
</li>
</ul>
<h3 id="6、分组查询：gourp-by"><a href="#6、分组查询：gourp-by" class="headerlink" title="6、分组查询：gourp by"></a>6、分组查询：gourp by</h3><ul>
<li><p>关键字：group by col …… having</p>
</li>
<li><p>功能：按照某些列进行分组，对分组后的数据进行处理，一般都会搭配聚合函数使用</p>
</li>
<li><p>统计各个分类商品的个数  </p>
<ul>
<li><p>分析过程</p>
<ul>
<li><p>结果长什么样？</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">category_id			个数</span><br><span class="line">c001				3</span><br><span class="line">c002				5</span><br><span class="line">c003				3</span><br><span class="line">c004				2</span><br><span class="line">c005				1</span><br></pre></td></tr></table></figure>
</li>
<li><p>按照什么分组？</p>
<ul>
<li>按照category_id进行分组</li>
</ul>
</li>
<li><p>统计每组商品的个数</p>
<ul>
<li>count</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select category_id,count(*) as &#x27;个数&#x27; from product group by category_id;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702203429283.png" alt="image-20200702203429283"></p>
</li>
<li><p>统计查询每种分类中的商品的最大价格和最小价格</p>
<ul>
<li>分析<ul>
<li>结果长什么样？<ul>
<li>三列：分类的id		最大价格			最小价格</li>
</ul>
</li>
<li>按照分类的id进行分组</li>
<li>统计每个分组内部的最大价格和最小价格</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">select  </span><br><span class="line">    category_id,</span><br><span class="line">    max(price) as maxprice,</span><br><span class="line">    min(price) as minprice   </span><br><span class="line">from </span><br><span class="line">    product  </span><br><span class="line">group by </span><br><span class="line">    category_id;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702203945489.png" alt="image-20200702203945489"></p>
</li>
<li><p>统计各个分类商品的个数,且只显示个数大于1的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select category_id,count(*) as &#x27;个数&#x27; from product group by category_id;</span><br></pre></td></tr></table></figure>

<ul>
<li>需要对分组后的结果再进行行的过滤</li>
<li>where：实现对数据行的过滤，指定条件<ul>
<li>这个需求中不能使用where</li>
<li>因为where会在group by之前执行，而个数是在分组之后才产生的列</li>
</ul>
</li>
<li>having：实现对数据行的过滤，指定条件，写法与where一致<ul>
<li>用于分组之后结果数据的过滤</li>
<li>对分组以后的 结果进行过滤</li>
</ul>
</li>
<li>什么时候用where，什么时候用having<ul>
<li>你要过滤的条件是分组之前就存在的，还是分组以后才产生的</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select category_id,count(*) as &#x27;个数&#x27; from product group by category_id having count(*) &gt; 1;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702204613303.png" alt="image-20200702204613303"></p>
</li>
</ul>
<h3 id="7、排序查询：order-by"><a href="#7、排序查询：order-by" class="headerlink" title="7、排序查询：order by"></a>7、排序查询：order by</h3><ul>
<li><p>关键字：order by col…… 【 asc | desc】</p>
</li>
<li><p>功能：将结果按照某些列进行升序或者 降序的排序来显示</p>
<ul>
<li>默认是升序</li>
<li>asc：升序</li>
<li>desc：降序</li>
</ul>
</li>
<li></li>
<li><p>查询所有商品的信息，并按照价格降序排序</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from product order by  price desc;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702205645391.png" alt="image-20200702205645391"></p>
</li>
<li><p>查询所有商品的信息，并按照价格排序(降序)，如果价格相同，以分类排序(降序)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from product order by price desc,category_id desc;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702205923524.png" alt="image-20200702205923524"></p>
</li>
<li><p>统计各个分类商品的个数 ，并按照个数降序排序</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> category_id,<span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> <span class="string">&#x27;个数&#x27;</span> <span class="keyword">from</span> product <span class="keyword">group</span> <span class="keyword">by</span> category_id;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702210133787.png" alt="image-20200702210133787"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> category_id,<span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> <span class="string">&#x27;个数&#x27;</span> <span class="keyword">from</span> product <span class="keyword">group</span> <span class="keyword">by</span> category_id <span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702210257846.png" alt="image-20200702210257846"></p>
<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702210324695.png" alt="image-20200702210324695"></p>
</li>
</ul>
<h3 id="8、分页查询：limit"><a href="#8、分页查询：limit" class="headerlink" title="8、分页查询：limit"></a>8、分页查询：limit</h3><ul>
<li><p>关键字：limit</p>
</li>
<li><p>功能：限制输出的结果</p>
</li>
<li><p>语法：limit M,N</p>
<ul>
<li>M：你想从第M+1条开始显示</li>
<li>N：显示N条</li>
<li>显示第一条到第三条<ul>
<li>M：0</li>
<li>N：3</li>
</ul>
</li>
<li>显示第9条到第10条<ul>
<li>M：8</li>
<li>N：2</li>
</ul>
</li>
<li>如果从第一条开始，M为0，可以省略不写<ul>
<li>limit N</li>
</ul>
</li>
</ul>
</li>
<li><p>查询product表的前5条记录</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> product limit <span class="number">0</span>,<span class="number">5</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> product limit <span class="number">5</span>;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702211351033.png" alt="image-20200702211351033"></p>
</li>
<li><p>查询product表的第4条和第5条记录</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> product limit <span class="number">3</span>,<span class="number">2</span>;</span><br></pre></td></tr></table></figure>

<p>  <img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702211507141.png" alt="image-20200702211507141"></p>
</li>
<li><p>查询商品个数最多的分类的前三名</p>
<ul>
<li><p>查询所有商品分类的商品个数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> category_id,<span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> numb  <span class="keyword">from</span> product <span class="keyword">group</span> <span class="keyword">by</span> category_id;</span><br></pre></td></tr></table></figure>
</li>
<li><p>对上一步做排序</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> category_id,<span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> numb  <span class="keyword">from</span> product <span class="keyword">group</span> <span class="keyword">by</span> category_id <span class="keyword">order</span> <span class="keyword">by</span> numb <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>limit选择前三名</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> category_id,<span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> numb  <span class="keyword">from</span> product <span class="keyword">group</span> <span class="keyword">by</span> category_id <span class="keyword">order</span> <span class="keyword">by</span> numb <span class="keyword">desc</span> limit <span class="number">3</span>;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702211811837.png" alt="image-20200702211811837"></p>
</li>
</ul>
</li>
</ul>
<h3 id="9、结果保存"><a href="#9、结果保存" class="headerlink" title="9、结果保存"></a>9、结果保存</h3><ul>
<li><p>语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert into 表的名称   select……</span><br></pre></td></tr></table></figure>
</li>
<li><p>功能：将一条select语句运行的结果写入一张表中</p>
</li>
<li><p>注意：结果表的列一定要与Select语句的结果的列要匹配</p>
<ul>
<li>列的名称可以不一样</li>
<li>但是列的类型和个数必须一一对应</li>
</ul>
</li>
<li><p>统计各个分类商品的个数 ，并按照个数降序排序，并将结果进行保存</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_id,</span><br><span class="line">  <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> numb</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  product</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  category_id</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  numb <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702213032264.png" alt="image-20200702213032264"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--创建一张表用于存储分析的结果</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">result</span> (</span><br><span class="line">  cid <span class="type">varchar</span>(<span class="number">100</span>),</span><br><span class="line">  numb <span class="type">int</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702213133496.png" alt="image-20200702213133496"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--将分析的结果存储在这张表中</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">result</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">  category_id,</span><br><span class="line">  <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> numb</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  product</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  category_id</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">  numb <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702213315008.png" alt="image-20200702213315008"></p>
</li>
</ul>
<h2 id="七、多表关系与查询"><a href="#七、多表关系与查询" class="headerlink" title="七、多表关系与查询"></a>七、多表关系与查询</h2><h3 id="1、多表关系"><a href="#1、多表关系" class="headerlink" title="1、多表关系"></a>1、多表关系</h3><ul>
<li>电商数据库<ul>
<li>用户表<ul>
<li>用户id、用户名称、用户手机……</li>
</ul>
</li>
<li>商品表<ul>
<li>商品id、商品名称、商品价格、库存、尺寸……</li>
</ul>
</li>
<li>订单表<ul>
<li>订单id、用户id、商品id、总金额、支付方式</li>
</ul>
</li>
</ul>
</li>
<li>员工数据库<ul>
<li>员工表<ul>
<li>员工id、员工姓名、员工性别、年龄、部门id……</li>
</ul>
</li>
<li>部门表<ul>
<li>部门id、部门名称、部门位置、部门领导……</li>
</ul>
</li>
<li>员工和部门之间的关系<ul>
<li>员工属于某一个部门</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702220547447.png" alt="image-20200702220547447"></p>
<ul>
<li>表与表之间通过某些列来实现关联，表现数据之间的关系</li>
</ul>
<h3 id="2、join"><a href="#2、join" class="headerlink" title="2、join"></a>2、join</h3><ul>
<li><p>功能：通过两张表之间关联的列，实现将两张表的列进行合并</p>
</li>
<li><p>关键字：A    join B   on  条件</p>
</li>
<li><p>语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select 查询的两张表的哪些列  from A表  join  B表  on 关联条件;</span><br></pre></td></tr></table></figure>
</li>
<li><p>本质：通过某种列的关系，将两张表的列进行了关联</p>
</li>
<li><p>需求1：查询每个商品的名称以及所属分类的名称</p>
<ul>
<li><p>分析结果长什么样？</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">商品名称			分类名称</span><br><span class="line">联想					电脑</span><br><span class="line">……</span><br><span class="line">weiC 100			 吃的</span><br></pre></td></tr></table></figure>
</li>
<li><p>问题：商品名称 属于商品表，分类名称属于分类表</p>
</li>
<li><p>关系：分类id：categor_id</p>
</li>
<li><p>查询</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--将商品表与分类表通过分类id进行关联，并显示两张表的所有列</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">   a.<span class="operator">*</span>,</span><br><span class="line">   b.<span class="operator">*</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">   product <span class="keyword">as</span> a <span class="keyword">join</span> category <span class="keyword">as</span> b <span class="keyword">on</span> a.category_id <span class="operator">=</span> b.category_id;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702222803025.png" alt="image-20200702222803025"></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  a.pname,</span><br><span class="line">  b.category_name</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  product a <span class="keyword">join</span> category b <span class="keyword">on</span> a.category_id <span class="operator">=</span> b.category_id;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702223043014.png" alt="image-20200702223043014"></p>
</li>
</ul>
</li>
<li><p>需求2：统计每个分类名称对应的商品个数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">  b.category_name,</span><br><span class="line">  <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> numb</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">  product a <span class="keyword">join</span> category b <span class="keyword">on</span> a.category_id <span class="operator">=</span> b.category_id</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">  b.category_name;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702223505148.png" alt="image-20200702223505148"></p>
</li>
<li><p>需求3：统计除了吃的分类以外的所有分类的商品个数，并显示个数最多的前三个分类</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">   b.category_name,</span><br><span class="line">   <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">as</span> numb</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">   product a <span class="keyword">join</span> category b <span class="keyword">on</span> a.category_id <span class="operator">=</span> b.category_id</span><br><span class="line"><span class="keyword">where</span></span><br><span class="line">   b.category_name <span class="operator">!=</span> <span class="string">&#x27;吃的&#x27;</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">   b.category_name</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span></span><br><span class="line">   numb <span class="keyword">desc</span></span><br><span class="line">limit <span class="number">3</span>;</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702224023108.png" alt="image-20200702224023108"></p>
</li>
<li><p>分类</p>
<ul>
<li><p>inner join：内连接，inner可以省略</p>
<ul>
<li>关联条件中，两张表都有这个值，结果就有</li>
<li>类似于集合中两个集合的交集</li>
</ul>
</li>
<li><p>left outer join：左外连接，outer可以省略</p>
<ul>
<li><p>关联条件中，左表中有，结果就有</p>
</li>
<li><p>类似于集合中左表的全集</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">  a.pname,</span><br><span class="line">  b.category_name</span><br><span class="line">from</span><br><span class="line">  product a left join category b on a.category_id = b.category_id;</span><br></pre></td></tr></table></figure>

<ul>
<li>左表是product，右表是category</li>
<li>如果product表中有一条数据的category_id是c006，而category中没有</li>
<li>结果有</li>
</ul>
</li>
</ul>
</li>
<li><p>right   outer join：右外连接，outer可以省略</p>
<ul>
<li><p>关联条件中，右表中有，结果就有</p>
</li>
<li><p>类似于集合中右表的全集</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">select</span><br><span class="line">  a.pname,</span><br><span class="line">  b.category_name</span><br><span class="line">from</span><br><span class="line">  product a right join category b on a.category_id = b.category_id;</span><br></pre></td></tr></table></figure>

<ul>
<li>左表是product，右表是category</li>
<li>如果category表中有一条数据的category_id是c006，而product中没有</li>
<li>结果有</li>
</ul>
</li>
</ul>
</li>
<li><p>full   join：全连接</p>
<ul>
<li>关联条件中，两张表任意一边有，结果就有</li>
<li>类似于集合中的两张表全集</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3、子查询"><a href="#3、子查询" class="headerlink" title="3、子查询"></a>3、子查询</h3><ul>
<li><p>功能：在select语句中嵌套select语句</p>
</li>
<li><p>需求1：查询化妆品这个分类对应的所有商品信息</p>
<ul>
<li>分析结果长什么样？<ul>
<li>所有商品信息：product</li>
</ul>
</li>
<li>条件：化妆品这个分类对应的商品<ul>
<li>化妆品：category</li>
</ul>
</li>
<li>解决：先获取化妆品对应的分类id，然后根据分类id到商品表中查询这个分类id对应的商品</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select category_id from category where category_name = &#x27;化妆品&#x27;;</span><br><span class="line">select * from product where category_id = &#x27;c003&#x27;;</span><br></pre></td></tr></table></figure>

<p>|</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select * from product where category_id = (select category_id from category where category_name = &#x27;化妆品&#x27;);</span><br></pre></td></tr></table></figure>

<ul>
<li>先执行内层的SQL语句</li>
<li>然后执行外层的SQL语句</li>
</ul>
<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702230753367.png" alt="image-20200702230753367"></p>
</li>
<li><p>需求2：查询相宜本草对应的分类的名称</p>
<ul>
<li>结果：显示分类的 名称：category</li>
<li>条件：相宜本草  pname：product</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">--先查询相宜本草对应的分类id</span><br><span class="line">select category_id from product where pname = &#x27;相宜本草&#x27;;</span><br><span class="line">--根据分类id到分类表中查询分类的名称</span><br><span class="line">select category_name from category where category_id = &#x27;c003&#x27;;</span><br><span class="line">|</span><br><span class="line">select category_name from category where category_id = (select category_id from product where pname = &#x27;相宜本草&#x27;);</span><br></pre></td></tr></table></figure>

<p><img src="/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL.assets/image-20200702231153230.png" alt="image-20200702231153230"></p>
</li>
<li><p>join与子查询的应用场景</p>
<ul>
<li>如果你的查询结果包含多张表的列<ul>
<li>join</li>
</ul>
</li>
<li>如果你的查询结果只有一张表的列，条件来自于别的表<ul>
<li>子查询</li>
</ul>
</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/13/Day01%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9FMySQL/" data-id="clj25kfy70001n0urfvhb84w6" data-title="MySQL" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
  </nav>
</section>
        <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/06/19/zookeeper%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/">Zookeeper部署</a>
          </li>
        
          <li>
            <a href="/2023/06/19/Sqoop/">Sqoop</a>
          </li>
        
          <li>
            <a href="/2023/06/19/Spark%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/">Spark部署</a>
          </li>
        
          <li>
            <a href="/2023/06/19/Kafka/">Kafka</a>
          </li>
        
          <li>
            <a href="/2023/06/19/Hive3%E5%AE%89%E8%A3%85/">Hive</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 By Autoload<br>
      Driven - <a href="https://hexo.io/" target="_blank">Hexo</a>|Theme - <a href="https://github.com/autoload/hexo-theme-auto" target="_blank">Auto</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>