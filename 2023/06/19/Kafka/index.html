<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Kafka | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Kafka" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/06/19/Kafka/" class="article-date">
  <time class="dt-published" datetime="2023-06-19T00:49:49.449Z" itemprop="datePublished">2023-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Kafka
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="一、Kafka简介"><a href="#一、Kafka简介" class="headerlink" title="一、Kafka简介"></a>一、Kafka简介</h2><h3 id="1-1-什么是kafka"><a href="#1-1-什么是kafka" class="headerlink" title="1.1 什么是kafka"></a>1.1 什么是kafka</h3><p>Kafka 它最初由LinkedIn 公司开发，之后成为Apache 项目的一部分。<br>Kafka 是一个<code>分布式消息中间件,支持分区的、多副本的、多订阅者的</code>、<code>基于zookeeper 协调的</code>分布式<br>消息系统。<br>通俗来说：<code> kafka 就是一个存储系统，存储的数据形式为“消息&quot;；</code><br>它的主要作用类似于蓄水池，起到一个<code>缓冲</code>作用；</p>
<img src=".\md图\kafka.assets\image-20230204221620734.png" alt="image-20230204221620734" style="zoom:50%;" />

<p>Kafka是由Apache软件基金会开发的一个开源流平台，由Scala和Java编写。Kafka的Apache官网是这样介绍Kakfa的。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Apache Kafka是一个分布式流平台。一个分布式的流平台应该包含3点关键的能力：</span><br><span class="line">1. 发布和订阅流数据流，类似于消息队列或者是企业消息传递系统</span><br><span class="line">2. 以容错的持久化方式存储数据流</span><br><span class="line">3. 处理数据流</span><br><span class="line"></span><br><span class="line">英文原版：</span><br><span class="line">-Publish and subscribe to streams of records, similar to a message queue or enterprise </span><br><span class="line">messaging system.</span><br><span class="line">-Store streams of records in a fault-tolerant durable way.</span><br><span class="line">-Process streams of records as they occur.</span><br><span class="line"></span><br><span class="line">我们重点关键三个部分的关键词：</span><br><span class="line">1. Publish and subscribe：发布与订阅</span><br><span class="line">2. Store：存储</span><br><span class="line">3. Process：处理</span><br><span class="line">后续我们的课程主要围绕这三点来讲解。</span><br></pre></td></tr></table></figure>

<h4 id="1-1-1-消息队列"><a href="#1-1-1-消息队列" class="headerlink" title="1.1.1 消息队列"></a>1.1.1 消息队列</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">消息队列常用于两个系统之间的数据传递；</span><br><span class="line"></span><br><span class="line">-消息队列——用于存放消息的组件</span><br><span class="line"></span><br><span class="line">-程序员可以将消息放入到队列中，也可以从消息队列中获取消息</span><br><span class="line"></span><br><span class="line">-很多时候消息队列不是一个永久性的存储，是作为临时存储存在的（设定一个期限：设置消息在MQ中保存10天）</span><br><span class="line"></span><br><span class="line">-消息队列中间件：常见的消息队列组件，例如：Kafka、Active MQ、RabbitMQ、RocketMQ、ZeroMQ</span><br></pre></td></tr></table></figure>



<h3 id="1-2Kafka的应用场景"><a href="#1-2Kafka的应用场景" class="headerlink" title="1.2Kafka的应用场景"></a>1.2Kafka的应用场景</h3><h4 id="（1）应用场景"><a href="#（1）应用场景" class="headerlink" title="（1）应用场景"></a>（1）应用场景</h4><p>我们通常将Apache Kafka用在两类程序：</p>
<ol>
<li><p>建立实时数据管道，以可靠地在系统或应用程序之间获取数据</p>
</li>
<li><p>构建实时流应用程序，以转换或响应数据流</p>
</li>
</ol>
<img src=".\md图\kafka.assets\image-20230204221832864.png" alt="image-20230204221832864" style="zoom:80%;" />

<p>上图，我们可以看到：</p>
<ol>
<li><p><code>Producers：</code>可以有很多的应用程序，将消息数据放入到Kafka集群中。</p>
</li>
<li><p><code>Consumers：</code>可以有很多的应用程序，将消息数据从Kafka集群中拉取出来。</p>
</li>
<li><p><code>Connectors：</code>Kafka的连接器可以将数据库中的数据导入到Kafka，也可以将Kafka的数据导出到</p>
</li>
</ol>
<p>数据库中。</p>
<ol start="4">
<li><code>Stream Processors：</code>流处理器可以Kafka中拉取数据，也可以将数据写入到Kafka中。</li>
</ol>
<h4 id="（2）kafka诞生背景"><a href="#（2）kafka诞生背景" class="headerlink" title="（2）kafka诞生背景"></a>（2）kafka诞生背景</h4><p>kafka的诞生，是为了解决<code>linkedin的数据管道问题</code>，起初linkedin采用了<code>ActiveMQ来进行数据交换</code>，大约是在2010年前后，那时的ActiveMQ还<code>远远无法满足linkedin对数据传递系统的要求</code>，经常由于各种缺陷而导致<code>消息阻塞</code>或者<code>服务无法正常访问</code>，为了能够解决这个问题，linkedin决定研发自己的消息传递系统，当时linkedin的首席架构师jay kreps便开始组织团队进行<code>消息传递系统</code>的研发。</p>
<p>提示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. Linkedin还是挺厉害的</span><br><span class="line">2. Kafka比ActiveMQ厉害得多</span><br></pre></td></tr></table></figure>



<ul>
<li><code>异步处理</code><ul>
<li>可以将一些比较耗时的操作放在其他系统中，通过消息队列将需要进行处理的消息进行存储，其他系统可以消费消息队列中的数据</li>
<li>比较常见的：发送短信验证码、发送邮件</li>
</ul>
</li>
</ul>
<p><img src="/md%E5%9B%BE/kafka.assets/image-20200916093856262.png" alt="image-20200916093856262"></p>
<ul>
<li><code>系统解耦</code><ul>
<li>原先一个微服务是通过接口（HTTP）调用另一个微服务，这时候耦合很严重，只要接口发生变化就会导致系统不可用</li>
<li>使用消息队列可以将系统进行解耦合，现在第一个微服务可以将消息放入到消息队列中，另一个微服务可以从消息队列中把消息取出来进行处理。进行系统解耦</li>
</ul>
</li>
</ul>
<p><img src="/md%E5%9B%BE/kafka.assets/image-20200916093908261.png" alt="image-20200916093908261"></p>
<ul>
<li><code>流量削峰</code><ul>
<li>因为消息队列是低延迟、高可靠、高吞吐的，可以应对大量并发</li>
</ul>
</li>
</ul>
<p><img src="/md%E5%9B%BE/kafka.assets/image-20200916093919754.png" alt="image-20200916093919754"></p>
<ul>
<li><code>日志处理</code><ul>
<li>可以使用消息队列作为临时存储，或者一种通信管道</li>
</ul>
</li>
</ul>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203213716547.png" alt="image-20230203213716547"></p>
<h3 id="1-3消息队列的两种模型"><a href="#1-3消息队列的两种模型" class="headerlink" title="1.3消息队列的两种模型"></a>1.3消息队列的两种模型</h3><ul>
<li>生产者、消费者模型<ul>
<li>生产者负责将消息生产到MQ中</li>
<li>消费者负责从MQ中获取消息</li>
<li>生产者和消费者是解耦的，可能是生产者一个程序、消费者是另外一个程序</li>
</ul>
</li>
<li>消息队列的模式<ul>
<li>点对点：一个消费者消费一个消息</li>
<li>发布订阅：多个消费者可以消费一个消息</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">分布式消息传递基于可靠的消息队列，在客户端应用和消息系统之间异步传递消息。</span><br><span class="line"></span><br><span class="line">有两种主要的消息传递模式：点对点传递模式、发布-订阅模式。</span><br></pre></td></tr></table></figure>

<img src=".\md图\kafka.assets\image-20230203212920000.png" alt="image-20230203212920000" style="zoom:80%;" />

<img src=".\md图\kafka.assets\image-20230203213006782.png" alt="image-20230203213006782" style="zoom:80%;" />

<p><strong>大部分的消息系统选用发布-订阅模式。</strong><br><strong>kafka 是发布-订阅模式。</strong></p>
<h3 id="1-4-kafka的特点与优势"><a href="#1-4-kafka的特点与优势" class="headerlink" title="1.4 kafka的特点与优势"></a>1.4 kafka的特点与优势</h3><h4 id="（1）特点"><a href="#（1）特点" class="headerlink" title="（1）特点"></a>（1）特点</h4><ol>
<li>高吞吐量、低延迟：kafka 每秒可以处理几十万条消息，它的延迟最低只有几毫秒，每个topic 可</li>
<li>以分多个partition, 由多个consumer group 对partition 进行consume 操作。</li>
<li>可扩展性：kafka 集群支持热扩展</li>
<li>持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失</li>
<li>容错性：允许集群中节点失败（若副本数量为n,则允许n-1 个节点失败）</li>
<li>高并发：支持数千个客户端同时读写</li>
</ol>
<h4 id="（2）优势"><a href="#（2）优势" class="headerlink" title="（2）优势"></a>（2）优势</h4><p>前面我们了解到，消息队列中间件有很多，为什么我们要选择Kafka？</p>
<table>
<thead>
<tr>
<th>特性</th>
<th>ActiveMQ</th>
<th>RabbitMQ</th>
<th>Kafka</th>
<th>RocketMQ</th>
</tr>
</thead>
<tbody><tr>
<td>所属社区&#x2F;公司</td>
<td>Apache</td>
<td>Mozilla Public License</td>
<td>Apache</td>
<td>Apache&#x2F;Ali</td>
</tr>
<tr>
<td>成熟度</td>
<td>成熟</td>
<td>成熟</td>
<td>成熟</td>
<td>比较成熟</td>
</tr>
<tr>
<td>生产者-消费者模式</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>发布-订阅</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>REQUEST-REPLY</td>
<td>支持</td>
<td>支持</td>
<td>-</td>
<td>支持</td>
</tr>
<tr>
<td>API完备性</td>
<td>高</td>
<td>高</td>
<td>高</td>
<td>低（静态配置）</td>
</tr>
<tr>
<td>多语言支持</td>
<td>支持JAVA优先</td>
<td>语言无关</td>
<td>支持，JAVA优先</td>
<td>支持</td>
</tr>
<tr>
<td>单机呑吐量</td>
<td>万级（最差）</td>
<td>万级</td>
<td><strong>十万级</strong></td>
<td>十万级（最高）</td>
</tr>
<tr>
<td>消息延迟</td>
<td>-</td>
<td>微秒级</td>
<td><strong>毫秒级</strong></td>
<td>-</td>
</tr>
<tr>
<td>可用性</td>
<td>高（主从）</td>
<td>高（主从）</td>
<td><strong>非常高（分布式）</strong></td>
<td>高</td>
</tr>
<tr>
<td>消息丢失</td>
<td>-</td>
<td>低</td>
<td><strong>理论上不会丢失</strong></td>
<td>-</td>
</tr>
<tr>
<td>消息重复</td>
<td>-</td>
<td>可控制</td>
<td>理论上会有重复</td>
<td>-</td>
</tr>
<tr>
<td>事务</td>
<td>支持</td>
<td>不支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>文档的完备性</td>
<td>高</td>
<td>高</td>
<td>高</td>
<td>中</td>
</tr>
<tr>
<td>提供快速入门</td>
<td>有</td>
<td>有</td>
<td>有</td>
<td>无</td>
</tr>
<tr>
<td>首次部署难度</td>
<td>-</td>
<td>低</td>
<td>中</td>
<td>高</td>
</tr>
</tbody></table>
<h3 id="1-5kafka的使用场景"><a href="#1-5kafka的使用场景" class="headerlink" title="1.5kafka的使用场景"></a>1.5kafka的使用场景</h3><h4 id="（1）使用场景"><a href="#（1）使用场景" class="headerlink" title="（1）使用场景"></a>（1）使用场景</h4><p><strong>主要用于数据处理系统中的缓冲！（尤其是实时流式数据处理）</strong></p>
<ol>
<li><p><code>日志收集</code>：一个公司可以用kafka 可以收集各种服务的log，通过kafka 以统一接口服务的方式开</p>
<p>放给各种consumer，例如hadoop、HBase、Solr 等。</p>
</li>
<li><p><code>消息系统：</code>解耦和生产者和消费者、缓存消息等。</p>
</li>
<li><p><code>用户活动跟踪：</code>kafka 经常被用来记录web 用户或者app 用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka 的topic 中，然后订阅者通过订阅这些topic</p>
<p>来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。</p>
</li>
<li><p><code>运营指标：</code>kafka 也经常用来记录运维监控数据。包括收集各种分布式应用的数据，各种操作的</p>
<p>集中反馈，比如报警和报告。</p>
</li>
<li><p><code>流式数据处理：</code>比如spark streaming 和Flink</p>
</li>
</ol>
<h4 id="（2）哪些公司在使用kafka"><a href="#（2）哪些公司在使用kafka" class="headerlink" title="（2）哪些公司在使用kafka"></a>（2）哪些公司在使用kafka</h4><img src=".\md图\kafka.assets\image-20230204222605927.png" alt="image-20230204222605927" style="zoom: 50%;" />

<img src=".\md图\kafka.assets\image-20230204222623936.png" alt="image-20230204222623936" style="zoom:50%;" />

<h4 id="（3）kafka生态圈"><a href="#（3）kafka生态圈" class="headerlink" title="（3）kafka生态圈"></a>（3）kafka生态圈</h4><p>Apache Kafka这么多年的发展，目前也有一个较庞大的生态圈。</p>
<p>Kafka生态圈官网地址：<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem">https://cwiki.apache.org/confluence/display/KAFKA/Ecosystem</a></p>
<img src=".\md图\kafka.assets\image-20230204222917611.png" alt="image-20230204222917611" style="zoom: 67%;" />



<h2 id="二、Kafka系统架构（基础重点）"><a href="#二、Kafka系统架构（基础重点）" class="headerlink" title="二、Kafka系统架构（基础重点）"></a>二、Kafka系统架构（基础重点）</h2><p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203214337255.png" alt="image-20230203214337255"></p>
<p><strong>Kafka 架构分为以下几个部分</strong></p>
<h3 id="2-1-producer"><a href="#2-1-producer" class="headerlink" title="2.1 producer"></a>2.1 producer</h3><p>消息生产者，就是向kafka broker 发消息的客户端。</p>
<h3 id="2-2-consumer"><a href="#2-2-consumer" class="headerlink" title="2.2 consumer"></a>2.2 consumer</h3><p>consumer ：消息消费者，从kafka broker 取消息的客户端。<br>consumer group：单个或多个consumer 可以组成一个consumer group；这是kafka 用来实现消息的广<br>播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic 可以有多个ConsumerGroup。</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203214453794.png" alt="image-20230203214453794"></p>
<h3 id="2-3-topic"><a href="#2-3-topic" class="headerlink" title="2.3 topic"></a>2.3 topic</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">数据的逻辑分类；</span><br><span class="line">可以理解为数据库中“表&quot;的概念；</span><br></pre></td></tr></table></figure>

<h4 id="1-partition"><a href="#1-partition" class="headerlink" title="1.partition"></a>1.<strong>partition</strong></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">topic 中数据的具体管理单元；（可以理解为hbase 中表的“region&quot;概念）</span><br><span class="line"></span><br><span class="line">一个topic 可以划分为多个partition，分布到多个broker 上管理；</span><br><span class="line">每个partition 由一个kafka broker 服务器管理；</span><br><span class="line">partition 中的每条消息都会被分配一个递增的id（offset）；</span><br><span class="line">每个partition 是一个有序的队列，kafka 只保证按一个partition 中的消息的顺序，不保证一个topic的整体（多个partition 间）的顺序。</span><br><span class="line">每个partition 都可以有多个副本；</span><br></pre></td></tr></table></figure>

<h4 id="2-broker"><a href="#2-broker" class="headerlink" title="2.broker"></a>2.<strong>broker</strong></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">一台kafka 服务器就是一个broker。</span><br><span class="line">一个kafka 集群由多个broker 组成。</span><br><span class="line">一个broker 可以容纳多个topic 的多个partition。</span><br><span class="line">分区对于kafka 集群的好处是：实现topic 数据的负载均衡。分区对于消费者来说，可以提高并发度，</span><br><span class="line">提高效率。</span><br></pre></td></tr></table></figure>

<h4 id="3-offset"><a href="#3-offset" class="headerlink" title="3.offset"></a>3.offset</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">消息在底层存储中的索引位置，kafka 底层的存储文件就是以文件中第一条消息的offset 来命名的，</span><br><span class="line">通过offset 可以快速定位到消息的具体存储位置；</span><br></pre></td></tr></table></figure>

<h3 id="2-4-Leader"><a href="#2-4-Leader" class="headerlink" title="2.4 Leader"></a>2.4 Leader</h3><p><code>partition replica</code> 中的一个角色，producer 和consumer 只跟leader 交互（负责读写）。</p>
<h3 id="2-5-副本Replica"><a href="#2-5-副本Replica" class="headerlink" title="2.5 副本Replica"></a>2.5 副本Replica</h3><p>partition 的副本，保障partition 的高可用（replica 副本数目不能大于kafka broker 节点的数目，否则报<br>错。<br>每个partition 的所有副本中，必包括一个leader 副本，其他的就是follower 副本</p>
<h3 id="2-6-Follower"><a href="#2-6-Follower" class="headerlink" title="2.6 Follower"></a>2.6 Follower</h3><p>partition replica 中的一个角色，从leader 中拉取复制数据（只负责备份）。<br>如果leader 所在节点宕机，follower 中会选举出新的leader；</p>
<h3 id="2-7-偏移量Offset"><a href="#2-7-偏移量Offset" class="headerlink" title="2.7 偏移量Offset"></a>2.7 偏移量Offset</h3><p>每一条数据都有一个offset，是数据在该partition 中的唯一标识（其实就是消息的索引号）。<br>各个consumer 会保存其消费到的offset 位置，这样下次可以从该offset 位置开始继续消费；<br>consumer 的消费offset 保存在一个专门的topic（__consumer_offsets）中；（0.10.x 版本以前是保存在zk 中）</p>
<h3 id="2-8-消息Message"><a href="#2-8-消息Message" class="headerlink" title="2.8 消息Message"></a>2.8 消息Message</h3><p>在客户端编程代码中，消息的类叫做ProducerRecord； ConsumerRecord；<br>简单来说，kafka 中的每个massage 由一对key-value 构成<br>Kafka 中的message 格式经历了3 个版本的变化了：version0 、version1 、version2</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203215042738.png" alt="image-20230203215042738"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">各个字段的含义介绍如下：</span><br><span class="line">crc：占用4 个字节，主要用于校验消息的内容；</span><br><span class="line"></span><br><span class="line">magic：这个占用1 个字节，主要用于标识Kafka 版本。Kafka 0.10.x magic 默认值为1</span><br><span class="line"></span><br><span class="line">attributes：占用1 个字节，这里面存储了消息压缩使用的编码以及Timestamp 类型。目前Kafka 支</span><br><span class="line">持gzip、snappy 以及lz4（0.8.2 引入） 三种压缩格式；后四位如果是0001 则表示gzip 压缩，</span><br><span class="line">如果是0010 则是snappy 压缩，如果是0011 则是lz4 压缩，如果是0000 则表示没有使用压缩。</span><br><span class="line">第4 个bit 位如果为0，代表使用create time；如果为1 代表append time；其余位（第5~8 位）保</span><br><span class="line">留；</span><br><span class="line"></span><br><span class="line">key length：占用4 个字节。主要标识Key 的内容的长度；</span><br><span class="line"></span><br><span class="line">key：占用N 个字节，存储的是key 的具体内容；</span><br><span class="line"></span><br><span class="line">value length：占用4 个字节。主要标识value 的内容的长度；</span><br><span class="line"></span><br><span class="line">value：value 即是消息的真实内容，在Kafka 中这个也叫做payload。</span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203215333470.png" alt="image-20230203215333470"></p>
<h2 id="三、kafka-的数据存储结构"><a href="#三、kafka-的数据存储结构" class="headerlink" title="三、kafka 的数据存储结构"></a>三、kafka 的数据存储结构</h2><h3 id="3-1-kafka-的整体存储结构"><a href="#3-1-kafka-的整体存储结构" class="headerlink" title="3.1 kafka 的整体存储结构"></a>3.1 kafka 的整体存储结构</h3><p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203215459084.png" alt="image-20230203215459084"></p>
<h3 id="3-2-服务器存储结构示例"><a href="#3-2-服务器存储结构示例" class="headerlink" title="3.2 服务器存储结构示例"></a>3.2 服务器存储结构示例</h3><p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203215550273.png" alt="image-20230203215550273"></p>
<p><strong>注：“t1”即为一个topic 的名称；</strong><br><strong>而“t1-0 &#x2F; t1-1”则表明这个目录是t1 这个topic 的哪个partition；</strong></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203215645133.png" alt="image-20230203215645133"></p>
<p>由于生产者生产的消息会不断追加到log 文件末尾，为防止log 文件过大导致数据定位效率低下，<br>Kafka 采取了分片和索引机制，将每个partition 分为多个segment。每个segment 对应两个文件：<br>“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic 名称-<br>分区序号。</p>
<p>index 和log 文件以当前segment 的第一条消息的offset 命名。</p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230203215709598.png" alt="image-20230203215709598"></p>
<p>“.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中message 的物理偏移地址。</p>
<p>Kafka 中的索引文件以稀疏索引（ sparse index ）的方式构造消息的索引，它并不保证每个消息在<br>索引文件中都有对应的索引；每当写入一定量（由broker 端参数log.index.interval.bytes 指定，<br>默认值为4096 ，即4KB ）的消息时，偏移量索引文件和时间戳索引文件分别增加一个偏移量索引<br>项和时间戳索引项，增大或减小log.index.interval.bytes 的值，对应地可以增加或缩小索引项的<br>密度；<br>偏移量索引文件中的偏移量是单调递增的，查询指定偏移量时，使用二分查找法来快速定位偏移量的<br>位置。</p>
<h2 id="四、Kafka集群搭建"><a href="#四、Kafka集群搭建" class="headerlink" title="四、Kafka集群搭建"></a>四、Kafka集群搭建</h2><h3 id="4-0-kafka版本"><a href="#4-0-kafka版本" class="headerlink" title="4.0 kafka版本"></a>4.0 kafka版本</h3><p>本次课程使用的Kafka版本为2.4.1，是2020年3月12日发布的版本。</p>
<p>可以注意到Kafka的版本号为：kafka_2.12-2.4.1，因为kafka主要是使用scala语言开发的，2.12为scala的版本号。<a target="_blank" rel="noopener" href="http://kafka.apache.org/downloads%E5%8F%AF%E4%BB%A5%E6%9F%A5%E7%9C%8B%E5%88%B0%E6%AF%8F%E4%B8%AA%E7%89%88%E6%9C%AC%E7%9A%84%E5%8F%91%E5%B8%83%E6%97%B6%E9%97%B4%E3%80%82">http://kafka.apache.org/downloads可以查看到每个版本的发布时间。</a></p>
<h3 id="4-1-安装zookeeper-集群"><a href="#4-1-安装zookeeper-集群" class="headerlink" title="4.1 安装zookeeper 集群"></a>4.1 安装zookeeper 集群</h3><ul>
<li>Kafka集群是必须要有<code>ZooKeeper</code>的</li>
</ul>
<p>注意：</p>
<ul>
<li>每一个Kafka的节点都需要修改<code>broker.id</code>（每个节点的标识，不能重复）</li>
<li><code>log.dir</code>数据存储目录需要配置</li>
</ul>
<p>详见<code>zookeeper部署文档.md</code></p>
<h3 id="4-2-安装kafka-集群"><a href="#4-2-安装kafka-集群" class="headerlink" title="4.2 安装kafka 集群"></a>4.2 安装kafka 集群</h3><p><strong>上传安装包</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">移动到指定文件夹</span><br><span class="line"></span><br><span class="line">mv kafka_2.12-2.4.1.tgz /export/server</span><br><span class="line"></span><br><span class="line">tar -zxvf kafka_2.12-2.4.1.tgz</span><br></pre></td></tr></table></figure>

<p><strong>目录结构分析</strong></p>
<table>
<thead>
<tr>
<th>目录名称</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>bin</td>
<td>Kafka的所有执行脚本都在这里。例如：启动Kafka服务器、创建Topic、生产者、消费者程序等等</td>
</tr>
<tr>
<td>config</td>
<td>Kafka的所有配置文件</td>
</tr>
<tr>
<td>libs</td>
<td>运行Kafka所需要的所有JAR包</td>
</tr>
<tr>
<td>logs</td>
<td>Kafka的所有日志文件，如果Kafka出现一些问题，需要到该目录中去查看异常信息</td>
</tr>
<tr>
<td>site-docs</td>
<td>Kafka的网站帮助文件</td>
</tr>
</tbody></table>
<p><strong>修改配置文件</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(1)进入配置文件目录</span></span><br><span class="line">cd /export/server/kafka_2.11-2.0.0/config</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(2)编辑配置文件</span></span><br><span class="line">vi server.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#为依次增长的:0、1、2、3、4,集群中唯一 id --》从0开始，每台不能重复，第一块要改的</span></span><br><span class="line"><span class="attr">broker.id</span>=<span class="string">0 </span></span><br><span class="line"></span><br><span class="line"><span class="attr">----Logbasic------</span></span><br><span class="line"><span class="comment">#数据存储的目录，第二块要改的</span></span><br><span class="line"><span class="attr">log.dirs</span>=<span class="string">/export/data/kafka-logs  </span></span><br><span class="line"></span><br><span class="line"><span class="attr">---zookeeper----</span></span><br><span class="line"><span class="comment">#指定 zk 集群地址，第四块要改的</span></span><br><span class="line"><span class="attr">zookeeper.connect</span>=<span class="string">node1:2181,node2:2181,node3:2181</span></span><br></pre></td></tr></table></figure>



<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">分发kafka</span></span><br><span class="line">cd /export/server/</span><br><span class="line">syncfile /export/server/kafka_2.12-2.4.1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">配置环境变量</span></span><br><span class="line">vi /etc/profile </span><br><span class="line"></span><br><span class="line">export KAFKA_HOME=/export/server/kafka </span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin </span><br><span class="line">source /etc/profile </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">注意:还需要分发环境变量</span></span><br><span class="line">syncfile /etc/profile</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">分别在node2和node3上修改配置文件</span></span><br><span class="line"></span><br><span class="line">vim /export/server/kafka/config/server.propertie</span><br><span class="line">broker.id=1 </span><br><span class="line">broker.id=2</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(broker.id 不能重复)</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启停集群(在各个节点上启动)</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">启动集群</span></span><br><span class="line">kafka-server-start.sh -daemon /export/server/kafka/config/server.properties </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">停止集群</span></span><br><span class="line">kafka-server-stop.sh stop</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kafka一键启停脚本</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ $# -eq 0 ]</span><br><span class="line">then</span><br><span class="line">echo &quot;please input param:start stop&quot;</span><br><span class="line">else</span><br><span class="line"></span><br><span class="line">if [ $1 = start  ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;$&#123;1&#125;ing node$&#123;i&#125;&quot;</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/kafka/bin/kafka-server-start.sh -daemon /export/server/kafka/config/server.properties&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $1 = stop ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/kafka/bin/kafka-server-stop.sh&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>



<h3 id="4-3-kafka命令行操作"><a href="#4-3-kafka命令行操作" class="headerlink" title="4.3 kafka命令行操作"></a>4.3 kafka命令行操作</h3><p>Kafka 中提供了许多命令行工具（位于$KAFKA HOME&#x2F;bin 目录下）用于管理集群的变更。</p>
<table>
<thead>
<tr>
<th align="center">命令</th>
<th align="center">用途</th>
</tr>
</thead>
<tbody><tr>
<td align="center">kafka-configs.sh</td>
<td align="center">用于配置管理</td>
</tr>
<tr>
<td align="center">kafka-console-consumer.sh</td>
<td align="center">用于消费消息</td>
</tr>
<tr>
<td align="center">kafka-console-producer.sh</td>
<td align="center">用于生产消息</td>
</tr>
<tr>
<td align="center">kafka-consumer-perf-test.sh</td>
<td align="center">用于测试消费性能</td>
</tr>
<tr>
<td align="center">kafka-topics.sh</td>
<td align="center">用于管理主题</td>
</tr>
<tr>
<td align="center">kafka-dump-log.sh</td>
<td align="center">用于查看日志内容</td>
</tr>
<tr>
<td align="center">kafka-server-stop.sh</td>
<td align="center">用于关闭Kafka服务</td>
</tr>
<tr>
<td align="center">kafka-preferred-replica-election.sh</td>
<td align="center">用于优先副本的选举</td>
</tr>
<tr>
<td align="center">kafka-server-start.sh</td>
<td align="center">用于启动Kafka服务</td>
</tr>
<tr>
<td align="center">kafka-producer-perf-test.sh</td>
<td align="center">用于测试生产性能</td>
</tr>
<tr>
<td align="center">kafka-reassign-partitions.sh</td>
<td align="center">用于分区重分配</td>
</tr>
</tbody></table>
<h4 id="（1）创建topic"><a href="#（1）创建topic" class="headerlink" title="（1）创建topic"></a>（1）创建topic</h4><p>创建一个topic（主题）。Kafka中所有的消息都是保存在主题中，要生产消息到Kafka，首先必须要有一个确定的主题。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">基本方式</span></span><br><span class="line">./kafka-topics.sh --create --topic tpc_1 --partitions 2 --replication-factor 2 --zookeeper node1:2181</span><br><span class="line"></span><br><span class="line">--replication-factor 副本数量</span><br><span class="line">--partitions 分区数量</span><br><span class="line">--topic topic 名称</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">手动指定副本的存储位置</span></span><br><span class="line">bin/kafka-topics.sh --create --topic tpc_1 --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line">该方式下,命令会自动判断所要创建的 topic 的分区数及副本数</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">bootstrap方式</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建名为<span class="built_in">test</span>的主题</span></span><br><span class="line">bin/kafka-topics.sh --create --bootstrap-server node1:9092 --topic test</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看目前Kafka中的主题</span></span><br><span class="line">bin/kafka-topics.sh --list --bootstrap-server node1:9092</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">--replica-assignment 不能同时使用--partitions --replication-factor参数指定partition的AR列表，未指定AR列表则会根据负载均衡算法将partition的replica均衡的分布在Kafka集群中。</span><br><span class="line"></span><br><span class="line">--replica-assignment 1:3,2:1,3:2，</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">逗号区分不同的partition，</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">冒号区别相同partition中的replica，</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">partition-0的AR=[1,3]，partition-1的AR=[2,1]，partition-2的AR=[3,2]。</span></span><br><span class="line"></span><br><span class="line">Eg：testMcdull222AR列表计算出来时--replica-assignment 2:3,1:3,1:2 。数字指的是broker的ID号</span><br></pre></td></tr></table></figure>

<img src=".\md图\kafka.assets\image-20230204132807408.png" alt="image-20230204132807408" style="zoom:50%;" />

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">--replica-assignment 参数一般不由用户指定，由Kafka默认分配算法保证，有两个原则：</span><br><span class="line"></span><br><span class="line">（1）使Topic的所有Partition Replica能够均匀地分配至各个Kafka Broker（负载均衡）；</span><br><span class="line">（2）Partition 内的replica能够均匀地分配在不同Kafka Broker。</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如果Partition的第一个Replica分配至某一个Kafka Broker，那么这个Partition的其它Replica则需要分配至其它的Kafka Brokers，即Partition Replica分配至不同的Broker；</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">分配原则</span></span><br><span class="line">1、从Broker随机位置开始按照轮询方式选择每个Partition的第一个replica</span><br><span class="line">2、不同Partition剩余replica按照一定的偏移量紧跟着各自的第一个replica</span><br></pre></td></tr></table></figure>

<img src=".\md图\kafka.assets\image-20230204133027944.png" alt="image-20230204133027944" style="zoom: 50%;" />

<h4 id="（2）删除topic"><a href="#（2）删除topic" class="headerlink" title="（2）删除topic"></a>（2）删除topic</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh  --delete --topic tpc_1 --zookeeper node1：2181</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">（异步线程去删除）删除 topic,需要一个参数处于启用状态: delete.topic.enable = <span class="literal">true</span>,否则删不掉</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">使用 kafka-topics.sh 脚本删除主题的行为本质上只是在 ZooKeeper 中的 /admin/delete_topics 路径下 建一个与待删除主题同名的节点,以标记该主题为待删除的状态。与创建主题相同的是,真正删除主题的动作也是由 Kafka 的控制器负责完成的。</span></span><br></pre></td></tr></table></figure>

<h4 id="（3）查看topic"><a href="#（3）查看topic" class="headerlink" title="（3）查看topic"></a>（3）查看topic</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(1)列出当前系统中的所有 topic</span> </span><br><span class="line">bin/kafka-topics.sh --zookeeper node1:2181,node2:2181,node3:2181 –list</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(2)查看 topic 详细信息</span></span><br><span class="line">bin/kafka-topics.sh --create --topic tpc_1   --zookeeper node1:2181 --replica-assignment 0:1,1:2</span><br><span class="line">bin/kafka-topics.sh --describe --topic tpc_1 --zookeper node1:2181 </span><br></pre></td></tr></table></figure>

<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204133351092.png" alt="image-20230204133351092"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">	 Topic: tpc_1 PartitionCount:2 ReplicationFactor:2 Configs: </span><br><span class="line">	 Topic: tpc_1 Partition: 0 Leader: 0 Replicas: 0,1 Isr: 0,1</span><br><span class="line">	 Topic: tpc_1 Partition: 1 Leader: 1 Replicas: 1,2 Isr: 1,2</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">从上面的结果中, 可以看出, topic 的分区数量, 以及每个分区的副本数量, 以及每个副本所在的 broker 节点,以及每个分区的 leader 副本所在 broker 节点,以及每个分区的 ISR 副本列表;</span> </span><br><span class="line"></span><br><span class="line">AR=ISR+OSR</span><br><span class="line"></span><br><span class="line">ISR: in sync replicas 同步副本(当然也包含 leader 自身) -&gt;follower去找leader同步数据</span><br><span class="line"></span><br><span class="line">OSR:out of sync replicas 失去同步的副本(数据与 leader 之间的差距超过配置的阈值)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kafka不是完全同步，也不是完全异步</span></span><br><span class="line">1.leader会维持一个与其保持同步的replica集合，该集合就是ISR，每一个partition都有一个ISR，它是有leader动态维护。</span><br><span class="line"></span><br><span class="line">2.我们要保证kafka不丢失message，就要保证ISR这组集合存活（至少有一个存活），并且消息commit成功。</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">英文简称</span></span><br><span class="line">--AR：  Assigned Replicas的缩写，是每个partition下所有副本（replicas）的统称；</span><br><span class="line">--ISR： In-Sync Replicas的缩写，是指副本同步队列，ISR是AR中的一个子集；</span><br><span class="line">--LEO：LogEndOffset的缩写，表示每个partition的log最后一条Message的位置。</span><br><span class="line">--HW： HighWatermark的缩写，是指consumer能够看到的此partition的位置。 取一个partition对应的ISR中最小的LEO作为HW，consumer最多只能消费到HW所在的位置。</span><br></pre></td></tr></table></figure>

<h4 id="（4）增加分区数"><a href="#（4）增加分区数" class="headerlink" title="（4）增加分区数"></a>（4）增加分区数</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --alter --topic tpc_1 --partitions 3 --zookeeper node1:2181</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">Kafka 只支持增加分区,不支持减少分区</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">原因是:减少分区,代价太大(数据的转移,日志段拼接合并)</span> </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如果真的需要实现此功能,则完全可以重新创建一个分区数较小的主题,然后将现有主题中的消息按照既定的逻辑复制过去;</span></span><br></pre></td></tr></table></figure>

<h4 id="（5）动态配置topic-参数"><a href="#（5）动态配置topic-参数" class="headerlink" title="（5）动态配置topic 参数"></a>（5）动态配置topic 参数</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">通过管理命令,可以为已创建的 topic 增加、修改、删除 topic level 参数</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">添加、修改配置参数(开启压缩发送传输种提高kafka消息吞吐量的有效办法(‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’))</span></span><br><span class="line"></span><br><span class="line">bin/kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --add-config compression.type=gzip </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">删除配置参数</span></span><br><span class="line">bin/kafka-configs.sh --zookeeper node1:2181 --entity-type topics --entity-name tpc_1 --alter --delete-config compression.type</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（6）生产消息到Kafka并进行消费"><a href="#（6）生产消息到Kafka并进行消费" class="headerlink" title="（6）生产消息到Kafka并进行消费"></a>（6）生产消息到Kafka并进行消费</h4><p>使用Kafka内置的测试程序，生产一些消息到Kafka的tpc_1主题中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example1-kafka-console-producer</span></span><br><span class="line">bin/kafka-console-producer.sh --broker-list node1:9092, node2:9092, node3:9092 --topic tpc_1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello word</span> </span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">kafka</span> </span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">nihao</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">example2-kafka-console-consumer</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(1)消费消息(从头开始)</span></span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server node1:9092, node2:9092, node1:9092 --topic tpc_1 --from-beginning</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">(2)指定要消费的分区,和要消费的起始 offset</span> </span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server node1:9092,node2:9092,node3:9092 --topic tcp_1 --offset 2 --partition 0</span><br></pre></td></tr></table></figure>

<h4 id="（7）配置管理-kafka-configs"><a href="#（7）配置管理-kafka-configs" class="headerlink" title="（7）配置管理 kafka-configs"></a>（7）<strong>配置管理</strong> <strong>kafka</strong>-configs</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">kafka-configs.sh 脚本是专门用来对配置进行操作的, 这里的操作是运行状态修改原有的配置, 如此可以达到动态变更的目; </span><br><span class="line"></span><br><span class="line">kafka-configs.sh 脚本包含:</span><br><span class="line">-变更 alter</span><br><span class="line">-查看 describe 这两种指令类型。</span><br><span class="line">同使用 kafka-topics.sh 脚本变更配置一样,增、删、改的行为都可以看做变更操作,不过 kafka-configs.sh 脚本不仅可支持操作主题相关的配置,还支持操 broker 、用户和客户端这 3 个类型的配置。实体entity</span><br><span class="line"></span><br><span class="line">kafka-configs.sh 脚本使用 entity-type 参数来指定操作配置的类型, 并且使 entity-name 参数来指定操作配置的名称。</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">比如查看 topic 的配置可以按如下方式执行:</span></span><br><span class="line">bin/kafka-configs.sh zookeeper node1: 2181 --describe --entity-type topics --entity-name tpc_2 </span><br><span class="line"></span><br><span class="line">比如查看 broker 的动态配置可以按如下方式执行:</span><br><span class="line">bin/kafka-configs.sh zookeeper node1: 2181 --describe --entity-type brokers --entity-name 0 --zookeeper node1:2181</span><br></pre></td></tr></table></figure>

<p><strong>entity-type 和entity-name 的对应关系</strong><br><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204213257261.png" alt="image-20230204213257261"></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">示例:添加 topic 级别参数</span></span><br><span class="line">bin/kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name tpc_2 --add-config cleanup.policy=compact , max.message.bytes=10000</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">使用 kafka-configs.sh 脚本来变更( alter )配置时,会在 ZooKeeper 中创建一个命名形式为: /config/&lt;entity-type&gt;/&lt;ent ity name &gt;的节点,并将变更的配置写入这个节点</span></span><br></pre></td></tr></table></figure>



<h4 id="（8）Kafka的生产者-x2F-消费者-x2F-工具"><a href="#（8）Kafka的生产者-x2F-消费者-x2F-工具" class="headerlink" title="（8）Kafka的生产者&#x2F;消费者&#x2F;工具"></a>（8）Kafka的生产者&#x2F;消费者&#x2F;工具</h4><ul>
<li>安装Kafka集群，可以测试以下<ul>
<li>创建一个topic主题（消息都是存放在topic中，类似mysql建表的过程）</li>
<li>基于kafka的内置测试生产者脚本来读取标准输入（键盘输入）的数据，并放入到topic中</li>
<li>基于kafka的内置测试消费者脚本来消费topic中的数据</li>
</ul>
</li>
<li>推荐大家开发的使用Kafka Tool<ul>
<li>浏览Kafka集群节点、多少个topic、多少个分区</li>
<li>创建topic&#x2F;删除topic</li>
<li>浏览ZooKeeper中的数据</li>
</ul>
</li>
</ul>
<img src=".\md图\kafka.assets\image-20230204213731698.png" alt="image-20230204213731698"  />

<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204213753683.png" alt="image-20230204213753683"></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204213812492.png" alt="image-20230204213812492"></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204213824345.png" alt="image-20230204213824345"></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204213832931.png" alt="image-20230204213832931"></p>
<p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cimage-20230204213840932.png" alt="image-20230204213840932"></p>
<h4 id="（9）Kafka的基准测试"><a href="#（9）Kafka的基准测试" class="headerlink" title="（9）Kafka的基准测试"></a>（9）Kafka的基准测试</h4><p>基准<a target="_blank" rel="noopener" href="http://www.blogjava.net/qileilove/archive/2012/07/05/382241.html">测试</a>（benchmark testing）是一种测量和评估软件性能指标的活动。我们可以通过基准测试，了解到软件、硬件的性能水平。主要测试负载的执行时间、传输速度、吞吐量、资源占用率等。</p>
<ul>
<li><p>Kafka中提供了内置的性能测试工具</p>
<ul>
<li><p>生产者：测试生产每秒传输的数据量（多少条数据、多少M的数据）</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5000000 records sent, 11825.446943 records/sec (11.28 MB/sec), 2757.61 ms avg latency</span><br></pre></td></tr></table></figure>
</li>
<li><p>消费者：测试消费每条拉取的数据量</p>
</li>
</ul>
</li>
<li><p>对比生产者和消费者：消费者的速度更快</p>
</li>
</ul>
<h2 id="五、Kafka-Java-API开发"><a href="#五、Kafka-Java-API开发" class="headerlink" title="五、Kafka Java API开发"></a>五、Kafka Java API开发</h2><h3 id="5-1-API-开发：producer-生产者"><a href="#5-1-API-开发：producer-生产者" class="headerlink" title="5.1 API 开发：producer 生产者"></a>5.1 API 开发：producer 生产者</h3><h4 id="（1）生产者api示例"><a href="#（1）生产者api示例" class="headerlink" title="（1）生产者api示例"></a>（1）生产者api示例</h4><blockquote>
<p>一个正常的生产逻辑需要具备以下几个步骤</p>
</blockquote>
<p><code>(1)配置生产者客户端参数</code></p>
<p><code>(2)创建相应的生产者实例</code></p>
<p><code>(3)构建待发送的消息</code></p>
<p><code>(4)发送消息</code></p>
<p><code>(5)关闭生产者实例</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//首先,引入 maven 依赖</span></span><br><span class="line">&lt;dependency&gt; </span><br><span class="line">	&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; </span><br><span class="line">	&lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; </span><br><span class="line">	&lt;version&gt;<span class="number">2.0</span><span class="number">.0</span>&lt;/version&gt; </span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//采用默认分区方式将消息散列的发送到各个分区当中</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer; </span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Producer; </span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord; </span><br><span class="line"><span class="keyword">import</span> java.util.Properties; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyProducer</span> &#123; </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[ ] args)</span> <span class="keyword">throws</span> InterruptedException &#123; </span><br><span class="line">		<span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line">		<span class="comment">//设置 kafka 集群的地址</span></span><br><span class="line">		props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>);</span><br><span class="line">		<span class="comment">//ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，</span></span><br><span class="line">		<span class="comment">//0-&gt;不等响应就继续发（可靠性低），</span></span><br><span class="line">		<span class="comment">//1-&gt;leader会写到本地日志后，然后给响应，producer拿到响应才继续发（follwer还没同步）</span></span><br><span class="line">		props.put(“acks”, “all”); <span class="comment">//--》很重要</span></span><br><span class="line"></span><br><span class="line">		props.put(“retries”, <span class="number">3</span>); <span class="comment">//失败重试次数-&gt;失败会自动重试（可恢复/不可恢复）--&gt;(有可能会造成数据的乱序)</span></span><br><span class="line">		props.put(“batch.size”, <span class="number">10</span>); <span class="comment">//数据发送的批次大小提高效率/吞吐量太大会数据延迟</span></span><br><span class="line">		props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">10000</span>); <span class="comment">//消息在缓冲区保留的时间,超过设置的值就会被提交到服务端</span></span><br><span class="line">		props.put(<span class="string">&quot;max.request.size&quot;</span>,<span class="number">10</span>); <span class="comment">//数据发送请求的最大缓存数</span></span><br><span class="line">		props.put(“buffer.memory”, <span class="number">10240</span>); <span class="comment">//整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端</span></span><br><span class="line">		<span class="comment">//buffer.memory 要大于 batch.size,否则会报申请内存不足的错误降低阻塞的可能性</span></span><br><span class="line">		props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>); <span class="comment">//key-value序列化器</span></span><br><span class="line">		props.put(<span class="string">&quot;value.serializer&quot;</span>, 	<span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);<span class="comment">//字符串最好</span></span><br><span class="line">		Producer&lt;String, String&gt; producer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(props); </span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; i++) </span><br><span class="line">			producer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;String, String&gt;(<span class="string">&quot;test&quot;</span>, Integer.toString(i), <span class="string">&quot;dd:&quot;</span>+i)); </span><br><span class="line">        <span class="comment">//Thread.sleep(1000000); </span></span><br><span class="line">        producer.close(); </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//消息对象 ProducerRecord,它并不是单纯意义上的消息,它包含了多个属性,原本需要发送的与业务关的消息体只是其中的一个 value 属性 ,比“ Hello, rgzn!&quot;只是 ProducerRecord 对象的一个属性。 ProducerRecord 类的定义如下:</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProducerRecord</span>&lt;K, V&gt; &#123; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> String topic; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Integer partition;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Headers headers; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> K key; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> V value; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Long timestamp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="（2）必要的参数配置"><a href="#（2）必要的参数配置" class="headerlink" title="（2）必要的参数配置"></a>（2）必要的参数配置</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//在创建真正的生产者实例前需要配置相应的参数,比如需要连接的 Kafka 集群地址。在 Kafka 生产者客户端 KatkaProducer 中有 3 个参数是必填的。</span></span><br><span class="line">-bootstrap.servers </span><br><span class="line">-key.serializer </span><br><span class="line">-value.serializer</span><br><span class="line">    </span><br><span class="line"><span class="comment">//为了防止参数名字符串书写错误,可以使用如下方式进行设置: </span></span><br><span class="line">props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,ProducerInterceptorPrefix.class.getName());</span><br><span class="line">props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;node1:9092,node2:9092&quot;</span>); </span><br><span class="line">props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName()); props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());</span><br></pre></td></tr></table></figure>

<h4 id="（3）发送消息"><a href="#（3）发送消息" class="headerlink" title="（3）发送消息"></a>（3）发送消息</h4><p>创建生产者实例和构建消息之后就可以开始发送消息了。发送消息主要有3 种模式：</p>
<h5 id="1-发后即忘-fire-and-forget）"><a href="#1-发后即忘-fire-and-forget）" class="headerlink" title="1.发后即忘( fire-and-forget）"></a>1.发后即忘( fire-and-forget）</h5><p><code>天文领域--&gt;三体--&gt;叶文洁红岸</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#发后即忘,producer只管往 Kafka 发送,并不关心消息是否正确到达。</span></span><br><span class="line"></span><br><span class="line">在大多数情况下,这种发送方式没有问题; </span><br><span class="line">不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。</span><br><span class="line">这种发送方式的性能最高,可靠性最差。</span><br><span class="line"></span><br><span class="line">ack--&gt;作用在broker </span><br><span class="line">Future&lt;RecordMetadata&gt; send = producer.send(rcd);-》也是异步</span><br><span class="line"></span><br><span class="line">没成功的话，producer也不管了</span><br></pre></td></tr></table></figure>

<h5 id="2-同步发送（sync-）"><a href="#2-同步发送（sync-）" class="headerlink" title="2.同步发送（sync ）"></a>2.同步发送（sync ）</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">	producer.send(rcd).get( ); <span class="comment">//--》一旦调用get方法，就会阻塞</span></span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123; </span><br><span class="line">	e.printStackTrace( );</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">0.8.x 前,有一个参数 `producer.type=sycn|asycn` 来决定生产者的发送模式;<span class="comment">#-&gt;取消了</span></span><br><span class="line"></span><br><span class="line">现已失效(其实，新版中,producer 在底层只有异步方式，若想同步，发送一次，get一次就可实现)</span><br><span class="line"></span><br><span class="line">Future  future = Callable.run( ) <span class="comment">#-&gt; 有返回值，future.get（）</span></span><br><span class="line">runnable.run（）<span class="comment">#-&gt;无返回值</span></span><br><span class="line">多线程，new thread，然后new一个runnable<span class="comment">#-&gt;线程干活去了-&gt;没有返回值（拿不到）</span></span><br><span class="line">Future future =  Callable.run()<span class="comment">#-&gt; future.get()-&gt;可以有同步的实现方式了-&gt;使用.get()方法，就可以实现同步了</span></span><br></pre></td></tr></table></figure>

<h5 id="3-异步发送-async"><a href="#3-异步发送-async" class="headerlink" title="3.异步发送(async )"></a>3.异步发送(async )</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">回调函数会在 producer 收到 ack 时调用,为异步调用,该方法有两个参数,分别是 `RecordMetadata` 和`Exception`,如果 `Exception 为 null`,说明消息`发送成功`,如果 `Exception 不为 null`,说明消息`发送失败`。同时，则recordMetadata是有值的</span><br><span class="line"></span><br><span class="line"><span class="comment">#注意:消息发送失败会自动重试,不需要我们在回调函数中手动重试。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//代码示例</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.*; </span><br><span class="line"><span class="keyword">import</span> java.util.Properties; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyProducer</span> &#123; </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123; </span><br><span class="line">		<span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); <span class="comment">// Kafka 服务端的主机名和端口号</span></span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>); </span><br><span class="line">		<span class="comment">// 等待所有副本节点的应答</span></span><br><span class="line">		props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>); <span class="comment">// 消息发送最大尝试次数</span></span><br><span class="line">		props.put(<span class="string">&quot;retries&quot;</span>, <span class="number">0</span>); <span class="comment">// 一批消息处理大小</span></span><br><span class="line">		props.put(<span class="string">&quot;batch.size&quot;</span>, <span class="number">16384</span>); <span class="comment">// 增加服务端请求延时</span></span><br><span class="line">		props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">1</span>); <span class="comment">// 发送缓存区内存大小</span></span><br><span class="line">		props.put(<span class="string">&quot;buffer.memory&quot;</span>, <span class="number">33554432</span>); <span class="comment">// key 序列化</span></span><br><span class="line">		props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>); </span><br><span class="line">		<span class="comment">// value 序列化</span></span><br><span class="line">		props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>); 				  			KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(props); </span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">50</span>; i++) &#123; </span><br><span class="line">			kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;String, String&gt;(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;hello&quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123; </span><br><span class="line">			<span class="meta">@Override</span> </span><br><span class="line">			<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123; </span><br><span class="line">				<span class="keyword">if</span> (metadata != <span class="literal">null</span>) &#123; </span><br><span class="line">					System.out.println(metadata.partition() + <span class="string">&quot;---&quot;</span> + metadata.offset()); </span><br><span class="line">                &#125; </span><br><span class="line">            &#125; </span><br><span class="line">            &#125;); </span><br><span class="line">        &#125; </span><br><span class="line">        kafkaProducer.close(); </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（4）生产者原理解析"><a href="#（4）生产者原理解析" class="headerlink" title="（4）生产者原理解析"></a>（4）生产者原理解析</h4><p><img src="/.%5Cmd%E5%9B%BE%5Ckafka.assets%5Cproducer%E6%B5%81%E7%A8%8B%E5%9B%BE.jpg" alt="producer流程图"></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">1.一个生产者客户端由两个线程协调运行,这两个线程分别为`主线程`和 `Sender 线程 。</span><br><span class="line"></span><br><span class="line">2.在主线程中由` kafkaProducer `创建消息,然后通过可能的`拦截器`、`序列化器`和`分区器`的作用之后缓存到消息累加器(`RecordAccumulator`, 也称为消息收集器)中。</span><br><span class="line"></span><br><span class="line">3.`Sender线程`负责从 `RecordAccumulator` 获取消息并将其发送到 Kafka 中; </span><br><span class="line"></span><br><span class="line">4.`RecordAccumulator `主要用来`缓存消息`以便 Sender 线程可以批量发送, 进而减少网络传输的资源消耗以提升性能。 </span><br><span class="line"></span><br><span class="line">5.`RecordAccumulator `缓存的大小可以通过生产者客户端参数 `buffer.memory` 配置, 默认值为 33554432B ,即 32M。</span><br><span class="line">如果`生产者发送消息的速度超过发送到服务器的速度`,则会导致生产者空间不足,这个时候 `KafkaProducer.send()`方法调用要么被阻塞,要么抛出异常,这个取决于参数`max.block.ms` 的配置,此参数的默认值为 60000,即 60 秒。</span><br><span class="line"></span><br><span class="line">6.主线程中发送过来的消息都会被迫加到 `RecordAccumulator `的某个`双端队列( Deque )`中, `RecordAccumulator `内部为每个分区都维护了一个双端队列,即 `Deque&lt;ProducerBatch&gt;``。</span><br><span class="line">消息写入缓存时,`追加到双端队列的尾部`;</span><br><span class="line"></span><br><span class="line">7.`Sender `读取消息时,`从双端队列的头部读取`。</span><br><span class="line"></span><br><span class="line">8.注意:`ProducerBatch `是指一个消息批次; </span><br><span class="line">与此同时,会将较小的 `ProducerBatch `凑成一个较大 `ProducerBatch` ,也可以减少网络请求的次数以提升整体的吞吐量。</span><br><span class="line"></span><br><span class="line"><span class="comment">#问题：什么情况下，消息累加器中的分区会增多？</span></span><br><span class="line">9.`ProducerBatch` 大小和 `batch.size` 参数也有着密切的关系。</span><br><span class="line"></span><br><span class="line">10.当一条消息(`ProducerRecord `) 流入`RecordAccumulator` 时,会先寻找与消息分区所对应的双端队列(如果没有则新建),再从这个双端队列的尾部获取一个 `ProducerBatch` (如果没有则新建),查看 `ProducerBatch` 中是否还可以写入这个 `ProducerRecord`,如果可以写入,如果不可以则需要创建一个新的 `Producer Batch`。</span><br><span class="line"></span><br><span class="line">11.在新建`ProducerBatch `时评估这条消息的大小是否超过 `batch.size` 参数大小, 如果不超过, 那么就以 `batch.size` 参数的大小来创建 `ProducerBatch`。</span><br><span class="line"></span><br><span class="line"><span class="comment">#如果生产者客户端需要向很多分区发送消息, 则可以将 buffer.memory 参数适当调大以增加整体的吞吐量。</span></span><br><span class="line"></span><br><span class="line">12.`Sender `从 `RecordAccumulator` 获取缓存的消息之后,会进一步将`&lt;分区,Deque&lt;Producer Batch&gt;&gt;`的形式转变成`&lt;Node,List&lt; ProducerBatch&gt;`的形式,其中 Node 表示 Kafka 集群 broker 节点。</span><br><span class="line"></span><br><span class="line">13.对于网络连接来说,生产者客户端是与具体 `broker` 节点建立的连接,也就是向具体的` broker `节点发送消息,而并不关心消息属于哪一个分区;</span><br><span class="line"></span><br><span class="line">14.而对于 `KafkaProducer `的应用逻辑而言,我们只关注向哪个分区中发送哪些消息,所以在这里需要做一个应用逻辑层面到网络 I/O 层面的转换。</span><br><span class="line"></span><br><span class="line">15.在转换成`&lt;Node, List&lt;ProducerBatch&gt;&gt;`的形式之后, Sender 会进一步封装成`&lt;Node,Request&gt; `的形式, 这样就可以将 `Request `请求发往各个 Node 了,这里的 `Request `是 Kafka 各种协议请求;</span><br><span class="line"></span><br><span class="line">16.请求在从 sender 线程发往 Kafka 之前还会保存到 `InFlightRequests `中,`InFlightRequests` 保存对象的具体形式为 `Map&lt;Nodeld, Deque&lt;request&gt;&gt;`,它的主要作用是缓存了已经发出去但还没有收到服务端响应的请求(Nodeld 是一个 String 类型,表示节点的 <span class="built_in">id</span> 编号)。</span><br><span class="line"></span><br><span class="line">17.与此同时,`InFlightRequests` 还提供了许多管理类的方法,并且通过配置参数还可以限制每个连接(也就是客户端与 Node 之间的连接) 最多缓存的请求数。</span><br><span class="line"></span><br><span class="line">18.这个配置参数为 `max.in.flight.request.per.connection` ,默认值为 5,即每个连接最多只能缓存 5 个未响应的请求,超过该数值之后就不能再向这个连接发送更多的请求了,除非有缓存的请求收到了响应( Response )。</span><br><span class="line"></span><br><span class="line">19.通过比较 `Deque&lt;Request&gt;` 的 `size` 与这个`参数的大小`来判断对应的 Node 中是否己经堆积了很多未响应的消息, 如果真是如此, 那么`说明这个 Node 节点负载较大或网络连接有问题,再继其发送请求会增大请求超时的可能`。</span><br></pre></td></tr></table></figure>

<h4 id="（5）重要的生产者参数"><a href="#（5）重要的生产者参数" class="headerlink" title="（5）重要的生产者参数"></a>（5）重要的生产者参数</h4><h5 id="1-acks"><a href="#1-acks" class="headerlink" title="1.acks"></a>1.acks</h5><table>
<thead>
<tr>
<th>acks</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>Producer 往集群发送数据不需要等到集群的返回，不确保消息发送成功。安全性最低但是效率最高。</td>
</tr>
<tr>
<td>1</td>
<td>Producer 往集群发送数据只要Leader 成功写入消息就可以发送下一条，只确保Leader 接收成功。</td>
</tr>
<tr>
<td>-1或all</td>
<td>Producer 往集群发送数据需要所有的ISR Follower 都完成从Leader 的同步才会发送下一条，确保<br/>Leader 发送成功和所有的副本都成功接收。安全性最高，但是效率最低。</td>
</tr>
</tbody></table>
<h5 id="2-max-request-size"><a href="#2-max-request-size" class="headerlink" title="2.max.request.size"></a>2.max.request.size</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">这个参数用来限制生产者客户端能发送的消息的最大值,默认值为 1048576B ,即 1MB 一般情况下,这个默认值就可以满足大多数的应用场景了。</span><br><span class="line"></span><br><span class="line">这个参数还涉及一些其它参数的联动,比如 broker 端的` message.max.bytes` 参数,如果配置错误可能会引起一些不必要的异常 ; </span><br><span class="line"></span><br><span class="line">比如将broker端的`message.max.bytes`参数配置为10 , 而`max.request.size` 参数配置为 20,那么当发送一条大小为 15B 的消息时,生产者客户端就会报出异常</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="3-compression-type"><a href="#3-compression-type" class="headerlink" title="3.compression.type"></a>3.compression.type</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">这个参数用来`指定消息的压缩方式,默认值为“none <span class="string">&quot;`,即默认情况下,消息不会被压缩。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">该参数还可以配置为 &quot;</span>gzip<span class="string">&quot;,&quot;</span>snappy<span class="string">&quot; 和 &quot;</span>lz4<span class="string">&quot;。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">`（服务端也有压缩参数，先解压，再压缩）`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">对消息进行压缩可以极大地减少网络传输、降低网络 I/O,从而提高整体的性能 。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">消息压缩是一种以时间换空间的优化方式,如果对时延有一定的要求,则不推荐对消息进行压缩;</span></span><br><span class="line"><span class="string">`没有必要，不需要压缩`</span></span><br><span class="line"><span class="string"></span></span><br></pre></td></tr></table></figure>

<h5 id="4-retries-和-retry-backoff-ms"><a href="#4-retries-和-retry-backoff-ms" class="headerlink" title="4.retries 和 retry.backoff.ms"></a>4.retries 和 retry.backoff.ms</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">retries 参数用来`配置生产者重试的次数`,默认值为 0,即在发生异常的时候不进行任何重试动作。</span><br><span class="line"></span><br><span class="line">消息在从生产者发出到成功写入服务器之前可能发生一些`临时性的异常`,比如`网络抖动`、 `leader 副本的选举`等,这种异常往往是可以自行恢复的,生产者可以`通过配置 retries 大于 0 的值`,以此通过内部重试来恢复而不是一味地将异常抛给生产者的应用程序。如果重试达到设定的次数,那么生产者就会放弃重试并返回异常。</span><br><span class="line"></span><br><span class="line">重试还和另一个参数 `retry.backoff.ms` 有关,这个参数的默认值为 100,它用来`设定两次重试之间的时间间隔,避免无效的频繁重试`。</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Kafka `可以保证同一个分区中的消息是有序的`。如果生产者按照一定的顺序发送消息,那么这些消息也会顺序地写入分区,进而消费者也可以按照同样的顺序消费它们。</span><br><span class="line"></span><br><span class="line">对于某些应用来，顺序性非常重要 ，比如 `MySQL binlog` 的传输,如果出现错误就会造成非常严重的后果; </span><br><span class="line"></span><br><span class="line"><span class="comment">#MySQL binlog --》mysql插入数据--》操作结果体会在表中--》mysql为了提高可靠性会把操作记录在日志中--》为了以后的主从同步（mysql集群，主表，子表）--》读写分离--》binlog（mysql自己设计的格式，二进制形式）</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Canal--》监听binlog--》解析为json--》写入kafka--》flume就可以读到---》在kafka中的顺序不能错</span></span><br><span class="line"></span><br><span class="line">如果将` acks 参数`配置为非零值,并且 `max.flight.requests.per.connection `参数配置为大于 1 的值,那可能会出现错序的现象:</span><br><span class="line"><span class="comment">#如果第一批次消息写入失败,而第二批次消息写入成功,那么生产者会重试发送第一批次的消息,此时如果第一次的消息写入成功,那么这两个批次的消息就出现了错序。</span></span><br><span class="line"></span><br><span class="line">一般而言,在需要保证消息顺序的场合建议把参数`max.in.flight.requests.per.connection `配置为 1 ,而不是把 acks 配置为 0,不过这样也会影响整体的吞吐。<span class="comment">#--》吞吐量降低</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="5-batch-size"><a href="#5-batch-size" class="headerlink" title="5.batch.size"></a>5.batch.size</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">每个 Batch 要存放` batch.size `大小的数据后,才可以发送出去。比如说 `batch.size` 默认值是 16KB,那么里面凑够 16KB 的数据才会发送。</span><br><span class="line"></span><br><span class="line">理论上来说, 提升 `batch.size` 的大小, 可以允许更多的数据缓冲在里面, 那么一次 `Request` 发送出去的数据量就更多了,这样吞吐量可能会有所提升。</span><br><span class="line"></span><br><span class="line">但是 `batch.size` 也不能过大,要是数据老是缓冲在 `Batch `里迟迟不发送出去,那么发送消息的延迟就会很高。</span><br><span class="line"></span><br><span class="line">一般可以尝试把这个参数调节大些,利用生产环境发消息负载测试一下。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="6-linger-ms-和batchsize有联系"><a href="#6-linger-ms-和batchsize有联系" class="headerlink" title="6.linger.ms(和batchsize有联系)"></a>6.linger.ms(和batchsize有联系)</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">这个参数用来指定生产者发送 `ProducerBatch` 之前等待更多消息( `ProducerRecord` )加入`ProducerBatch` 时间,默认值为 0。</span><br><span class="line"></span><br><span class="line">生产者客户端会在 `ProducerBatch `填满或等待时间超过 `linger.ms `值时发送出去。</span><br><span class="line"></span><br><span class="line">增大这个参数的值会增加消息的延迟,但是同时能提升一定的吞吐量。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="7-enable-idempotence-gt-true-x2F-false"><a href="#7-enable-idempotence-gt-true-x2F-false" class="headerlink" title="7.enable.idempotence  -&gt;true&#x2F;false"></a>7.enable.idempotence  -&gt;true&#x2F;false</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">是否开启幂等性功能,详见后续原理加强;</span><br><span class="line"></span><br><span class="line">幂等性,就是一个操作重复做,每次的结果都一样，x*1=1，x*1=1，x*1=1，</span><br><span class="line"></span><br><span class="line">在 kafka 中，就是生产者生产的一条消息，如果多次重复发送，在服务器中的结果还是只有一条</span><br><span class="line"></span><br><span class="line">Kafka很难实现幂等性，如果重复发，kafka肯定有多条消息---》需要有机制判断曾经是否发送过--》各种手段判断--》事务管理的概念----》加入幂等性，吞吐量会急剧下降</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="8-partitioner-classes"><a href="#8-partitioner-classes" class="headerlink" title="8.partitioner.classes"></a>8.partitioner.classes</h5><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">用来指定分区器,默认:`org.apache.kafka.internals.DefaultPartitioner` --》用`hashcode`分</span><br><span class="line"></span><br><span class="line">自定义 partitioner 需要实现 `org.apache.kafka.clients.producer.Partitioner` 接口--》自己写很简单</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="5-2-API-开发：consumer生产者"><a href="#5-2-API-开发：consumer生产者" class="headerlink" title="5.2 API 开发：consumer生产者"></a>5.2 API 开发：consumer生产者</h3><h4 id="（1）消费者Api-示例"><a href="#（1）消费者Api-示例" class="headerlink" title="（1）消费者Api 示例"></a>（1）消费者Api 示例</h4><blockquote>
<p>一个正常的消费逻辑需要具备以下几个步骤: </p>
</blockquote>
<p><code>(1)配置消费者客户端参数</code></p>
<p><code>(2)创建相应的消费者实例; </code></p>
<p><code>(3)订阅主题; </code></p>
<p><code>(4)拉取消息并消费; </code></p>
<p><code>(5)提交消费位移 offset;</code></p>
<p><code>(6)关闭消费者实例。</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//消费者实例代码</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*; </span><br><span class="line"><span class="keyword">import</span> java.util.Arrays; </span><br><span class="line"><span class="keyword">import</span> java.util.Properties; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyConsumer</span> &#123; </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123; </span><br><span class="line">		<span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line">		<span class="comment">// 定义 kakfa 服务的地址,不需要将所有 broker 指定上</span></span><br><span class="line">		props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1:9092&quot;</span>); </span><br><span class="line">		<span class="comment">// 指定 consumer group </span></span><br><span class="line">		props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;g1&quot;</span>); </span><br><span class="line">		<span class="comment">// 是否自动提交 offset </span></span><br><span class="line">		props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>); </span><br><span class="line">		<span class="comment">// 自动提交 offset 的时间间隔</span></span><br><span class="line">		props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line">		<span class="comment">// key 的反序列化类</span></span><br><span class="line">		props.put(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>); </span><br><span class="line">		<span class="comment">// value 的反序列化类</span></span><br><span class="line">		props.put(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>); </span><br><span class="line">		<span class="comment">// 如果没有消费偏移量记录,则自动重设为起始 offset:latest, earliest, none</span></span><br><span class="line">		<span class="comment">//Earliest-&gt;目前状态下最前面的一条消息（日志在一定保存时间后会自动清空）</span></span><br><span class="line">		<span class="comment">//none（上次记录的偏移量，如果没有，会抛异常） </span></span><br><span class="line"></span><br><span class="line">		props.put(<span class="string">&quot;auto.offset.reset&quot;</span>,<span class="string">&quot;earliest&quot;</span>); </span><br><span class="line">		<span class="comment">// 定义 consumer </span></span><br><span class="line">		KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props); </span><br><span class="line">		<span class="comment">// 消费者订阅的 topic, 可同时订阅多个</span></span><br><span class="line">		consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;test&quot;</span>,<span class="string">&quot;test1&quot;</span>)); </span><br><span class="line">		<span class="keyword">while</span> (<span class="literal">true</span>) &#123; </span><br><span class="line">		<span class="comment">// 读取数据,读取超时时间为 100ms </span></span><br><span class="line">			ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>); </span><br><span class="line">			<span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) </span><br><span class="line">				System.out.printf(<span class="string">&quot;offset = %d, key = %s, value = %s%n&quot;</span>, 		record.offset(), record.key(), record.value()); </span><br><span class="line">        &#125; </span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="（2）必要参数配置"><a href="#（2）必要参数配置" class="headerlink" title="（2）必要参数配置"></a>（2）必要参数配置</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//也可以使用如下形式:</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,brokerList);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.GROUP_ID_CONFIG,groupid);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.CLIENT_ID_CONFIG,clientid);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（3）subscribe-订阅主题"><a href="#（3）subscribe-订阅主题" class="headerlink" title="（3）subscribe 订阅主题"></a>（3）subscribe 订阅主题</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//subscribe 有如下重载方法: </span></span><br><span class="line"><span class="comment">//(1)前面两种是通过集合的方式订阅一到多个topic</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Collection&lt;String&gt; topics,ConsumerRebalanceListener listener)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Collection&lt;String&gt; topics)</span></span><br><span class="line"><span class="comment">//(2)后两种主要是采用正则的方式订阅一到多个topic</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Pattern pattern, ConsumerRebalanceListener listener)</span> </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Pattern pattern)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//指定集合方式订阅主题</span></span><br><span class="line">consumer.subscribe(Arrays.asList(topic1)); </span><br><span class="line">consumer <span class="title function_">subscribe</span><span class="params">(Arrays.asList(topic2)</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//正则方式订阅主题</span></span><br><span class="line">如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中,如果有人又创建了新的主题,并且主题名字与正表达式相匹配,那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题,并且可以处理不同的类型,那么这种订阅方式就很有效。</span><br><span class="line"></span><br><span class="line">正则表达式的方式订阅的示例如下</span><br><span class="line">consumer.subscribe(Pattern.compile (<span class="string">&quot;topic.*&quot;</span> )); </span><br><span class="line">利用正则表达式订阅主题,可实现动态订阅;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ConsumerRebalanceListener listener</span><br><span class="line"><span class="comment">//一般同一个消费组中，一旦有触发消费者的增减变化，都会触发消费组的rebalance再均衡，如果消费者a消费一批消息后还没来得及提交偏移量offset，而它所负责的分区在rebalance中转移给了消费者b，则有可能发生消息的重复消费，那么此时可以通过再均衡器做一定程度的补救。</span></span><br><span class="line">consumer.subscribe(Arrays.asList(”tpc_1<span class="string">&quot;), new ConsumerRebalanceListener()&#123; @Override </span></span><br><span class="line"><span class="string">public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; </span></span><br><span class="line"><span class="string">log.info(&quot;</span>&lt;&gt;&lt;&gt; Before start consume the message &lt;&gt;&lt;&gt;<span class="string">&quot;); &#125; </span></span><br><span class="line"><span class="string">@Override </span></span><br><span class="line"><span class="string">public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; </span></span><br><span class="line"><span class="string">log.info(&quot;</span>&lt;&gt;&lt;&gt; After stop consume the message &lt;&gt;&lt;&gt;<span class="string">&quot;); &#125; &#125;);</span></span><br></pre></td></tr></table></figure>



<h4 id="（4）assign订阅主题"><a href="#（4）assign订阅主题" class="headerlink" title="（4）assign订阅主题"></a>（4）assign订阅主题</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">消费者不仅可以通过 KafkaConsumer.subscribe() 方法订阅主题,还可直接订阅某些主题的指定分区; </span><br><span class="line"></span><br><span class="line">在 KafkaConsumer 中提供了 assign() 方法来实现这些功能,此方法的具体定义如下: </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">//这个方法只接受参数 partitions,用来指定需要订阅的分区集合。</span></span><br><span class="line"></span><br><span class="line">示例如下: </span><br><span class="line">consumer.assign(Arrays.asList(<span class="keyword">new</span> <span class="title class_">TopicPartition</span> (<span class="string">&quot;tpc_1&quot;</span> , <span class="number">0</span>),<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(“tpc_2”,<span class="number">1</span>))) ;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（5）subscribe-与assign-的区别"><a href="#（5）subscribe-与assign-的区别" class="headerlink" title="（5）subscribe 与assign 的区别"></a>（5）subscribe 与assign 的区别</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(1)通过 `subscribe()`方法订阅主题具有消费者自动再均衡功能 ; </span><br><span class="line"></span><br><span class="line">在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。 </span><br><span class="line">当消费组的消费者增加或减少时,分区分配关系会自动调整,以实现消费负载均衡及故障自动转移。</span><br><span class="line"></span><br><span class="line">(2)`assign() `方法订阅分区时,是不具备消费者自动均衡的功能的; </span><br><span class="line"></span><br><span class="line">其实这一点从 `assign()`方法参数可以看出端倪,两种类型 `subscribe()`都有 `ConsumerRebalanceListener` 类型参数的方法,而 `assign()`方法却没有。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（6）取消订阅"><a href="#（6）取消订阅" class="headerlink" title="（6）取消订阅"></a>（6）取消订阅</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//既然有订阅,那么就有取消订阅; </span></span><br><span class="line"></span><br><span class="line">可以使用 KafkaConsumer 中的 unsubscribe()方法采取消主题的订阅,这个方法既可以取消通过subscribe( Collection)方式实现的订阅; </span><br><span class="line"></span><br><span class="line">也可以取消通过 subscribe(Pattem)方式实现的订阅,还可以取消通过 assign( Collection)方式实现的订阅。示例码如下: </span><br><span class="line"></span><br><span class="line">consumer.unsubscribe(); </span><br><span class="line"></span><br><span class="line">如果将 subscribe(Collection )或 assign(Collection)集合参数设置为空集合,作用与 unsubscribe()方法相同,如下示例中三行代码的效果相同: </span><br><span class="line"></span><br><span class="line"><span class="number">1.</span>consumer.unsubscribe(  ); </span><br><span class="line"><span class="number">2.</span>consumer.subscribe(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;(  )) ; </span><br><span class="line"><span class="number">3.</span>consumer.assign(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;TopicPartition&gt;(  ));</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（7）消息的消费模式"><a href="#（7）消息的消费模式" class="headerlink" title="（7）消息的消费模式"></a>（7）消息的消费模式</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。</span></span><br><span class="line"><span class="comment">//推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息。</span></span><br><span class="line"></span><br><span class="line">Kafka 中的消息消费是一个不断轮询的过程,消费者所要做的就是重复地调用 poll() 方法,poll()方法返回的是所订阅的主题(分区)上的一组消息。</span><br><span class="line"></span><br><span class="line">对于 poll ()方法而言,如果某些分区中没有可供消费的消息,那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息,那么 poll()方法返回为空的消息集; poll () 方法具体定义如下: </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> ConsumerRecords&lt;K, V&gt; <span class="title function_">poll</span><span class="params">(<span class="keyword">final</span> Duration timeout)</span> </span><br><span class="line">    </span><br><span class="line">超时时间参数 timeout , 用来控制 poll() 方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率,可以把 timeout 设置为Long.MAX_VALUE;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//消费者消费到的每条消息的类型为 ConsumerRecord</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConsumerRecord</span>&lt;K, V&gt; &#123; </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">NO_TIMESTAMP</span> <span class="operator">=</span> RecordBatch.NO_TIMESTAMP; </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">NULL_SIZE</span> <span class="operator">=</span> -<span class="number">1</span>; </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">NULL_CHECKSUM</span> <span class="operator">=</span> -<span class="number">1</span>; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> String topic; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> partition; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> offset;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> timestamp; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> TimestampType timestampType; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedKeySize; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedValueSize; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Headers headers; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> K key; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> V value; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">volatile</span> Long checksum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-topic -partition 这两个字段分别代表消息所属主题的名称和所在分区的编号。</span><br><span class="line">-offsset 表示消息在所属分区的偏移量。</span><br><span class="line">-timestamp 表示时间戳,与此对应的 timestampType 表示时间戳的类型。</span><br><span class="line">-timestampType 有两种类型 CreateTime 和 LogAppendTime , 分别代表消息创建的时间戳和消息追加到日志的时间戳。</span><br><span class="line">-headers 表示消息的头部内容。</span><br><span class="line">-key value 分别表示消息的键和消息的值,一般业务应用要读取的就是 value ; </span><br><span class="line">-serializedKeySize、serializedValueSize 分别表示 key、value 经过序列化之后的大小,如果 key 为空, 则 serializedKeySize 值为 -1,同样,如果 value 为空,则 serializedValueSize 的值也会为 -1; </span><br><span class="line">-checksum 是 CRC32 的校验值。</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//示例代码片段</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 订阅与消费方式2</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="type">TopicPartition</span> <span class="variable">tp1</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;x&quot;</span>, <span class="number">0</span>);</span><br><span class="line"><span class="type">TopicPartition</span> <span class="variable">tp2</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;y&quot;</span>, <span class="number">0</span>);</span><br><span class="line"><span class="type">TopicPartition</span> <span class="variable">tp3</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(<span class="string">&quot;z&quot;</span>, <span class="number">0</span>);</span><br><span class="line">List&lt;TopicPartition&gt; tps = Arrays.asList(tp1, tp2, tp3);</span><br><span class="line">consumer.assign(tps);</span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">	ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">	<span class="keyword">for</span> (TopicPartition tp : tps) &#123;</span><br><span class="line">		List&lt;ConsumerRecord&lt;String, String&gt;&gt; rList = records.records(tp);</span><br><span class="line">		<span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; r : rList) &#123;</span><br><span class="line">			r.topic();</span><br><span class="line">			r.partition();</span><br><span class="line">			r.offset();</span><br><span class="line">			r.value();</span><br><span class="line">			<span class="comment">//do something to process record.</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="（8）指定位移消费"><a href="#（8）指定位移消费" class="headerlink" title="（8）指定位移消费"></a>（8）指定位移消费</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">有些时候,我们需要一种更细粒度的掌控,可以让我们从特定的位移处开始拉取消息,而KafkaConsumer 中的 seek() 方法正好提供了这个功能,让我们可以追前消费或回溯消费。</span><br><span class="line"></span><br><span class="line">seek()方法的具体定义如下: </span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">seek</span><span class="params">(TopicPartiton partition,<span class="type">long</span> offset)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//代码示例</span></span><br><span class="line"><span class="comment">// 在调用seek 方法之前，需要先调用一次poll，以分配到分区</span></span><br><span class="line">consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所分配到的分区信息</span></span><br><span class="line">Set&lt;TopicPartition&gt; assignment = consumer.assignment();</span><br><span class="line"><span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">	<span class="comment">// 为指定partition 设置读取起始offset</span></span><br><span class="line">	consumer.seek(topicPartition, <span class="number">80</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 开始正式消费</span></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">	ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">	<span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">	<span class="comment">// do some process</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="（9）再均衡监听器"><a href="#（9）再均衡监听器" class="headerlink" title="（9）再均衡监听器"></a>（9）再均衡监听器</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡; </span><br><span class="line">如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//代码示例</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 再均衡处理</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">consumer.subscribe(Collections.singletonList(<span class="string">&quot;tpc_5&quot;</span>), <span class="keyword">new</span> <span class="title class_">ConsumerRebalanceListener</span>() &#123;</span><br><span class="line">	<span class="comment">// 再均衡开始前和消费者停止读取消息之后，被调用</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; collection)</span> &#123;</span><br><span class="line">	<span class="comment">// store the current offset to db</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="comment">// 重新分配到分区后和消费者开始读取消息之前，被调用</span></span><br><span class="line">	<span class="meta">@Override</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; collection)</span> &#123;</span><br><span class="line">	<span class="comment">// fetch the current offset from db</span></span><br><span class="line">	&#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<h4 id="（10）自动位移提交"><a href="#（10）自动位移提交" class="headerlink" title="（10）自动位移提交"></a>（10）自动位移提交</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Kafka 中默认的消费位移的提交方式是自动提交,这个由消费者客户端参数 `enable.auto.commit `配置, 默认值为 <span class="literal">true</span> 。</span><br><span class="line"></span><br><span class="line">当然这个默认的自动提交不是每消费一条消息就提交一次,而是定期提交,这个定期的周期时间由客户端参数 `auto.commit.interval.ms` 配置, 默认值为 5 秒, 此参数生效的前提是 <span class="built_in">enable</span>.</span><br><span class="line">`auto.commit` 参数为 <span class="literal">true</span>。</span><br><span class="line"></span><br><span class="line">在默认的方式下,消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在 `poll() `方法的逻辑里完成的,在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交,如果可以,那么就会提交上一次轮询的位移。</span><br><span class="line"></span><br><span class="line"><span class="comment">#Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#重复消费</span></span><br><span class="line">假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。</span><br><span class="line"><span class="comment">#丢失消息</span></span><br><span class="line">按照一般思维逻辑而言,自动提交是延时提交,重复消费可以理解,那么消息丢失又是在什么情形下会发生的呢?我们来看下图中的情形: 拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第 y+1次拉取,以及第 m 次位移提交的时候,也就是x+6 之前的位移己经确认提交了, 处理线程却还正在处理 x+3 的消息; 此时如果处理线程发生了异常, 待其恢复之后会从第 m 次位移提交处,也就是 x+6 的位置开始拉取消息,那么 x+3 至 x+6 之间的消息就没有得到相应的处理,这样便发生消息丢失的现象。</span><br><span class="line"></span><br><span class="line"><span class="comment">#数据丢失不会发生在kafka集群里</span></span><br><span class="line"><span class="comment">#往往发生在处理具体业务逻辑的缓存里</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<img src=".\md图\kafka.assets\image-20230221221618036.png" alt="image-20230221221618036" style="zoom:50%;" />

<h4 id="（11）手动位移提交（调用kafka-api）"><a href="#（11）手动位移提交（调用kafka-api）" class="headerlink" title="（11）手动位移提交（调用kafka api）"></a>（11）手动位移提交（调用kafka api）</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象, 但是在编程的世界里异常无可避免; 同时, 自动位移提交也无法做到精确的位移管理。 </span><br><span class="line"></span><br><span class="line">在 Kafka 中还提供了手动位移提交的方式, 这样可以使得开发人员对消费位移的管理控制更加灵活。</span><br><span class="line">    </span><br><span class="line">很多时候并不是说拉取到消息就算消费完成,而是需要将消息写入数据库、写入本地缓存,或者是更加复杂的业务处理。在这些场景下,所有的业务处理完成才能认为消息被成功消费; 手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。 </span><br><span class="line">开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 <span class="literal">false</span> ,示例如下</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>); </span><br><span class="line"></span><br><span class="line">手动提交可以细分为同步提交和异步提交,对应于 KafkaConsumer 中的 commitSync()和commitAsync()两种类型的方法。</span><br></pre></td></tr></table></figure>

<p><code>1.同步提交的方式</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">//commitSync()方法的定义如下:</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 手动提交offset</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">	ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">	<span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; r : records) &#123;</span><br><span class="line"><span class="comment">//do something to process record.</span></span><br><span class="line">	&#125;</span><br><span class="line">	consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//对于采用 commitSync()的无参方法,它提交消费位移的频率和拉取批次消息、处理批次消息的频率是一样的, 如果想寻求更细粒度的、 更精准的提交, 那么就需要使用 commitSync()的另一个有参方法,具体定义如下：</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitSync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition，OffsetAndMetadata&gt; offsets)</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment">//示例代码如下：</span></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">	ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">	<span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; r : records) &#123;</span><br><span class="line">		<span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> r.offset();</span><br><span class="line">		<span class="comment">//do something to process record.</span></span><br><span class="line">		<span class="type">TopicPartition</span> <span class="variable">topicPartition</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(r.topic(), 	r.partition());</span><br><span class="line">		consumer.commitSync(Collections.singletonMap(topicPartition,<span class="keyword">new</span> 		<span class="title class_">OffsetAndMetadata</span>(offset+<span class="number">1</span>)));</span><br><span class="line">&#125;</span><br><span class="line">&#125;   </span><br><span class="line"><span class="comment">//提交的偏移量= 消费完的 record 的偏移量+ 1 因为,__consumer_offsets 中记录的消费偏移量,代表的是,消费者下一次要读取的位置</span></span><br></pre></td></tr></table></figure>

<p><code>2.异步提交方式</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//commitSync()方法相反,异步提交的方式( commitAsync())在执行的时候消费者线程不会被阻塞;可能在提交消费位移的结果还未返回之前就开始了新一次的拉取操 。异步提交以便消费者的性能得到一定的增强。 commitAsync 方法有一个不同的重载方法,具体定义如下</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitAsync</span><span class="params">()</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitAsync</span><span class="params">(0ffsetCommitcallback callback)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">commitAsync</span> <span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition,OffsetAndMetadata&gt; offsets,</span></span><br><span class="line"><span class="params">OffsetCommitCallback callback)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//示例代码</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">* 异步提交offset</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">	ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">	<span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; r : records) &#123;</span><br><span class="line">		<span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> r.offset();</span><br><span class="line">		<span class="comment">//do something to process record.</span></span><br><span class="line">		<span class="type">TopicPartition</span> <span class="variable">topicPartition</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">TopicPartition</span>(r.topic(), r.partition());</span><br><span class="line">		consumer.commitSync(Collections.singletonMap(topicPartition,<span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(offset+<span class="number">1</span>)));</span><br><span class="line">		consumer.commitAsync(Collections.singletonMap(topicPartition, <span class="keyword">new</span> 	<span class="title class_">OffsetAndMetadata</span>(offset + <span class="number">1</span>)), <span class="keyword">new</span></span><br><span class="line"><span class="title class_">OffsetCommitCallback</span>() &#123;</span><br><span class="line">		<span class="meta">@Override</span></span><br><span class="line">		<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onComplete</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; map, Exception e)</span> &#123;</span><br><span class="line">			<span class="keyword">if</span>(e == <span class="literal">null</span> )&#123;</span><br><span class="line">				System.out.println(map);</span><br><span class="line">			&#125;<span class="keyword">else</span>&#123;</span><br><span class="line">				System.out.println(<span class="string">&quot;error commit offset&quot;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;   </span><br></pre></td></tr></table></figure>

<h4 id="（12）其他重要参数"><a href="#（12）其他重要参数" class="headerlink" title="（12）其他重要参数"></a>（12）其他重要参数</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">fetch.min.bytes</span>=<span class="string">1B		 		一次拉取的最小字节数</span></span><br><span class="line"></span><br><span class="line"><span class="attr">fetch.max.bytes</span>=<span class="string">50M 				一次拉取的最大数据量</span></span><br><span class="line"></span><br><span class="line"><span class="attr">fetch.max.wait.ms</span>=<span class="string">500ms 			拉取时的最大等待时长</span></span><br><span class="line"></span><br><span class="line"><span class="attr">max.partition.fetch.bytes</span> = <span class="string">1MB 		每个分区一次拉取的最大数据量</span></span><br><span class="line"></span><br><span class="line"><span class="attr">max.poll.records</span>=<span class="string">500				一次拉取的最大条数</span></span><br><span class="line"><span class="attr">connections.max.idle.ms</span>=<span class="string">540000ms 	网络连接的最大闲置时长</span></span><br><span class="line"></span><br><span class="line"><span class="attr">request.timeout.ms</span>=<span class="string">30000ms 一次请求等待响应的最大超时时间consumer 等待请求响应的最长时间</span></span><br><span class="line"></span><br><span class="line"><span class="attr">metadata.max.age.ms</span>=<span class="string">300000 	元数据在限定时间内没有进行更新,则会被强制更新</span></span><br><span class="line"></span><br><span class="line"><span class="attr">reconnect.backoff.ms</span>=<span class="string">50ms 		尝试重新连接指定主机之前的退避时间</span></span><br><span class="line"></span><br><span class="line"><span class="attr">retry.backoff.ms</span>=<span class="string">100ms 		尝试重新拉取数据的重试间隔</span></span><br><span class="line"><span class="attr">isolation.level</span>=<span class="string">read_uncommitted 		隔离级别! 决定消费者能读到什么样的数据</span></span><br><span class="line"></span><br><span class="line"><span class="attr">read_uncommitted</span>:				<span class="string">可以消费到 LSO(LastStableOffset)位置; </span></span><br><span class="line"></span><br><span class="line"><span class="attr">read_committed</span>:				<span class="string">可以消费到 HW(High Watermark)位置</span></span><br><span class="line"></span><br><span class="line"><span class="attr">max.poll.interval.ms</span> 			<span class="string">超过时限没有发起 poll 操作,则消费组认为该消费者已离开消费组</span></span><br><span class="line"></span><br><span class="line"><span class="attr">enable.auto.commit</span>=<span class="string">true 			开启消费位移的自动提交</span></span><br><span class="line"></span><br><span class="line"><span class="attr">auto.commit.interval.ms</span>=<span class="string">5000 		自动提交消费位移的时间间隔</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>











<h3 id="5-3-API-开发：topic-管理"><a href="#5-3-API-开发：topic-管理" class="headerlink" title="5.3 API 开发：topic 管理"></a>5.3 API 开发：topic 管理</h3><p>一般情况下,我们都习惯使用 kafka-topic.sh 本来管理主题,如果希望将管理类的功能集成到公司内部的系统中,打造集管理、监控、运维、告警为一体的生态平台,那么就需要以程序调用 API 方式去实现。</p>
<p>这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类</p>
<p>KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">-创建主题:CreateTopicsResult <span class="title function_">createTopics</span><span class="params">(Collection&lt;NewTopic&gt; newTopics)</span>。</span><br><span class="line"></span><br><span class="line">-删除主题:DeleteTopicsResult <span class="title function_">deleteTopics</span><span class="params">(Collection&lt;String&gt; topics)</span>。</span><br><span class="line">    </span><br><span class="line">-列出所有可用的主题:ListTopicsResult <span class="title function_">listTopics</span><span class="params">()</span>。</span><br><span class="line">    </span><br><span class="line">-查看主题的信息:DescribeTopicsResult <span class="title function_">describeTopics</span><span class="params">(Collection&lt;String&gt; topicNames)</span>。</span><br><span class="line"></span><br><span class="line">-查询配置信息:DescribeConfigsResult <span class="title function_">describeConfigs</span><span class="params">(Collection&lt;ConfigResource&gt;resources)</span>。</span><br><span class="line">    </span><br><span class="line">-修改配置信息:AlterConfigsResult <span class="title function_">alterConfigs</span><span class="params">(Map&lt;ConfigResource, Config&gt; configs)</span>。</span><br><span class="line"></span><br><span class="line">-增加分区:CreatePartitionsResult <span class="title function_">createPartitions</span><span class="params">(Map&lt;String, NewPartitions&gt; newPartitions)</span>。</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">构造一个 KafkaAdminClient </span><br><span class="line"><span class="type">AdminClient</span> <span class="variable">adminClient</span> <span class="operator">=</span> KafkaAdminClient.create(props);</span><br></pre></td></tr></table></figure>

<h4 id="（1）列出主题"><a href="#（1）列出主题" class="headerlink" title="（1）列出主题"></a>（1）列出主题</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ListTopicsResult</span> <span class="variable">listTopicsResult</span> <span class="operator">=</span> adminClient.listTopics(); </span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; topics = listTopicsResult.names().get(); </span><br><span class="line"></span><br><span class="line">System.out.println(topics);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（2）查看主题信息"><a href="#（2）查看主题信息" class="headerlink" title="（2）查看主题信息"></a>（2）查看主题信息</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DescribeTopicsResult</span> <span class="variable">describeTopicsResult</span> <span class="operator">=</span> adminClient.describeTopics(Arrays.asList(<span class="string">&quot;tpc_4&quot;</span>, <span class="string">&quot;tpc_3&quot;</span>)); </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Map&lt;String, TopicDescription&gt; res = describeTopicsResult.all().get();</span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; ksets = res.keySet(); </span><br><span class="line"><span class="keyword">for</span> (String k : ksets) &#123; </span><br><span class="line">	System.out.println(res.get(k)); </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="（3）创建主题"><a href="#（3）创建主题" class="headerlink" title="（3）创建主题"></a>（3）创建主题</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//代码示例:</span></span><br><span class="line"><span class="comment">// 参数配置</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line">props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>); </span><br><span class="line">props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,<span class="number">3000</span>); </span><br><span class="line"><span class="comment">// 创建 admin client 对象</span></span><br><span class="line"><span class="type">AdminClient</span> <span class="variable">adminClient</span> <span class="operator">=</span> KafkaAdminClient.create(props); </span><br><span class="line"><span class="comment">// 由服务端 controller 自行分配分区及副本所在 broker </span></span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_3</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_3&quot;</span>, <span class="number">2</span>, (<span class="type">short</span>) <span class="number">1</span>); </span><br><span class="line"><span class="comment">// 手动指定分区及副本的 broker 分配</span></span><br><span class="line">HashMap&lt;Integer, List&lt;Integer&gt;&gt; replicaAssignments = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;(); </span><br><span class="line"><span class="comment">// 分区 0,分配到 broker0,broker1 replicaAssignments.put(0,Arrays.asList(0,1)); </span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 分区 1,分配到 broker0,broker2 </span></span><br><span class="line">replicaAssignments.put(<span class="number">0</span>,Arrays.asList(<span class="number">0</span>,<span class="number">1</span>));</span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_4</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_4&quot;</span>, replicaAssignments); </span><br><span class="line"><span class="type">CreateTopicsResult</span> <span class="variable">result</span> <span class="operator">=</span> adminClient.createTopics(Arrays.asList(tpc_3,tpc_4)); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 从 future 中等待服务端返回</span></span><br><span class="line"><span class="keyword">try</span> &#123; </span><br><span class="line">	result.all().get(); </span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123; </span><br><span class="line">e.printStackTrace(); </span><br><span class="line">&#125; </span><br><span class="line">adminClient.close();</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="（4）删除主题"><a href="#（4）删除主题" class="headerlink" title="（4）删除主题"></a>（4）删除主题</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//代码示例: </span></span><br><span class="line"><span class="type">DeleteTopicsResult</span> <span class="variable">deleteTopicsResult</span> <span class="operator">=</span> adminClient.deleteTopics(Arrays.asList(<span class="string">&quot;tpc_1&quot;</span>, <span class="string">&quot;tpc_1&quot;</span>)); </span><br><span class="line"></span><br><span class="line">Map&lt;String, KafkaFuture&lt;Void&gt;&gt; values = deleteTopicsResult.values();</span><br><span class="line"></span><br><span class="line">System.out.println(values);</span><br></pre></td></tr></table></figure>

<h4 id="（5）其他管理"><a href="#（5）其他管理" class="headerlink" title="（5）其他管理"></a>（5）其他管理</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">除了进行 topic 管理之外,KafkaAdminClient 也可以进行诸如动态参数管理,分区管理等各类管理操作;</span><br></pre></td></tr></table></figure>

<h3 id="5-4-复习"><a href="#5-4-复习" class="headerlink" title="5.4 复习"></a>5.4 复习</h3><h4 id="1-生产者程序开发"><a href="#1-生产者程序开发" class="headerlink" title="1.生产者程序开发"></a>1.生产者程序开发</h4><ol>
<li>创建连接<ul>
<li>bootstrap.servers：Kafka的服务器地址</li>
<li>acks：表示当生产者生产数据到Kafka中，Kafka中会以什么样的策略返回</li>
<li>key.serializer：Kafka中的消息是以key、value键值对存储的，而且生产者生产的消息是需要在网络上传到的，这里指定的是StringSerializer方式，就是以字符串方式发送（将来还可以使用其他的一些序列化框架：Google ProtoBuf、Avro）</li>
<li>value.serializer：同上</li>
</ul>
</li>
<li>创建一个生产者对象KafkaProducer</li>
<li>调用send方法发送消息（ProducerRecor，封装是key-value键值对）</li>
<li>调用Future.get表示等带服务端的响应</li>
<li>关闭生产者</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaProducerTest</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException &#123;</span><br><span class="line">        <span class="comment">// 1. 创建用于连接Kafka的Properties配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1.itcast.cn:9092&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;acks&quot;</span>, <span class="string">&quot;all&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 创建一个生产者对象KafkaProducer</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 发送1-100的消息到指定的topic中</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">100</span>; ++i) &#123;</span><br><span class="line">            <span class="comment">// 构建一条消息，直接new ProducerRecord</span></span><br><span class="line">            ProducerRecord&lt;String, String&gt; producerRecord = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;test&quot;</span>, <span class="literal">null</span>, i + <span class="string">&quot;&quot;</span>);</span><br><span class="line">            Future&lt;RecordMetadata&gt; future = kafkaProducer.send(producerRecord);</span><br><span class="line">            <span class="comment">// 调用Future的get方法等待响应</span></span><br><span class="line">            future.get();</span><br><span class="line">            System.out.println(<span class="string">&quot;第&quot;</span> + i + <span class="string">&quot;条消息写入成功！&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.关闭生产者</span></span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-消费者程序开发"><a href="#2-消费者程序开发" class="headerlink" title="2.消费者程序开发"></a>2.消费者程序开发</h4><ul>
<li>group.id：消费者组的概念，可以在一个消费组中包含多个消费者。如果若干个消费者的group.id是一样的，表示它们就在一个组中，一个组中的消费者是共同消费Kafka中topic的数据。</li>
<li>Kafka是一种拉消息模式的消息队列，在消费者中会有一个offset，表示从哪条消息开始拉取数据</li>
<li>kafkaConsumer.poll：Kafka的消费者API是一批一批数据的拉取</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 消费者程序</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 1.创建Kafka消费者配置</span></span><br><span class="line"><span class="comment"> * Properties props = new Properties();</span></span><br><span class="line"><span class="comment"> * props.setProperty(&quot;bootstrap.servers&quot;, &quot;node1.itcast.cn:9092&quot;);</span></span><br><span class="line"><span class="comment"> * props.setProperty(&quot;group.id&quot;, &quot;test&quot;);</span></span><br><span class="line"><span class="comment"> * props.setProperty(&quot;enable.auto.commit&quot;, &quot;true&quot;);</span></span><br><span class="line"><span class="comment"> * props.setProperty(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);</span></span><br><span class="line"><span class="comment"> * props.setProperty(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span></span><br><span class="line"><span class="comment"> * props.setProperty(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 2.创建Kafka消费者</span></span><br><span class="line"><span class="comment"> * 3.订阅要消费的主题</span></span><br><span class="line"><span class="comment"> * 4.使用一个while循环，不断从Kafka的topic中拉取消息</span></span><br><span class="line"><span class="comment"> * 5.将将记录（record）的offset、key、value都打印出来</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConsumerTest</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1.创建Kafka消费者配置</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1.itcast.cn:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// 消费者组（可以使用消费者组将若干个消费者组织到一起），共同消费Kafka中topic的数据</span></span><br><span class="line">        <span class="comment">// 每一个消费者需要指定一个消费者组，如果消费者的组名是一样的，表示这几个消费者是一个组中的</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;test&quot;</span>);</span><br><span class="line">        <span class="comment">// 自动提交offset</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>);</span><br><span class="line">        <span class="comment">// 自动提交offset的时间间隔</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line">        <span class="comment">// 拉取的key、value数据的</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2.创建Kafka消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 订阅要消费的主题</span></span><br><span class="line">        <span class="comment">// 指定消费者从哪个topic中拉取数据</span></span><br><span class="line">        kafkaConsumer.subscribe(Arrays.asList(<span class="string">&quot;test&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 4.使用一个while循环，不断从Kafka的topic中拉取消息</span></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="comment">// Kafka的消费者一次拉取一批的数据</span></span><br><span class="line">            ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">5</span>));</span><br><span class="line">            <span class="comment">// 5.将将记录（record）的offset、key、value都打印出来</span></span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">                <span class="comment">// 主题</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> consumerRecord.topic();</span><br><span class="line">                <span class="comment">// offset：这条消息处于Kafka分区中的哪个位置</span></span><br><span class="line">                <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> consumerRecord.offset();</span><br><span class="line">                <span class="comment">// key\value</span></span><br><span class="line">                <span class="type">String</span> <span class="variable">key</span> <span class="operator">=</span> consumerRecord.key();</span><br><span class="line">                <span class="type">String</span> <span class="variable">value</span> <span class="operator">=</span> consumerRecord.value();</span><br><span class="line"></span><br><span class="line">                System.out.println(<span class="string">&quot;topic: &quot;</span> + topic + <span class="string">&quot; offset:&quot;</span> + offset + <span class="string">&quot; key:&quot;</span> + key + <span class="string">&quot; value:&quot;</span> + value);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="3-生产者使用异步方式生产消息"><a href="#3-生产者使用异步方式生产消息" class="headerlink" title="3.生产者使用异步方式生产消息"></a>3.生产者使用异步方式生产消息</h4><ul>
<li>使用匿名内部类实现Callback接口，该接口中表示Kafka服务器响应给客户端，会自动调用onCompletion方法<ul>
<li>metadata：消息的元数据（属于哪个topic、属于哪个partition、对应的offset是什么）</li>
<li>exception：这个对象Kafka生产消息封装了出现的异常，如果为null，表示发送成功，如果不为null，表示出现异常。</li>
</ul>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 二、使用异步回调的方式发送消息</span></span><br><span class="line">ProducerRecord&lt;String, String&gt; producerRecord = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;test&quot;</span>, <span class="literal">null</span>, i + <span class="string">&quot;&quot;</span>);</span><br><span class="line">kafkaProducer.send(producerRecord, <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 判断发送消息是否成功</span></span><br><span class="line">        <span class="keyword">if</span>(exception == <span class="literal">null</span>) &#123;</span><br><span class="line">            <span class="comment">// 发送成功</span></span><br><span class="line">            <span class="comment">// 主题</span></span><br><span class="line">            <span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> metadata.topic();</span><br><span class="line">            <span class="comment">// 分区id</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">partition</span> <span class="operator">=</span> metadata.partition();</span><br><span class="line">            <span class="comment">// 偏移量</span></span><br><span class="line">            <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> metadata.offset();</span><br><span class="line">            System.out.println(<span class="string">&quot;topic:&quot;</span> + topic + <span class="string">&quot; 分区id：&quot;</span> + partition + <span class="string">&quot; 偏移量：&quot;</span> + offset);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// 发送出现错误</span></span><br><span class="line">            System.out.println(<span class="string">&quot;生产消息出现异常！&quot;</span>);</span><br><span class="line">            <span class="comment">// 打印异常消息</span></span><br><span class="line">            System.out.println(exception.getMessage());</span><br><span class="line">            <span class="comment">// 打印调用栈</span></span><br><span class="line">            System.out.println(exception.getStackTrace());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>

<h2 id="六、Kafka整合"><a href="#六、Kafka整合" class="headerlink" title="六、Kafka整合"></a>六、Kafka整合</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Kafka 和flume 的整合有3 种方式：</span><br><span class="line">1.把kafka 当做source 的数据源</span><br><span class="line">2.把kafka 当做channel</span><br><span class="line">3.把kafka 作为sink 的目标存储</span><br></pre></td></tr></table></figure>

<h3 id="6-1-Kafka-Flume"><a href="#6-1-Kafka-Flume" class="headerlink" title="6.1 Kafka+Flume"></a>6.1 Kafka+Flume</h3><h4 id="6-1-1-Flume-从kafka-source-中读取数据"><a href="#6-1-1-Flume-从kafka-source-中读取数据" class="headerlink" title="6.1.1 Flume 从kafka-source  中读取数据"></a>6.1.1 Flume 从kafka-source  中读取数据</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">org.apache.flume.source.kafka.KafkaSource</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.consumer.group.id</span> = <span class="string">g00001</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.topics</span> = <span class="string">tpc_2</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.consumer.auto.offset.reset</span> = <span class="string">earliest</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">2000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">org.apache.flume.sink.kafka.KafkaSink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.topic</span> = <span class="string">tpc_3</span></span><br><span class="line"><span class="attr">a1.sinks.k1.flumeBatchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.acks</span> = <span class="string">-1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.allowTopicOverride</span> = <span class="string">false</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.linger.ms</span> = <span class="string">1000</span></span><br></pre></td></tr></table></figure>

<h4 id="6-1-2-Flume-把kafka-作为channel"><a href="#6-1-2-Flume-把kafka-作为channel" class="headerlink" title="6.1.2 Flume 把kafka 作为channel"></a>6.1.2 Flume 把kafka 作为channel</h4><p>有两种方式：<br>（1）配了一个source+channel</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="attr">a1.sources.r1.command</span> = <span class="string">tail -F /root/abc.log</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">org.apache.flume.channel.kafka.KafkaChannel</span></span><br><span class="line"><span class="attr">a1.channels.c1.kafka.topic</span> = <span class="string">flume-channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br></pre></td></tr></table></figure>

<p>（2）配了一个channle+sink</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">org.apache.flume.channel.kafka.KafkaChannel</span></span><br><span class="line"><span class="attr">a1.channels.c1.kafka.topic</span> = <span class="string">flume-channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">hdfs</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.path</span> = <span class="string">hdfs://node1:8020/logdata/%Y-%m-%d/%H/</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.filePrefix</span> = <span class="string">logdata_</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileSuffix</span> = <span class="string">.log</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollInterval</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollSize</span> = <span class="string">268435456</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.rollCount</span> = <span class="string">0</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.codeC</span> = <span class="string">gzip</span></span><br><span class="line"><span class="attr">a1.sinks.k1.hdfs.fileType</span> = <span class="string">CompressedStream</span></span><br></pre></td></tr></table></figure>

<h4 id="6-1-3-Flume-用kafka-sink-把数据写入kafka"><a href="#6-1-3-Flume-用kafka-sink-把数据写入kafka" class="headerlink" title="6.1.3 Flume 用kafka-sink 把数据写入kafka"></a>6.1.3 Flume 用kafka-sink 把数据写入kafka</h4><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">org.apache.flume.source.kafka.KafkaSource</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.consumer.group.id</span> = <span class="string">g00001</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.topics</span> = <span class="string">tpc_2</span></span><br><span class="line"><span class="attr">a1.sources.r1.batchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sources.r1.kafka.consumer.auto.offset.reset</span> = <span class="string">earliest</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">2000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">org.apache.flume.sink.kafka.KafkaSink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.topic</span> = <span class="string">tpc_3</span></span><br><span class="line"><span class="attr">a1.sinks.k1.flumeBatchSize</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.acks</span> = <span class="string">-1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.allowTopicOverride</span> = <span class="string">false</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.linger.ms</span> = <span class="string">1000</span></span><br></pre></td></tr></table></figure>

<h4 id="6-1-4-案例"><a href="#6-1-4-案例" class="headerlink" title="6.1.4 案例"></a>6.1.4 案例</h4><p>（1）配置flume</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define</span></span><br><span class="line"><span class="attr">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="attr">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="attr">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="comment"># source</span></span><br><span class="line"><span class="attr">a1.sources.r1.type</span> = <span class="string">exec</span></span><br><span class="line"><span class="attr">a1.sources.r1.command</span> = <span class="string">tail -F -c +0 /root/flume.log</span></span><br><span class="line"><span class="attr">a1.sources.r1.shell</span> = <span class="string">/bin/bash -c</span></span><br><span class="line"><span class="comment"># sink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.type</span> = <span class="string">org.apache.flume.sink.kafka.KafkaSink</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.bootstrap.servers</span> = <span class="string">node1:9092,node2:9092,node3:9092</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.topic</span> = <span class="string">test</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.flumeBatchSize</span> = <span class="string">20</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.acks</span> = <span class="string">1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.kafka.producer.linger.ms</span> = <span class="string">1</span></span><br><span class="line"><span class="comment"># channel</span></span><br><span class="line"><span class="attr">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="attr">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="attr">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"><span class="comment"># bind</span></span><br><span class="line"><span class="attr">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="attr">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure>

<p>（2）启动客户端消费者<br>（3）启动flume，进入flume bin 目录下<br>.&#x2F;flume-ng agent -c conf&#x2F; -n a1 -f jobs&#x2F;flume-kafka.conf<br>（4）向日志文件增加数据查看消费情况<br>echo hello &gt;&gt; &#x2F;root&#x2F;flume.log</p>
<h3 id="6-2-Kafka-sparkStreaming"><a href="#6-2-Kafka-sparkStreaming" class="headerlink" title="6.2 Kafka+sparkStreaming"></a>6.2 Kafka+sparkStreaming</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//以workCount 示意：</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerRecord</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">InputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">ConsumerStrategies</span>, <span class="type">KafkaUtils</span>, <span class="type">LocationStrategies</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">		<span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;spark streaming 整合kafka&quot;</span>).setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">        </span><br><span class="line">		<span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line">		<span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">			<span class="string">&quot;bootstrap.servers&quot;</span> -&gt; <span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>,</span><br><span class="line">			<span class="string">&quot;key.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">			<span class="string">&quot;value.deserializer&quot;</span> -&gt; classOf[<span class="type">StringDeserializer</span>],</span><br><span class="line">			<span class="string">&quot;group.id&quot;</span> -&gt; <span class="string">&quot;use_a_separate_group_id_for_each_stream&quot;</span>,</span><br><span class="line">			<span class="string">&quot;auto.offset.reset&quot;</span> -&gt; <span class="string">&quot;earliest&quot;</span>,</span><br><span class="line">			<span class="string">&quot;enable.auto.commit&quot;</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>)</span><br><span class="line">			)</span><br><span class="line">		<span class="keyword">val</span> topics = <span class="type">Array</span>(<span class="string">&quot;test&quot;</span>)</span><br><span class="line">		<span class="comment">// 获取数据</span></span><br><span class="line">		<span class="keyword">val</span> stream: <span class="type">InputDStream</span>[<span class="type">ConsumerRecord</span>[<span class="type">String</span>, <span class="type">String</span>]] = 		<span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">ssc,</span><br><span class="line"><span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span>, <span class="comment">// 如果计算节点和Broker 是同一台节点可以使用PreferBrokers</span></span><br><span class="line"><span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](topics, kafkaParams)</span><br><span class="line">)</span><br><span class="line">		stream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> words: <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">String</span>]] = rdd.map(_.value()).map(line =&gt; line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">			<span class="keyword">val</span> wordAndOne: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = words.flatMap(arr =&gt; arr.map(word =&gt; (word, <span class="number">1</span>)))</span><br><span class="line">		<span class="comment">// RDD[(K, V)]</span></span><br><span class="line">			<span class="keyword">val</span> wordCountResult = wordAndOne.reduceByKey(_ + _)</span><br><span class="line">wordCountResult.foreach(println)</span><br><span class="line">        &#125;)</span><br><span class="line">	ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h2 id="七、kafka原理加强"><a href="#七、kafka原理加强" class="headerlink" title="七、kafka原理加强"></a>七、kafka原理加强</h2><h3 id="7-1-Kafka中的重要概念"><a href="#7-1-Kafka中的重要概念" class="headerlink" title="7.1 Kafka中的重要概念"></a>7.1 Kafka中的重要概念</h3><h4 id="（1）架构组件"><a href="#（1）架构组件" class="headerlink" title="（1）架构组件"></a>（1）架构组件</h4><ul>
<li>broker<ul>
<li>Kafka服务器进程，生产者、消费者都要连接broker</li>
<li>一个集群由多个broker组成，功能实现Kafka集群的负载均衡、容错</li>
</ul>
</li>
<li>producer：生产者</li>
<li>consumer：消费者</li>
<li>topic：主题，一个Kafka集群中，可以包含多个topic。一个topic可以包含多个分区<ul>
<li>是一个逻辑结构，生产、消费消息都需要指定topic</li>
</ul>
</li>
<li>partition：Kafka集群的分布式就是由分区来实现的。一个topic中的消息可以分布在topic中的不同partition中</li>
<li>replica：副本，实现Kafkaf集群的容错，实现partition的容错。一个topic至少应该包含大于1个的副本</li>
<li>consumer group：消费者组，一个消费者组中的消费者可以共同消费topic中的分区数据。每一个消费者组都一个唯一的名字。配置group.id一样的消费者是属于同一个组中</li>
<li>offset：偏移量。相对消费者、partition来说，可以通过offset来拉取数据</li>
</ul>
<h4 id="（2）消费者组"><a href="#（2）消费者组" class="headerlink" title="（2）消费者组"></a>（2）消费者组</h4><ul>
<li>一个消费者组中可以包含多个消费者，共同来消费topic中的数据</li>
<li>一个topic中如果只有一个分区，那么这个分区只能被某个组中的一个消费者消费</li>
<li>有多少个分区，那么就可以被同一个组内的多少个消费者消费</li>
</ul>
<h4 id="（3）幂等性"><a href="#（3）幂等性" class="headerlink" title="（3）幂等性"></a>（3）幂等性</h4><ul>
<li><p>生产者消息重复问题</p>
<ul>
<li>Kafka生产者生产消息到partition，如果直接发送消息，kafka会将消息保存到分区中，但Kafka会返回一个ack给生产者，表示当前操作是否成功，是否已经保存了这条消息。如果ack响应的过程失败了，此时生产者会重试，继续发送没有发送成功的消息，Kafka又会保存一条一模一样的消息</li>
</ul>
</li>
<li><p>在Kafka中可以开启幂等性</p>
<ul>
<li>当Kafka的生产者生产消息时，会增加一个pid（生产者的唯一编号）和sequence number（针对消息的一个递增序列）</li>
<li>发送消息，会连着pid和sequence number一块发送</li>
<li>kafka接收到消息，会将消息和pid、sequence number一并保存下来</li>
<li>如果ack响应失败，生产者重试，再次发送消息时，Kafka会根据pid、sequence number是否需要再保存一条消息</li>
<li>判断条件：生产者发送过来的sequence number 是否小于等于 partition中消息对应的sequence</li>
</ul>
</li>
</ul>
<h3 id="7-2-事务编程"><a href="#7-2-事务编程" class="headerlink" title="7.2 事务编程"></a>7.2 事务编程</h3><ul>
<li><p>开启事务的条件</p>
<ul>
<li><p>生产者</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 开启事务必须要配置事务的ID</span></span><br><span class="line">props.put(<span class="string">&quot;transactional.id&quot;</span>, <span class="string">&quot;dwd_user&quot;</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>消费者</p>
  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 配置事务的隔离级别</span></span><br><span class="line">props.put(<span class="string">&quot;isolation.level&quot;</span>,<span class="string">&quot;read_committed&quot;</span>);</span><br><span class="line"><span class="comment">// 关闭自动提交，一会我们需要手动来提交offset，通过事务来维护offset</span></span><br><span class="line">props.setProperty(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>生产者</p>
<ul>
<li>初始化事务</li>
<li>开启事务</li>
<li>需要使用producer来将消费者的offset提交到事务中</li>
<li>提交事务</li>
<li>如果出现异常回滚事务</li>
</ul>
</li>
</ul>
</li>
</ul>
<blockquote>
<p>如果使用了事务，不要使用异步发送</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">TransactionProgram</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 调用之前实现的方法，创建消费者、生产者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = createConsumer();</span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = createProducer();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 生产者调用initTransactions初始化事务</span></span><br><span class="line">        producer.initTransactions();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 编写一个while死循环，在while循环中不断拉取数据，进行处理后，再写入到指定的topic</span></span><br><span class="line">        <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                <span class="comment">// (1)	生产者开启事务</span></span><br><span class="line">                producer.beginTransaction();</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 这个Map保存了topic对应的partition的偏移量</span></span><br><span class="line">                Map&lt;TopicPartition, OffsetAndMetadata&gt; offsetMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 从topic中拉取一批的数据</span></span><br><span class="line">                <span class="comment">// (2)	消费者拉取消息</span></span><br><span class="line">                ConsumerRecords&lt;String, String&gt; concumserRecordArray = consumer.poll(Duration.ofSeconds(<span class="number">5</span>));</span><br><span class="line">                <span class="comment">// (3)	遍历拉取到的消息，并进行预处理</span></span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; cr : concumserRecordArray) &#123;</span><br><span class="line">                    <span class="comment">// 将1转换为男，0转换为女</span></span><br><span class="line">                    <span class="type">String</span> <span class="variable">msg</span> <span class="operator">=</span> cr.value();</span><br><span class="line">                    String[] fieldArray = msg.split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 将消息的偏移量保存</span></span><br><span class="line">                    <span class="comment">// 消费的是ods_user中的数据</span></span><br><span class="line">                    <span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> cr.topic();</span><br><span class="line">                    <span class="type">int</span> <span class="variable">partition</span> <span class="operator">=</span> cr.partition();</span><br><span class="line">                    <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> cr.offset();</span><br><span class="line"></span><br><span class="line">                	<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span> / <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// offset + 1：offset是当前消费的记录（消息）对应在partition中的offset，而我们希望下一次能继续从下一个消息消息</span></span><br><span class="line">                    <span class="comment">// 必须要+1，从能消费下一条消息</span></span><br><span class="line">                    offsetMap.put(<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(topic, partition), <span class="keyword">new</span> <span class="title class_">OffsetAndMetadata</span>(offset + <span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 将字段进行替换</span></span><br><span class="line">                    <span class="keyword">if</span>(fieldArray != <span class="literal">null</span> &amp;&amp; fieldArray.length &gt; <span class="number">2</span>) &#123;</span><br><span class="line">                        <span class="type">String</span> <span class="variable">sexField</span> <span class="operator">=</span> fieldArray[<span class="number">1</span>];</span><br><span class="line">                        <span class="keyword">if</span>(sexField.equals(<span class="string">&quot;1&quot;</span>)) &#123;</span><br><span class="line">                            fieldArray[<span class="number">1</span>] = <span class="string">&quot;男&quot;</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                        <span class="keyword">else</span> <span class="keyword">if</span>(sexField.equals(<span class="string">&quot;0&quot;</span>))&#123;</span><br><span class="line">                            fieldArray[<span class="number">1</span>] = <span class="string">&quot;女&quot;</span>;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// 重新拼接字段</span></span><br><span class="line">                    msg = fieldArray[<span class="number">0</span>] + <span class="string">&quot;,&quot;</span> + fieldArray[<span class="number">1</span>] + <span class="string">&quot;,&quot;</span> + fieldArray[<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">                    <span class="comment">// (4)	生产消息到dwd_user topic中</span></span><br><span class="line">                    ProducerRecord&lt;String, String&gt; dwdMsg = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(<span class="string">&quot;dwd_user&quot;</span>, msg);</span><br><span class="line">                    <span class="comment">// 发送消息</span></span><br><span class="line">                    Future&lt;RecordMetadata&gt; future = producer.send(dwdMsg);</span><br><span class="line">                    <span class="keyword">try</span> &#123;</span><br><span class="line">                        future.get();</span><br><span class="line">                    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                        e.printStackTrace();</span><br><span class="line">                        producer.abortTransaction();</span><br><span class="line">                    &#125;</span><br><span class="line"><span class="comment">//                            new Callback()</span></span><br><span class="line"><span class="comment">//                    &#123;</span></span><br><span class="line"><span class="comment">//                        @Override</span></span><br><span class="line"><span class="comment">//                        public void onCompletion(RecordMetadata metadata, Exception exception) &#123;</span></span><br><span class="line"><span class="comment">//                            // 生产消息没有问题</span></span><br><span class="line"><span class="comment">//                            if(exception == null) &#123;</span></span><br><span class="line"><span class="comment">//                                System.out.println(&quot;发送成功:&quot; + dwdMsg);</span></span><br><span class="line"><span class="comment">//                            &#125;</span></span><br><span class="line"><span class="comment">//                            else &#123;</span></span><br><span class="line"><span class="comment">//                                System.out.println(&quot;生产消息失败:&quot;);</span></span><br><span class="line"><span class="comment">//                                System.out.println(exception.getMessage());</span></span><br><span class="line"><span class="comment">//                                System.out.println(exception.getStackTrace());</span></span><br><span class="line"><span class="comment">//                            &#125;</span></span><br><span class="line"><span class="comment">//                        &#125;</span></span><br><span class="line"><span class="comment">//                    &#125;);</span></span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                producer.sendOffsetsToTransaction(offsetMap, <span class="string">&quot;ods_user&quot;</span>);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// (6)	提交事务</span></span><br><span class="line">                producer.commitTransaction();</span><br><span class="line">            &#125;<span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">                <span class="comment">// (7)	捕获异常，如果出现异常，则取消事务</span></span><br><span class="line">                producer.abortTransaction();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 一、创建一个消费者来消费ods_user中的数据</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> KafkaConsumer&lt;String, String&gt; <span class="title function_">createConsumer</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 配置消费者的属性（添加对事务的支持）</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.setProperty(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1.itcast.cn:9092&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;ods_user&quot;</span>);</span><br><span class="line">        <span class="comment">// 配置事务的隔离级别</span></span><br><span class="line">        props.put(<span class="string">&quot;isolation.level&quot;</span>,<span class="string">&quot;read_committed&quot;</span>);</span><br><span class="line">        <span class="comment">// 关闭自动提交，一会我们需要手动来提交offset，通过事务来维护offset</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;false&quot;</span>);</span><br><span class="line">        <span class="comment">// 反序列化器</span></span><br><span class="line">        props.setProperty(<span class="string">&quot;key.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line">        props.setProperty(<span class="string">&quot;value.deserializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 构建消费者对象</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 订阅一个topic</span></span><br><span class="line">        kafkaConsumer.subscribe(Arrays.asList(<span class="string">&quot;ods_user&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> kafkaConsumer;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 二、编写createProducer方法，用来创建一个带有事务配置的生产者</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> KafkaProducer&lt;String, String&gt; <span class="title function_">createProducer</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="comment">// 1. 配置生产者带有事务配置的属性</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1.itcast.cn:9092&quot;</span>);</span><br><span class="line">        <span class="comment">// 开启事务必须要配置事务的ID</span></span><br><span class="line">        props.put(<span class="string">&quot;transactional.id&quot;</span>, <span class="string">&quot;dwd_user&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;value.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 2. 构建生产者</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> kafkaProducer;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h3 id="7-3-Kafka中的分区副本机制"><a href="#7-3-Kafka中的分区副本机制" class="headerlink" title="7.3 Kafka中的分区副本机制"></a>7.3 Kafka中的分区副本机制</h3><h4 id="（1）生产者的分区写入策略"><a href="#（1）生产者的分区写入策略" class="headerlink" title="（1）生产者的分区写入策略"></a>（1）生产者的分区写入策略</h4><ul>
<li>轮询（按照消息尽量保证每个分区的负载）策略，消息会均匀地分布到每个partition<ul>
<li>写入消息的时候，key为null的时候，默认使用的是轮询策略</li>
</ul>
</li>
<li>随机策略（不使用）</li>
<li>按key写入策略，key.hash() % 分区的数量</li>
<li>自定义分区策略（类似于MapReduce指定分区）</li>
</ul>
<blockquote>
<p>乱序问题</p>
<ul>
<li>在Kafka中生产者是有写入策略，如果topic有多个分区，就会将数据分散在不同的partition中存储</li>
<li>当partition数量大于1的时候，数据（消息）会打散分布在不同的partition中</li>
<li>如果只有一个分区，消息是有序的</li>
</ul>
</blockquote>
<h4 id="（2）消费组Consumer-Group-Rebalance机制"><a href="#（2）消费组Consumer-Group-Rebalance机制" class="headerlink" title="（2）消费组Consumer Group Rebalance机制"></a>（2）消费组Consumer Group Rebalance机制</h4><ul>
<li>再均衡：在某些情况下，消费者组中的消费者消费的分区会产生变化，会导致消费者分配不均匀（例如：有两个消费者消费3个，因为某个partition崩溃了，还有一个消费者当前没有分区要削峰），Kafka Consumer Group就会启用rebalance机制，重新平衡这个Consumer Group内的消费者消费的分区分配。</li>
<li>触发时机<ul>
<li>消费者数量发生变化<ul>
<li>某个消费者crash</li>
<li>新增消费者</li>
</ul>
</li>
<li>topic的数量发生变化<ul>
<li>某个topic被删除</li>
</ul>
</li>
<li>partition的数量发生变化<ul>
<li>删除partition</li>
<li>新增partition</li>
</ul>
</li>
</ul>
</li>
<li>不良影响<ul>
<li>发生rebalance，所有的consumer将不再工作，共同来参与再均衡，直到每个消费者都已经被成功分配所需要消费的分区为止（rebalance结束）</li>
</ul>
</li>
</ul>
<h4 id="（3）消费者的分区分配策略"><a href="#（3）消费者的分区分配策略" class="headerlink" title="（3）消费者的分区分配策略"></a>（3）消费者的分区分配策略</h4><p>分区分配策略：保障每个消费者尽量能够均衡地消费分区的数据，不能出现某个消费者消费分区的数量特别多，某个消费者消费的分区特别少</p>
<ul>
<li>Range分配策略（范围分配策略）：Kafka默认的分配策略<ul>
<li>n：分区的数量 &#x2F; 消费者数量</li>
<li>m：分区的数量 % 消费者数量</li>
<li>前m个消费者消费n+1个分区</li>
<li>剩余的消费者消费n个分区</li>
</ul>
</li>
<li>RoundRobin分配策略（轮询分配策略）<ul>
<li>消费者挨个分配消费的分区</li>
</ul>
</li>
<li>Striky粘性分配策略<ul>
<li>在没有发生rebalance跟轮询分配策略是一致的</li>
<li>发生了rebalance，轮询分配策略，重新走一遍轮询分配的过程。而粘性会保证跟上一次的尽量一致，只是将新的需要分配的分区，均匀的分配到现有可用的消费者中即可</li>
<li>减少上下文的切换</li>
</ul>
</li>
</ul>
<h4 id="（4）副本的ACK机制"><a href="#（4）副本的ACK机制" class="headerlink" title="（4）副本的ACK机制"></a>（4）副本的ACK机制</h4><p>producer是不断地往Kafka中写入数据，写入数据会有一个返回结果，表示是否写入成功。这里对应有一个ACKs的配置。</p>
<ul>
<li>acks &#x3D; 0：生产者只管写入，不管是否写入成功，可能会数据丢失。性能是最好的</li>
<li>acks &#x3D; 1：生产者会等到leader分区写入成功后，返回成功，接着发送下一条</li>
<li>acks &#x3D; -1&#x2F;all：确保消息写入到leader分区、还确保消息写入到对应副本都成功后，接着发送下一条，性能是最差的</li>
</ul>
<p>根据业务情况来选择ack机制，是要求性能最高，一部分数据丢失影响不大，可以选择0&#x2F;1。如果要求数据一定不能丢失，就得配置为-1&#x2F;all。</p>
<p>分区中是有leader和follower的概念，为了确保消费者消费的数据是一致的，只能从分区leader去读写消息，follower做的事情就是同步数据，Backup。</p>
<h4 id="（5）高级API（High-Level-API）、低级API（Low-Level-API）"><a href="#（5）高级API（High-Level-API）、低级API（Low-Level-API）" class="headerlink" title="（5）高级API（High-Level API）、低级API（Low-Level API）"></a>（5）高级API（High-Level API）、低级API（Low-Level API）</h4><ul>
<li>高级API就是直接让Kafka帮助管理、处理分配、数据<ul>
<li>offset存储在ZK中</li>
<li>由kafka的rebalance来控制消费者分配的分区</li>
<li>开发起来比较简单，无需开发者关注底层细节</li>
<li>无法做到细粒度的控制</li>
</ul>
</li>
<li>低级API：由编写的程序自己控制逻辑<ul>
<li>自己来管理Offset，可以将offset存储在ZK、MySQL、Redis、HBase、Flink的状态存储</li>
<li>指定消费者拉取某个分区的数据</li>
<li>可以做到细粒度的控制</li>
<li>原有的Kafka的策略会失效，需要我们自己来实现消费机制</li>
</ul>
</li>
</ul>
<h3 id="7-4-Kafka原理"><a href="#7-4-Kafka原理" class="headerlink" title="7.4 Kafka原理"></a>7.4 Kafka原理</h3><h4 id="（1）leader和follower"><a href="#（1）leader和follower" class="headerlink" title="（1）leader和follower"></a>（1）leader和follower</h4><ul>
<li>Kafka中的leader和follower是相对分区有意义，不是相对broker</li>
<li>Kafka在创建topic的时候，会尽量分配分区的leader在不同的broker中，其实就是负载均衡</li>
<li>leader职责：读写数据</li>
<li>follower职责：同步数据、参与选举（leader crash之后，会选举一个follower重新成为分区的leader</li>
<li>注意和ZooKeeper区分<ul>
<li>ZK的leader负责读、写，follower可以读取</li>
<li>Kafka的leader负责读写、follower不能读写数据（确保每个消费者消费的数据是一致的），Kafka一个topic有多个分区leader，一样可以实现数据操作的负载均衡</li>
</ul>
</li>
</ul>
<h4 id="（2）AR-ISR-OSR"><a href="#（2）AR-ISR-OSR" class="headerlink" title="（2）AR\ISR\OSR"></a>（2）AR\ISR\OSR</h4><ul>
<li>AR表示一个topic下的所有副本</li>
<li>ISR：In Sync Replicas，正在同步的副本（可以理解为当前有几个follower是存活的）</li>
<li>OSR：Out of Sync Replicas，不再同步的副本</li>
<li>AR &#x3D; ISR + OSR</li>
</ul>
<h4 id="（3）leader选举"><a href="#（3）leader选举" class="headerlink" title="（3）leader选举"></a>（3）leader选举</h4><ul>
<li><p>Controller：controller是kafka集群的老大，是针对Broker的一个角色</p>
<ul>
<li>Controller是高可用的，是用过ZK来进行选举</li>
</ul>
</li>
<li><p>Leader：是针对partition的一个角色</p>
<ul>
<li>Leader是通过ISR来进行快速选举</li>
</ul>
</li>
<li><p>如果Kafka是基于ZK来进行选举，ZK的压力可能会比较大。例如：某个节点崩溃，这个节点上不仅仅只有一个leader，是有不少的leader需要选举。通过ISR快速进行选举。</p>
</li>
<li><p>leader的负载均衡</p>
<ul>
<li>如果某个broker crash之后，就可能会导致partition的leader分布不均匀，就是一个broker上存在一个topic下不同partition的leader</li>
<li>通过以下指令，可以将leader分配到优先的leader对应的broker，确保leader是均匀分配的</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-leader-election.sh --bootstrap-server node1.itcast.cn:9092 --topic test --partition=2 --election-type preferred</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="（4）Kafka读写流程"><a href="#（4）Kafka读写流程" class="headerlink" title="（4）Kafka读写流程"></a>（4）Kafka读写流程</h4><ul>
<li>写流程<ul>
<li>通过ZooKeeper找partition对应的leader，leader是负责写的</li>
<li>producer开始写入数据</li>
<li>ISR里面的follower开始同步数据，并返回给leader ACK</li>
<li>返回给producer ACK</li>
</ul>
</li>
<li>读流程<ul>
<li>通过ZooKeeper找partition对应的leader，leader是负责读的</li>
<li>通过ZooKeeper找到消费者对应的offset</li>
<li>然后开始从offset往后顺序拉取数据</li>
<li>提交offset（自动提交——每隔多少秒提交一次offset、手动提交——放入到事务中提交）</li>
</ul>
</li>
</ul>
<h4 id="（5）Kafka的物理存储"><a href="#（5）Kafka的物理存储" class="headerlink" title="（5）Kafka的物理存储"></a>（5）Kafka的物理存储</h4><ul>
<li>Kafka的数据组织结构<ul>
<li>topic</li>
<li>partition</li>
<li>segment<ul>
<li>.log数据文件</li>
<li>.index（稀疏索引）</li>
<li>.timeindex（根据时间做的索引）</li>
</ul>
</li>
</ul>
</li>
<li>深入了解读数据的流程<ul>
<li>消费者的offset是一个针对partition全局offset</li>
<li>可以根据这个offset找到segment段</li>
<li>接着需要将全局的offset转换成segment的局部offset</li>
<li>根据局部的offset，就可以从（.index稀疏索引）找到对应的数据位置</li>
<li>开始顺序读取</li>
</ul>
</li>
</ul>
<h4 id="（6）消息传递的语义性"><a href="#（6）消息传递的语义性" class="headerlink" title="（6）消息传递的语义性"></a>（6）消息传递的语义性</h4><p>Flink里面有对应的每种不同机制的保证，提供Exactly-Once保障（二阶段事务提交方式）</p>
<ul>
<li>At-most once：最多一次（只管把数据消费到，不管有没有成功，可能会有数据丢失）</li>
<li>At-least once：最少一次（有可能会出现重复消费）</li>
<li>Exactly-Once：仅有一次（事务性性的保障，保证消息有且仅被处理一次）</li>
</ul>
<h4 id="（7）Kafka的消息不丢失"><a href="#（7）Kafka的消息不丢失" class="headerlink" title="（7）Kafka的消息不丢失"></a>（7）Kafka的消息不丢失</h4><ul>
<li>broker消息不丢失：因为有副本relicas的存在，会不断地从leader中同步副本，所以，一个broker crash，不会导致数据丢失，除非是只有一个副本。</li>
<li>生产者消息不丢失：ACK机制（配置成ALL&#x2F;-1）、配置0或者1有可能会存在丢失</li>
<li>消费者消费不丢失：重点控制offset<ul>
<li>At-least once：一种数据可能会重复消费</li>
<li>Exactly-Once：仅被一次消费</li>
</ul>
</li>
</ul>
<h4 id="（8）数据积压"><a href="#（8）数据积压" class="headerlink" title="（8）数据积压"></a>（8）数据积压</h4><ul>
<li>数据积压指的是消费者因为有一些外部的IO、一些比较耗时的操作（Full GC——Stop the world），就会造成消息在partition中一直存在得不到消费，就会产生数据积压</li>
<li>在企业中，我们要有监控系统，如果出现这种情况，需要尽快处理。虽然后续的Spark Streaming&#x2F;Flink可以实现背压机制，但是数据累积太多一定对实时系统它的实时性是有说影响的</li>
</ul>
<h4 id="（9）数据清理-amp-配额限速"><a href="#（9）数据清理-amp-配额限速" class="headerlink" title="（9）数据清理&amp;配额限速"></a>（9）数据清理&amp;配额限速</h4><ul>
<li>数据清理<ul>
<li>Log Deletion（日志删除）：如果消息达到一定的条件（时间、日志大小、offset大小），Kafka就会自动将日志设置为待删除（segment端的后缀名会以 .delete结尾），日志管理程序会定期清理这些日志<ul>
<li>默认是7天过期</li>
</ul>
</li>
<li>Log Compaction（日志合并）<ul>
<li>如果在一些key-value数据中，一个key可以对应多个不同版本的value</li>
<li>经过日志合并，就会只保留最新的一个版本</li>
</ul>
</li>
</ul>
</li>
<li>配额限速<ul>
<li>可以限制Producer、Consumer的速率</li>
<li>防止Kafka的速度过快，占用整个服务器（broker）的所有IO资源</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/06/19/Kafka/" data-id="clja4y95a000aakur4t3ka15v" data-title="Kafka" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2023/06/19/Spark%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Spark部署
        
      </div>
    </a>
  
  
    <a href="/2023/06/19/Hive3%E5%AE%89%E8%A3%85/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hive</div>
    </a>
  
</nav>

  
</article>


</section>
        <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/06/24/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/">Kafka命令行操作</a>
          </li>
        
          <li>
            <a href="/2023/06/24/Kafka%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/">Kafka环境配置</a>
          </li>
        
          <li>
            <a href="/2023/06/24/kafka-eagle%E5%90%84%E9%A1%B9%E5%8A%9F%E8%83%BD/">kafka-eagle各项功能</a>
          </li>
        
          <li>
            <a href="/2023/06/19/zookeeper%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/">Zookeeper部署</a>
          </li>
        
          <li>
            <a href="/2023/06/19/Sqoop/">Sqoop</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">June 2023</a></li></ul>
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 By Autoload<br>
      Driven - <a href="https://hexo.io/" target="_blank">Hexo</a>|Theme - <a href="https://github.com/autoload/hexo-theme-auto" target="_blank">Auto</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>